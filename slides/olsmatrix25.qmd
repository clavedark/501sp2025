---
title: "Deriving the OLS Estimator" 
author: "Dave Clark"
institute: "Binghamton University"
date: today
date-format: long
title-block-banner: TRUE
format: html
editor: source
embed-resources: true
#number-sections: true
#cache: true
#execute:
# freeze: true  # never re-render during project render
---
  
  ```{=html}
<style>
  table, th, td {
    font-size: 18px;
  }
</style>
  ```
```{r setup, include=FALSE ,echo=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(fig.retina = 2, fig.align = "center", warning=FALSE, error=FALSE, message=FALSE) 

library(leaflet.extras)
library(tidyverse)
library(ggthemes)
library(ggplot2)
library(leaflet)
library(lubridate)
library(haven) #read stata w/labels
library(ggridges) # multiple densities
library(tmap)
library(countrycode)
library(acled.api)
library(patchwork)
library(zoo)
library(countrycode)
library(mvtnorm)
library(modelsummary)
library("GGally")
library(stargazer)
library(highcharter)

```



# Deriving the OLS estimator

## Deriving $\hat{\beta}$

Start with:
$$y = X\beta + \epsilon$$

Minimize sum of squared errors:
$$\min_{\beta} \epsilon'\epsilon = \min_{\beta} (y - X\beta)'(y - X\beta)$$

Expand:
$$(y - X\beta)'(y - X\beta) = y'y - y'X\beta - \beta'X'y + \beta'X'X\beta$$

Simplify using symmetry ($y'X\beta = \beta'X'y$ as they're scalars):
$$= y'y - 2\beta'X'y + \beta'X'X\beta$$

Take derivative with respect to $\beta$ and set to zero:
$$\frac{\partial}{\partial\beta}(y'y - 2\beta'X'y + \beta'X'X\beta) = 0$$
$$-2X'y + 2X'X\beta = 0$$

Solve for $\beta$:
$$X'X\beta = X'y$$
$$\hat{\beta} = (X'X)^{-1}X'y$$

## Variance-Covariance Matrix

Start with $\hat{\beta} = (X'X)^{-1}X'y$ and substitute $y = X\beta + \epsilon$:

$$\hat{\beta} = (X'X)^{-1}X'(X\beta + \epsilon)$$
$$= \beta + (X'X)^{-1}X'\epsilon$$

Therefore:
$$\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon$$

The variance-covariance matrix is:
$$Var(\hat{\beta}) = E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)']$$
$$= E[(X'X)^{-1}X'\epsilon\epsilon'X(X'X)^{-1}]$$

Under homoskedasticity ($E[\epsilon\epsilon'] = \sigma^2I$):
$$Var(\hat{\beta}) = \sigma^2(X'X)^{-1}$$


<!-- --- -->
<!-- title: "OLS Derivation and Normal PDF Plot" -->
<!-- format: html -->
<!-- --- -->

<!-- ## OLS Derivation -->

<!-- We start with the linear regression model in matrix form: -->

<!-- $y = X\beta + \epsilon$ -->

<!-- where: -->

<!-- * $y$ is an $(n \times 1)$ vector of observed dependent variables. -->
<!-- * $X$ is an $(n \times k)$ matrix of observed independent variables (including a constant term if applicable). -->
<!-- * $\beta$ is a $(k \times 1)$ vector of unknown coefficients to be estimated. -->
<!-- * $\epsilon$ is an $(n \times 1)$ vector of unobserved error terms. -->

<!-- The Ordinary Least Squares (OLS) estimator aims to minimize the sum of squared residuals: -->

<!-- $\min_\beta (y - X\beta)^T(y - X\beta)$ -->

<!-- To find the minimum, we take the derivative with respect to $\beta$, set it to zero, and solve for $\beta$: -->

<!-- 1. **Derivative:** -->

<!-- $\frac{d}{d\beta} [(y - X\beta)^T(y - X\beta)] = -2X^Ty + 2X^TX\beta$ -->

<!-- 2. **Set to zero and solve:** -->

<!-- $-2X^Ty + 2X^TX\beta = 0$ -->
<!-- $X^TX\beta = X^Ty$ -->
<!-- $\hat{\beta} = (X^TX)^{-1}X^Ty$ (assuming $X^TX$ is invertible) -->

<!-- This is the OLS estimator for $\beta$. -->


<!-- ## Variance-Covariance Matrix of $\hat{\beta}$ -->

<!-- The variance-covariance matrix of $\hat{\beta}$ is given by: -->

<!-- $Var(\hat{\beta}) = E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^T]$ -->

<!-- Under the classical OLS assumptions (including homoscedasticity and no autocorrelation of errors), this simplifies to: -->

<!-- $Var(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$ -->

<!-- where $\sigma^2$ is the variance of the error term, $\epsilon$.  An unbiased estimator of $\sigma^2$ is given by: -->

<!-- $s^2 = \frac{(y - X\hat{\beta})^T(y - X\hat{\beta})}{(n - k)}$ -->

<!-- Thus, an estimator of the variance-covariance matrix is: -->

<!-- $\widehat{Var}(\hat{\beta}) = s^2 (X^TX)^{-1}$ -->



