---
title: "The Bivariate Model"
author: "Dave Clark"
institute: "Binghamton University"
date: today
date-format: long
title-block-banner: TRUE
format:
  html:
    embed-resources: true
  #pdf: default
editor: source  
#number-sections: true
#cache: true
#execute:
# freeze: true  # never re-render during project render
---
  
  ```{=html}
<style>
  table, th, td {
    font-size: 18px;
  }
</style>
  ```
```{r setup, include=FALSE ,echo=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(fig.retina = 2, fig.align = "center", warning=FALSE, error=FALSE, message=FALSE) 

library(leaflet.extras)
library(tidyverse)
library(ggthemes)
library(ggplot2)
library(leaflet)
library(lubridate)
library(haven) #read stata w/labels
library(ggridges) # multiple densities
library(tmap)
library(countrycode)
library(acled.api)
library(patchwork)
library(zoo)
library(mvtnorm)
library(stargazer)
library(highcharter)
library(GGally)
library(dplyr)
library(modelsummary)

```


# Regression

These slides aim to describe the elements of OLS regression - they attempt to connect the data structure to the matrix algebra that underlies the estimation of the coefficients.

Regression in any form is based on the conditional expectation of $y$ - the expected value of $Yy is conditional on some $X$s, but we don't know the actual conditions or effects of those $X$s.  So we can write the regression like this:


$$E[y|X_{1}, \ldots,X_{k}]= \beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}$$

## Estimating equation

\noindent Let $y$ be a linear function of the $X$s and the unknowns,  $\beta$, so the following produces a straight line: 


$$y_{i}=\beta_{0}+\beta_{1}X_{1} + \epsilon $$


and $\epsilon$ are the errors or disturbances. 

## Linear predictions

The predicted points that form the line are $\widehat{y_{i}}$ 

$$\widehat{y_{i}}=\widehat{\beta_{0}}+\widehat{\beta_{1}}X_{1,i}$$


$\widehat{y_{i}}$ is the sum of the $\beta$s multiplied by each value of the appropriate $X_i$, for $i=1 \ldots N$. 

**$\widehat{y_{i}}$ is  referred to as the "linear prediction" or as $x\hat{\beta}$.**

## Residuals

The differences between those predicted points, $\widehat{y_{i}}$ and the observed values  $y_i$ are:

$$
\begin{align}
  \widehat{u} = y_{i}-\widehat{y_{i}} \\ 
= y_{i}-\widehat{\beta_{0}}-\widehat{\beta_{1}}X_{1,i}\\
= y_{i}-\mathbf{x_i\widehat{\beta}}  \nonumber 
\end{align}
$$

These are the residuals, $\widehat{u}$ - the observed measure of the unobserved disturbances, $\epsilon$. 

 
## In matrix notation 
Restating in matrix notation: 

$$
\begin{align}
y_{i} = \mathbf{X_i}  \beta_{k}  +\varepsilon_i \nonumber \\
\widehat{y_{i}}= \mathbf{X_i} \widehat{\beta_{k}}   \nonumber \\
\widehat{u_{i}} = y_i - \mathbf{X_i} \beta_k  \nonumber \\
\widehat{u_i} = y_i - \widehat{y_{i}}  \nonumber
\end{align}
$$ 



## Matrix components

Begin with the estimating equation:

$$
y_{i} = \mathbf{X_i}  \widehat{\beta}  +\epsilon \nonumber
$$

We can solve for $\widehat{\beta}$ by minimizing the sum of the squared errors and arrive at:

$$ \widehat{\beta} = \mathbf{X'X}^{-1} \mathbf{X'}y$$
We'll do this below.

### Components of the estimating equation

$$
\left[
\begin{matrix}
  y_1 \\
  y_2\\
  y_3  \\
  \vdots \\
  y_n    \nonumber
\end{matrix}  \right] 
= \left[
\begin{matrix}
  1& X_{1,2} & X_{1,3} & \cdots & X_{1,k} \\
  1 & X_{2,2} & X_{2,3} &\cdots & X_{2,k} \\
  1 & X_{3,2} & X_{3,3} &\cdots & X_{3,k} \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  1 & X_{n,2} & X_{n,3} & \cdots & X_{n,k}  \nonumber
\end{matrix}  \right] 
\left[
\begin{matrix}
  \beta_1 \\
  \beta_2\\
  \beta_3  \\
  \vdots \\
  \beta_k    \nonumber
\end{matrix}  \right] 
+
\left[
\begin{matrix}
  \epsilon_1 \\
  \epsilon_2\\
  \epsilon_3  \\
  \vdots \\
  \epsilon_n    \nonumber
\end{matrix}  \right] 
$$


###  Components - Covariation $\mathbf{X'X}$

$$
\mathbf{X'X}= \left[
\begin{matrix}
  N& \sum X_{2,i} & \sum X_{3,i} & \cdots & \sum X_{k,i} \\
  \sum X_{2,i}&\sum X_{2,2}^{2} & \sum X_{2,i} X_{3,i} &\cdots & \sum X_{2,i}X_{k,i} \\
  \sum X_{3,i} & \sum X_{3,i}X_{2,i}& \sum X_{3,i}^{2} &\cdots & \sum X_{3,i}X_{k,i} \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  \sum X_{k,i} & \sum X_{k,i}X_{2,i} & \sum X_{k,i}X_{3,i} & \cdots & \sum X_{k,i}^{2}  \nonumber
\end{matrix}  \right] 
$$

\noindent In $\mathbf{X'X}$, the main diagonal is the sums of squares and the offdiagonals are the cross-products. 
 

### Components - Covariation $\mathbf{X'y}$

$$
\mathbf{X'y}= \left[
\begin{matrix}
\sum Y_{i}\\
\sum X_{2,i}Y_{i}\\
\sum X_{3,i}Y_{i}\\
\vdots \\
\sum X_{k,i}Y_{i} \nonumber
\end{matrix}  \right] 
$$

When we compute $\widehat{\beta}$, $\mathbf{X'y}$ is the covariation of $X$ and $Y$, and we pre-multiply by the inverse of $\mathbf{(X'X)^{-1}}$ to control for the relationship between $X_{1}$, $X_{2}$, etc.


# Deriving the OLS estimator

## Deriving $\hat{\beta}$

Start with:
$$y = X\beta + \epsilon$$

Minimize sum of squared errors:
$$\min_{\beta} \epsilon'\epsilon = \min_{\beta} (y - X\beta)'(y - X\beta)$$

Expand:
$$(y - X\beta)'(y - X\beta) = y'y - y'X\beta - \beta'X'y + \beta'X'X\beta$$

Simplify using symmetry ($y'X\beta = \beta'X'y$ as they're scalars):
$$= y'y - 2\beta'X'y + \beta'X'X\beta$$

Take derivative with respect to $\beta$ and set to zero:
$$\frac{\partial}{\partial\beta}(y'y - 2\beta'X'y + \beta'X'X\beta) = 0$$
$$-2X'y + 2X'X\beta = 0$$

Solve for $\beta$:
$$X'X\beta = X'y$$
$$\hat{\beta} = (X'X)^{-1}X'y$$

## Variance-Covariance Matrix

Start with $\hat{\beta} = (X'X)^{-1}X'y$ and substitute $y = X\beta + \epsilon$:

$$\hat{\beta} = (X'X)^{-1}X'(X\beta + \epsilon)$$
$$= \beta + (X'X)^{-1}X'\epsilon$$

Therefore:
$$\hat{\beta} - \beta = (X'X)^{-1}X'\epsilon$$

The variance-covariance matrix is:
$$Var(\hat{\beta}) = E[(\hat{\beta} - \beta)(\hat{\beta} - \beta)']$$
$$= E[(X'X)^{-1}X'\epsilon\epsilon'X(X'X)^{-1}]$$

Under homoskedasticity ($E[\epsilon\epsilon'] = \sigma^2I$):
$$Var(\hat{\beta}) = \sigma^2(X'X)^{-1}$$

Recall that $\sigma^2 = \epsilon'\epsilon / (N - k)$, where $N$ is the number of observations and $k$ is the number of regressors including the constant.


## Variance-Covariance of $\widehat{\beta}$

 
$$ E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)'] = $$ $~$



$$\left[
\begin{array}{cccc}
var(\beta_1) & cov(\beta_1,\beta_2) &\cdots &cov(\beta_1,\beta_k)\\
cov(\beta_2,\beta_1)& var(\beta_2) &\cdots &cov(\beta_2,\beta_k)\\
\vdots&\vdots&\ddots& \vdots\\
cov(\beta_k,\beta_1) &cov(\beta_2,\beta_k) &\cdots & var(\beta_k)\\
\end{array} \right] $$


## Standard Errors of $\beta_k$ 

$$
\left[
\begin{array}{cccc}
\sqrt{var(\beta_1)} & cov(\beta_1,\beta_2) &\cdots &cov(\beta_1,\beta_k)\\
cov(\beta_2,\beta_1)& \sqrt{var(\beta_2)} &\cdots &cov(\beta_2,\beta_k)\\
\vdots&\vdots&\ddots& \vdots\\
cov(\beta_k,\beta_1) &cov(\beta_k,\beta_2) &\cdots & \sqrt{var(\beta_k)}\\
\end{array} \right] 
$$


# Properties of OLS

Return to the normal equation:

$$
\begin{align}
\widehat{\mathbf{\beta}}=\mathbf{(X'X)^{-1}} \mathbf{X'y} \\ \\
\mathbf{(X'X)} \widehat{\mathbf{\beta}} = \mathbf{X'y} \nonumber \\ \\
\mathbf{(X'X)} \widehat{\mathbf{\beta}}  = \mathbf{X'(X\widehat{\beta}} + \mathbf{\widehat{\epsilon}}) \nonumber \\ \\
\mathbf{X' \widehat{\epsilon}}  = 0 \nonumber
\end{align}
$$



## Property 1

If $X' \widehat{\epsilon}  = 0$ holds, then the following properties exist:

::: {.callout-note icon="false"}
### Proposition

Each $x$ variable (each column vector of $\mathbf{X}$) is uncorrelated with $\epsilon$. 

:::


## Property 2

Assuming a constant in the matrix $\mathbf{X}$,

::: {.callout-note icon="false"}
### Proposition

$\sum\limits_{i-1}^{n} \epsilon_i = 0$


because each element of the matrix $X' \widehat{\epsilon}$ would be nonzero due to the constant; only by multiplying by $\epsilon_i=0$ would each element equal zero, and then the sum must be zero. 
:::

## Property 3

::: {.callout-note icon="false"}
### Proposition

The mean of the residuals is zero.

If the sum of the residuals is zero, that sum divided by $N$ must also be zero.
:::

## Property 4

::: {.callout-note icon="false"}
### Proposition
The regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, 
$\mathbf{X}$ and $y$.

$$
\begin{align}
\epsilon = y - X\widehat{\beta} \nonumber \\ \\
\text{multiplying by} ~  N^{-1} \nonumber \\ \\
\bar{\epsilon} = \bar{y} - \bar{X} \widehat{\beta} = 0  \nonumber 
\end{align}
$$

$\bar{y} - \bar{X} \widehat{\beta}=0$ implies $\bar{y} = \bar{X} \widehat{\beta}$, and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane $\bar{y} - \bar{X} \widehat{\beta}$, the mean of the residuals equals zero, implying the regression line or hyperplane passes through it. 
:::


## Properties

Note that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about $\widehat{\beta}$, and inferences about those estimates. 


