[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PLSC 501, Spring 2025",
    "section": "",
    "text": "This is the course website for PLSC 501 - it is a data science course focused on data management, coding in R, and learning basic regression techniques in OLS and MLE.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/matrix25.html",
    "href": "slides/matrix25.html",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model.\n\n\n\n\nA single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A.\n\n\n\nThe dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar.\n\n\n\n\nA is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]\n\n\n\n\n\n\nA rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3).\n\n\n\nA vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "slides/matrix25.html#matrix-notation",
    "href": "slides/matrix25.html#matrix-notation",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "For our purposes, matrix algebra is useful for (at least) three reasons:\n\nSimplifies mathematical expressions.\nHas a clear, visual relationship to the data.\nProvides intuitive ways to understand elements of the model."
  },
  {
    "objectID": "slides/matrix25.html#matrices",
    "href": "slides/matrix25.html#matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A single number is known as a scalar.\nA matrix is a rectangular or square array of numbers arranged in rows and columns.\n\\[\\mathbf{A} = \\left[\n\\begin{array}{cccc}\na_{11} & a_{12} &\\cdots &a_{1m}\\\\\na_{21}& a_{22} &\\cdots &a_{2m}\\\\\n&&\\vdots\\\\\na_{n1} &a_{n2} &\\cdots & a_{nm}\\\\\n\\end{array} \\right] \\]\nwhere each element of the matrix is written as \\(a_{row,column}\\). Matrices are denoted by capital, bold faced letters, A."
  },
  {
    "objectID": "slides/matrix25.html#dimensions",
    "href": "slides/matrix25.html#dimensions",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "The dimensions of a matrix are the number of rows (n) and columns (m) it contains. The dimensions of matrices are always read \\(n\\) by \\(m\\), row by column. We would say that matrix A is of order \\((n,m)\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nA is of order (3,3) and is a square matrix. If matrix A is of order (1,1), then it is a scalar."
  },
  {
    "objectID": "slides/matrix25.html#symmetric-matrices",
    "href": "slides/matrix25.html#symmetric-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A is a square matrix if \\(n=m\\). This matrix is also symmetric.\nA symmetric matrix is one where each element \\(a_{n,m}\\) is equal to its opposite element, \\(a_{m,n}\\). In other words, a matrix is symmetric if \\(a_{n,m}=a_{n,m}\\), \\(\\forall\\) \\(n\\) and \\(m\\).\nAll symmetric matrices are square, but not all square matrices are symmetric. A correlation matrix is and example of a symmetric matrix.\n\n\nDiagonal matrices have zeros for all off-diagonal elements.\n\nDiagonalScalarIdentity\n\n\nDiagonal matrices only have non-zero elements on the main diagonal (from upper left to lower right). That is, \\(a_{n,m}=0\\) if \\(n \\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 3& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nB is a special kind of diagonal matrix known as a scalar matrix because all of the elements on the main diagonal are equal to each other; \\(a_{n,m}=k\\) if \\(n=m\\), else, zero.\n\\[\\mathbf{B} =  \\left[\n\\begin{array}{ccc}\n2 &0 &0\\\\\n0& 2& 0\\\\\n0& 0& 2\\\\\n\\end{array} \\right] \\]\n\n\nC is a special kind of scalar matrix called the identity matrix; the diagonal elements of this matrix are all equal to 1 (\\(a_{n,m}=1\\) if \\(n=m\\), else, zero). This is useful in some matrix manipulations we’ll talk about later; it is denoted I.\n\\[\\mathbf{C} =  \\left[\n\\begin{array}{ccc}\n1 &0 &0\\\\\n0& 1 & 0\\\\\n0& 0& 1\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "slides/matrix25.html#rectangular-matrices",
    "href": "slides/matrix25.html#rectangular-matrices",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A rectangular matrix is one where \\(n\\neq m\\).\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right] \\]\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5& 2\\\\\n\\end{array} \\right] \\]\nIn the first, case, A is of order (3,2); in the second, A is of order (2,3)."
  },
  {
    "objectID": "slides/matrix25.html#vectors",
    "href": "slides/matrix25.html#vectors",
    "title": "Matrix algebra basics",
    "section": "",
    "text": "A vector is a special kind of matrix wherein one of its dimensions is 1; the matrix has either one row or one column. Vectors are denoted by lower case, bold faced letters, a and may be row or column vectors.\n\\[\\mathbf{\\beta} =  \\left[\n\\begin{array}{cccc}\n\\beta_{1} &\\beta_{2} &\\beta_{3}&\\beta_{4}\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{a} =  \\left[\n\\begin{array}{c}\n1 \\\\\n3\\\\\n5\\\\\n7\\\\\n\\end{array} \\right] \\]\nParameter matrices (e.g. \\(\\beta\\)) follow the same notation except that they are indicated by Greek rather than Roman letters."
  },
  {
    "objectID": "slides/matrix25.html#transposition",
    "href": "slides/matrix25.html#transposition",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nThe transpose of a matrix A is the matrix flipped on its side such that its rows become columns and columns become rows; the transpose of A is denoted \\({\\mathbf A'}\\) and is referred to as “\\({\\mathbf A}\\) transpose.”\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{cc}\n1 &3 \\\\\n3& 5\\\\\n7& 2\\\\\n\\end{array} \\right]\n~~~~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n1 &3 &7\\\\\n3& 5 & 2\\\\\n\\end{array} \\right] \\]\nso \\({\\mathbf X}\\), a (3,2) matrix, becomes \\({\\mathbf X'}\\), a (2,3) matrix."
  },
  {
    "objectID": "slides/matrix25.html#transposition-1",
    "href": "slides/matrix25.html#transposition-1",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nIf \\({\\mathbf A}\\) is symmetric, then \\({\\mathbf A}={\\mathbf A'}\\) – take a look at the following symmetric matrix and you’ll see why:\n\\[\\mathbf{X} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n~~~~~~~~\n\\mathbf{X'} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "slides/matrix25.html#transposition-2",
    "href": "slides/matrix25.html#transposition-2",
    "title": "Matrix algebra basics",
    "section": "Transposition",
    "text": "Transposition\nAlso note that the transpose of a transposed matrix results in the original matrix: \\((\\mathbf{X}')'=\\mathbf{X}\\).\nTransposing a row vector results in a column vector and vice versa:\n\\[\\mathbf{x} =  \\left[\n\\begin{array}{c}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n\\end{array} \\right]\n~~~~~~~~~\n\\mathbf{x'} =  \\left[\n\\begin{array}{cccc}\n-1 & -9 &4 &16\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "slides/matrix25.html#trace-of-a-matrix",
    "href": "slides/matrix25.html#trace-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nThe trace of a matrix is the sum of the diagonal elements of a square matrix, and is denoted \\(tr(\\mathbf{A})=a_{11}+a_{22}+a_{33}\\ldots+a_{nn} = \\sum\\limits_{i=1}^{n}a_{ii}\\)\n\\[\\mathbf{A} =  \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right] \\]\nsuch that\n\\(tr(\\mathbf{A})=2+5+2=9\\)"
  },
  {
    "objectID": "slides/matrix25.html#addition-subtraction",
    "href": "slides/matrix25.html#addition-subtraction",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\n\nAddition and subtraction of matrices is analogous to the same scalar operations, but depend on the conformatibility of the matrices to be added or subtracted.\nTwo matrices are conformable for addition or subtraction iff they are of the same order.\nNote that conformability means something different for addition/subtraction than for multiplication."
  },
  {
    "objectID": "slides/matrix25.html#addition-subtraction-1",
    "href": "slides/matrix25.html#addition-subtraction-1",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider the following (2,2) matrices and the problem \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{C}\\):\n\\[ \\left[\n\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21}& a_{22} \\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{cc}\nb_{11} & b_{12}\\\\\nb_{21}& b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21}& a_{22}+b_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\nc_{11} & c_{12}\\\\\nc_{21}& c_{22} \\\\\n\\end{array} \\right] \\]\nAddition and subtraction are only possible among matrices that share the same order; otherwise, they are nonconformable."
  },
  {
    "objectID": "slides/matrix25.html#addition-subtraction-2",
    "href": "slides/matrix25.html#addition-subtraction-2",
    "title": "Matrix algebra basics",
    "section": "Addition & Subtraction",
    "text": "Addition & Subtraction\nConsider a second example of addition, and note that subtraction would follow directly:\n\\[ \\left[\n\\begin{array}{ccc}\n2 &4 &8\\\\\n4& 5& 3\\\\\n8& 3& 2\\\\\n\\end{array} \\right]\n+\n\\left[\n\\begin{array}{ccc}\n1 &5 &7\\\\\n3& 7& 1\\\\\n2& 4& 9\\\\\n\\end{array} \\right]\n=\n  \\left[\n\\begin{array}{ccc}\n3 &9 &15\\\\\n7& 12& 4\\\\\n10& 7& 11\\\\\n\\end{array} \\right] \\]\nMatrix addition adheres to the commutative and associative properties such that:\n\\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) (Commutative), and \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}= \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) (Associative)."
  },
  {
    "objectID": "slides/matrix25.html#multiplication",
    "href": "slides/matrix25.html#multiplication",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\nLet’s begin by multiplying a scalar and a matrix - the product is a matrix whose elements are the scalar multiplied by each element of the original matrix, so:\n\\[ \\beta\n\\left[\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right]\n=\n\\left[\n\\begin{array}{cc}\n\\beta x_{11} & \\beta x_{12}\\\\\n\\beta x_{21}&  \\beta x_{22} \\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "slides/matrix25.html#multiplication-1",
    "href": "slides/matrix25.html#multiplication-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication",
    "text": "Multiplication\n\nIn order to multiply two matrices (or vectors), they must be conformable for multiplication.\nTwo matrices are conformable for multiplication iff the number of columns in the first matrix is equal to the number of rows in the second matrix. That is, \\(\\mathbf{A_{i,j}}\\) and \\(\\mathbf{B_{j,k}}\\).\nAn easy way to think of this is to write the dimensions of the two matrices, (i,j),(j,k) - the inner dimensions are the same, so the matrix is conformable for multiplication - moreover, the outer dimensions (i,k) give the dimension of the product matrix."
  },
  {
    "objectID": "slides/matrix25.html#multiplication---inner-product",
    "href": "slides/matrix25.html#multiplication---inner-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\n\nTo illustrate multiplication, let’s start with a column vector, \\(\\mathbf{e}\\) whose order is (N,1) and take its inner product.\nThe inner product of a column vector is the transpose of the column vector (thus, a row vector) post-multiplied by the column vector, so \\({\\mathbf e'\\mathbf e}\\). The inner product is a scalar."
  },
  {
    "objectID": "slides/matrix25.html#multiplication---inner-product-1",
    "href": "slides/matrix25.html#multiplication---inner-product-1",
    "title": "Matrix algebra basics",
    "section": "Multiplication - inner product",
    "text": "Multiplication - inner product\nWhen we transpose the column vector \\({\\mathbf e}\\) of (N,1) order, we get a row vector \\({\\mathbf e'}\\) of (1,N) order. Inner dimensions match, so they are conformable.\n\\[\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n~~~~~\n\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right] \\] \\[=\n(-1 \\cdot -1)+(-9 \\cdot -9)+(4 \\cdot 4)+ (16 \\cdot 16)+ (-10 \\cdot -10) = 454\\]"
  },
  {
    "objectID": "slides/matrix25.html#least-squares-foreshadowing",
    "href": "slides/matrix25.html#least-squares-foreshadowing",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nNote the inner product of a column vector is a scalar; the inner product of \\(\\mathbf{e}\\), is the sum of the squares of \\(\\mathbf{e}\\).\n\\[\\mathbf{e'e}= e_{1}e_{1}+e_{2}e_{2}+\\ldots e_{N}e_{N} = \\sum\\limits_{i=1}^{N}e_{i}^{2}\\]"
  },
  {
    "objectID": "slides/matrix25.html#multiplication---outer-product",
    "href": "slides/matrix25.html#multiplication---outer-product",
    "title": "Matrix algebra basics",
    "section": "Multiplication - outer product",
    "text": "Multiplication - outer product\nThe outer product of a column vector is the transpose of the column vector (thus, a row vector) pre-multiplied by the column vector, so \\({\\mathbf e\\mathbf e'}\\). The outer product is an (N,N) matrix.\n\\[\\mathbf{e} =  \\left[\n\\begin{array}{r}\n-1 \\\\\n-9\\\\\n4\\\\\n16\\\\\n-10\\\\\n\\end{array} \\right]\n\\mathbf{e'} =  \\left[\n\\begin{array}{rrrrr}\n-1 & -9 &4 &16 & -10\\\\\n\\end{array} \\right]\n\\]\nLet’s multiply \\({\\mathbf e\\mathbf e'}\\), a column vector of (5,1) by a row vector of (1,5); we’ll obtain a square matrix of (5,5)."
  },
  {
    "objectID": "slides/matrix25.html#least-squares-foreshadowing-1",
    "href": "slides/matrix25.html#least-squares-foreshadowing-1",
    "title": "Matrix algebra basics",
    "section": "Least Squares foreshadowing",
    "text": "Least Squares foreshadowing\nLet \\(\\mathbf{e}\\) represent the residuals or errors in our regression. The outer product\n\\[{\\mathbf e\\mathbf e'} =  \\left[\n\\begin{array}{rrrrr}\n1 & 9 &-4 &-16 &10\\\\\n9 & 81 &-36 &-144 &90\\\\\n-4 & -36 &16 &64 &-40\\\\\n-16 & -144 &64 &256 &-160\\\\\n10 & 90 &-40 &-160 &100\\\\\n\\end{array} \\right] \\]\nis the variance-covariance matrix of \\(\\mathbf{e}\\). The squares on the main diagonal (which sums to the sum of squares) and the symmetry of the off-diagonal elements."
  },
  {
    "objectID": "slides/matrix25.html#inverting-matrices-1",
    "href": "slides/matrix25.html#inverting-matrices-1",
    "title": "Matrix algebra basics",
    "section": "Inverting Matrices",
    "text": "Inverting Matrices\n\nYou’ll have noticed that division has been conspicuously absent so far. In scalar algebra, we sometimes represent division in fractions, \\(\\frac{1}{2}\\).\nAnother way to represent the same quantity is \\(2^{-1}\\), or the inverse of 2. Of course, in scalar algebra, a number multiplied by its inverse equals 1, e.g., \\(2 \\cdot 2^{-1}=1\\) or \\(2 \\cdot \\frac{1}{2}=1\\). So, if we are given \\(4x=1\\) and want to solve for \\(x\\) what we really want to know is “what is the inverse of 4 such that \\(4 \\cdot 4^{-1}=1\\)?” We consider matrices in a similar manner."
  },
  {
    "objectID": "slides/matrix25.html#inverting-square-matrices",
    "href": "slides/matrix25.html#inverting-square-matrices",
    "title": "Matrix algebra basics",
    "section": "Inverting Square Matrices",
    "text": "Inverting Square Matrices\nImagine a square matrix, \\(\\mathbf{X}\\) and its inverse, denoted \\({\\mathbf{X^{-1}}}\\) (read as “X inverse”). Analogous to scalar algebra, a matrix multiplied by its inverse is equal to the identity matrix, \\(\\mathbf{I}\\).\nSo in order to find \\({\\mathbf{X^{-1}}}\\), we must find the matrix that, when multiplied by \\(\\mathbf{X}\\), produces \\(\\mathbf{I}\\). Thus, for a square matrix, its inverse (if it exists) is such that\n\\[\\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X^{-1}} \\mathbf{X}=\\mathbf{I}\\]\nNotice the commutative property at work here - this is because \\(\\mathbf{X}\\) is square, such that \\(n=m\\), so \\(\\mathbf{X}\\) and \\({\\mathbf{X^{-1}}}\\) have the same dimensions."
  },
  {
    "objectID": "slides/matrix25.html#determinant",
    "href": "slides/matrix25.html#determinant",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nAn important characteristic of square matrices is the determinant. The determinant, denoted \\(|X|\\), is a scalar; every square matrix has one. We evaluate the determinant by examining the cross-products of the matrice’s elements. This is simple in the 2x2 matrix, less so in larger matrices.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\nx_{11} & x_{12}\\\\\nx_{21}& x_{22} \\\\\n\\end{array} \\right| = x_{11}x_{22}-x_{12}x_{21}\\]"
  },
  {
    "objectID": "slides/matrix25.html#determinant-1",
    "href": "slides/matrix25.html#determinant-1",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nIn the 2x2 matrix, the determinant is the cross-product of the main diagonals minus the cross-product of the off-diagonals.\n\\[ |\\mathbf{X_{1}}|=\\left|\n\\begin{array}{cc}\n2 & 4\\\\\n6& 3 \\\\\n\\end{array} \\right| = 2 \\cdot 3-4 \\cdot 6 = -18\n\\]\nThe determinant is -18; \\(|\\mathbf{X_{1}}|=-18\\)."
  },
  {
    "objectID": "slides/matrix25.html#determinant-2",
    "href": "slides/matrix25.html#determinant-2",
    "title": "Matrix algebra basics",
    "section": "Determinant",
    "text": "Determinant\nThis matrix has a determinant of zero - this is an important case.\n\\[ |\\mathbf{X_{2}}|=\\left|\n\\begin{array}{cc}\n3 & 6\\\\\n2& 4 \\\\\n\\end{array} \\right| = 3 \\cdot 4-6 \\cdot 2 =0\n\\] A matrix with determinant zero is singular. A singluar matrix has no inverse."
  },
  {
    "objectID": "slides/matrix25.html#singular-matrices",
    "href": "slides/matrix25.html#singular-matrices",
    "title": "Matrix algebra basics",
    "section": "Singular matrices",
    "text": "Singular matrices\nThere are some important conditions that will produce a determinant of zero and thus a singular matrix:\n\nIf all the elements of any row or column of a matrix are equal to zero, then the determinant is zero.\nIf two rows or columns of a matrix are identical, the determinant is zero.\nIf a row or column of a matrix is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "slides/matrix25.html#least-squares-foreshadowing-2",
    "href": "slides/matrix25.html#least-squares-foreshadowing-2",
    "title": "Matrix algebra basics",
    "section": "Least Squares Foreshadowing",
    "text": "Least Squares Foreshadowing\nFor the linear model in least squares, this is important because computing estimates of \\(\\beta\\) requires inverting a matrix. Let’s derive \\(\\beta\\) in matrix notation to see how:\nThe estimated model is:\n\\[\\mathbf{y}={\\mathbf X{\\widehat{\\beta}}}+\\widehat{\\mathbf{\\epsilon}}\\] Minimizing \\(\\widehat{\\mathbf{\\epsilon}}'\\widehat{\\mathbf{\\epsilon}}\\) (skipping the math for now), we get\n\\[(\\mathbf{X'X}) \\widehat{\\beta}=\\mathbf{X'y}\\]"
  },
  {
    "objectID": "slides/matrix25.html#foreshadowing-least-squares",
    "href": "slides/matrix25.html#foreshadowing-least-squares",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThe matrix \\((\\mathbf{X'X})\\) gives the sums of squares (on the main diagonal) and the sums of cross products (in the off-diagonals) of all the \\(\\mathbf{X}\\) variables; the matrix is symmetric. Since \\(\\beta\\) is the unknown vector in this equation, solve for \\(\\beta\\) by dividing both sides by \\((\\mathbf{X'X})\\) - in matrix terms, we premultiply each side by \\((\\mathbf{X'X)^{-1}}\\):\n\\[(\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) \\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]"
  },
  {
    "objectID": "slides/matrix25.html#foreshadowing-least-squares-1",
    "href": "slides/matrix25.html#foreshadowing-least-squares-1",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nAs we know, \\((\\mathbf{X'X)^{-1}} (\\mathbf{X'X}) = \\mathbf{I}\\). So,\n\\[\\mathbf{I}\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\] or \\[\\widehat{\\beta}= (\\mathbf{X'X)^{-1}} \\mathbf{X'y}\\]\nEstimating \\(\\widehat{\\beta}\\) requires inverting \\(\\mathbf{(X'X)}\\) If the matrix \\(\\mathbf{(X'X)}\\) is singular, then \\(\\mathbf{(X'X)^{-1}}\\) does not exist, and we cannot estimate \\(\\widehat{\\beta}\\). Least Squares fails."
  },
  {
    "objectID": "slides/matrix25.html#foreshadowing-least-squares-2",
    "href": "slides/matrix25.html#foreshadowing-least-squares-2",
    "title": "Matrix algebra basics",
    "section": "Foreshadowing Least Squares",
    "text": "Foreshadowing Least Squares\nThinking specifically of the \\(\\mathbf{X}\\) variables in the model, any of these conditions produce perfect collinearity - estimation fails because the matrix can’t invert.\n\nIf all the elements of any row or column of \\(\\mathbf{X}\\) are equal to zero, then the determinant is zero.\nIf two rows or columns of \\(\\mathbf{X}\\) are identical, the determinant is zero.\nIf a row or column of \\(\\mathbf{X}\\) is a multiple of another row or column, the determinant is zero; if one row or column is a linear combination of other rows or columns, the determinant is zero."
  },
  {
    "objectID": "slides/matrix25.html#examples",
    "href": "slides/matrix25.html#examples",
    "title": "Matrix algebra basics",
    "section": "Examples",
    "text": "Examples\n\nExample 1Example 2Example 3\n\n\n\\({\\mathbf X}\\) below has a row of zeros; compute the determinant using the difference of cross-products.\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n0 & 0\\\\\n19& 12 \\\\\n\\end{array} \\right| = 0 \\cdot 12-0 \\cdot 19 = 0 \\]\nYou can see in \\({\\mathbf X}\\) that because all the cross-products involve multiplying by zero, the determinant will always be zero; this applies to larger matrices as well.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n5 & 7\\\\\n5& 7 \\\\\n\\end{array} \\right| = 5 \\cdot 7-5 \\cdot 7 =0 \\]\nIn \\({\\mathbf X}\\) it’s easy to see that the cross-products will always be equal to one another if the rows or columns are identical, and the difference between identical values is zero.\n\n\n\\[ |\\mathbf{X}|=\\left|\n\\begin{array}{cc}\n64 & 48\\\\\n16& 12 \\\\\n\\end{array} \\right| = 64 \\cdot 12-48 \\cdot 16 = 768-768=0\\]\nFinally \\({\\mathbf X}\\) shows that if one row or column is a linear combination of other rows or columns, the determinant will be zero; in that matrix, the top row is 4 times the bottom row."
  },
  {
    "objectID": "slides/matrix25.html#inversion",
    "href": "slides/matrix25.html#inversion",
    "title": "Matrix algebra basics",
    "section": "Inversion",
    "text": "Inversion\nThe matrix we need to invert, \\(\\mathbf{X'X}\\) is rectangular, so inversion is more complex. I’m going to illustrate the method of cofactor expansion - the purpose here is to give you an idea what software does every time you estimate an OLS model."
  },
  {
    "objectID": "slides/matrix25.html#minor-of-a-matrix",
    "href": "slides/matrix25.html#minor-of-a-matrix",
    "title": "Matrix algebra basics",
    "section": "Minor of a matrix",
    "text": "Minor of a matrix\nThe minor of a matrix is the determinant of a submatrix created by deleting the \\(i\\)th row and the \\(j\\)th column of the full matrix, where the minor is denoted by the element where the deleted rows intersect.\n\\[ \\mathbf{A} =  \\left[\n\\begin{array}{ccc}\na_{11} & a_{12} &a_{13}\\\\\na_{21}& a_{22} &a_{23}\\\\\na_{31} &a_{32} & a_{33}\\\\\n\\end{array} \\right] \\] \\[\\text{so the minor of } a_{11} \\text{ is }\n|\\mathbf{M_{11}}| =  \\left|\n\\begin{array}{cc}\na_{22} &a_{23}\\\\\na_{32} & a_{33}\\\\\n\\end{array} \\right|  =  a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\]"
  },
  {
    "objectID": "slides/matrix25.html#cofactor-matrix",
    "href": "slides/matrix25.html#cofactor-matrix",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nThe cofactor of an element (\\(c_{ij}\\)) is the minor with a positive or negative sign depending on whether \\(i+j\\) is odd (negative) or even (positive). This is given by:\n\\[\\theta_{ij}=(-1)^{i+j}|\\mathbf{M_{ij}}|\\]\nso, from the example finding the minor of \\(a_{11}\\), \\(i\\)=1 and \\(j=1\\); their sum is 2 which is even, so the cofactor is \\(+a_{22} \\cdot a_{33}- a_{23} \\cdot a_{32}\\). If we find the cofactor for every element, \\(a_{ij}\\) in a matrix and replace each element with its cofactor, the new matrix is called the cofactor matrix of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "slides/matrix25.html#cofactor-matrix-1",
    "href": "slides/matrix25.html#cofactor-matrix-1",
    "title": "Matrix algebra basics",
    "section": "Cofactor Matrix",
    "text": "Cofactor Matrix\nWhat we have so far is the signed matrix of minors:\n\\[\\mathbf{A} = \\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "slides/matrix25.html#cofactor-expansion",
    "href": "slides/matrix25.html#cofactor-expansion",
    "title": "Matrix algebra basics",
    "section": "Cofactor Expansion",
    "text": "Cofactor Expansion\nFind the signed determinant of each 2x2 submatrix.\n\\[\\mathbf{\\Theta_{A}} \\left[\n\\begin{array}{cccccc}\n~\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n3&1\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n3&3\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n-\\left|  \\begin{array}{cc}\n4&2\\\\\n2&4\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&2\\\\\n2&4\\\\\n\\end{array} \\right|\n-\\left|  \\begin{array}{cc}\n1&4\\\\\n2&2\\\\\n\\end{array} \\right|\\\\ \\\\\n~\\left|\\begin{array}{cc}\n4&2\\\\\n3&1\\\\\n\\end{array} \\right|\n-\\left| \\begin{array}{cc}\n1&2\\\\\n3&1\\\\\n\\end{array} \\right|\n~\\left|\\begin{array}{cc}\n1&4\\\\\n3&3\\\\\n\\end{array} \\right|\n\\end{array} \\right]\n\\mathbf{\\Theta_{A}} = \\left[\n\\begin{array}{rrr}\n10&-10&0\\\\\n-12&0&6\\\\\n-2&5&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "slides/matrix25.html#find-the-determinant",
    "href": "slides/matrix25.html#find-the-determinant",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nUsing the cofactor matrix, \\(\\mathbf{\\Theta_{A}}\\) we perform cofactor expansion in order to find the determinant, \\(|\\mathbf{A}|\\). Cofactor expansion is given by:\n\\[|\\mathbf{A}|=\\sum\\limits_{i,j=1}^{n}a_{ij}\\Theta_{ij}\\]\nwhich means that we take two corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) and sum the products of their elements - this gives us the determinant of \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "slides/matrix25.html#find-the-determinant-1",
    "href": "slides/matrix25.html#find-the-determinant-1",
    "title": "Matrix algebra basics",
    "section": "Find the Determinant",
    "text": "Find the Determinant\nSo taking the first row from \\(\\mathbf{A}\\) (1,4,2) and from \\(\\mathbf{\\Theta_{A}}\\) (10,-10,0) above, we compute\n\\[a_{11} \\cdot \\Theta_{A11}+a_{12} \\cdot \\Theta_{A12}+a_{13} \\cdot \\Theta_{A13}\\]\n\\[1 \\cdot 10+ 4 \\cdot -10 + 2 \\cdot 0 = -30\\] This is the determinant of \\(\\mathbf{A}\\) , \\(|\\mathbf{A}|\\). Note that any corresponding rows or columns from \\(\\mathbf{A}\\) and \\(\\mathbf{\\Theta_{A}}\\) will produce the same result."
  },
  {
    "objectID": "slides/matrix25.html#adjoint-matrix",
    "href": "slides/matrix25.html#adjoint-matrix",
    "title": "Matrix algebra basics",
    "section": "Adjoint Matrix",
    "text": "Adjoint Matrix\nIf we transpose the cofactor matrix, (cof \\(\\mathbf{A'}\\)), the new matrix is called the adjoint matrix, denoted adj\\(\\mathbf{A}\\)=(cof \\(\\mathbf{A'}\\)).\n\\[adj\\mathbf{A} = \\mathbf{\\Theta_{A}'} = \\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "slides/matrix25.html#inverting-the-matrix",
    "href": "slides/matrix25.html#inverting-the-matrix",
    "title": "Matrix algebra basics",
    "section": "Inverting the matrix",
    "text": "Inverting the matrix\nBelieve it or not, this all leads us to inverting the matrix, provided it is nonsingular.\n\\[\\mathbf{A^{-1}}=\\frac{1}{|\\mathbf{A}|}(adj {\\mathbf A})\\] The inverse of A is equal to one over the determinant of A times the adjoint matrix of A. So we’re pre-multiplying \\(adj {\\mathbf A}\\) by the (scalar) determinant, -30."
  },
  {
    "objectID": "slides/matrix25.html#inverse-of-matrix-a",
    "href": "slides/matrix25.html#inverse-of-matrix-a",
    "title": "Matrix algebra basics",
    "section": "Inverse of Matrix A",
    "text": "Inverse of Matrix A\n\\[{\\mathbf A^{-1}}=-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n=\\left[\n\\begin{array}{rrr}\n-\\frac{1}{3}&\\frac{2}{5}&\\frac{1}{15}\\\\\n\\frac{1}{3}&0&-\\frac{1}{6}\\\\\n0&-\\frac{1}{5}&\\frac{3}{10}\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "slides/matrix25.html#checking-our-work",
    "href": "slides/matrix25.html#checking-our-work",
    "title": "Matrix algebra basics",
    "section": "Checking our work",
    "text": "Checking our work\nWe can check our work to be sure we’ve inverted correctly by making sure that \\(\\mathbf{A^{-1}A}=\\mathbf{I_{3}}\\); so,\n\\[-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n10&-12&-2\\\\\n-10&0&5\\\\\n0&6&-9\\\\\n\\end{array} \\right]\n\\left[\n\\begin{array}{ccc}\n1&4&2\\\\\n3&3&1\\\\\n2&2&4\\\\\n\\end{array} \\right]\n=\n-\\frac{1}{30}\\left[\n\\begin{array}{rrr}\n-30&0&0\\\\\n0&-30&0\\\\\n0&0&-30\\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "slides/matrix25.html#connecting-data-matrices-to-ols",
    "href": "slides/matrix25.html#connecting-data-matrices-to-ols",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\nLet’s examine these matrices a little to get a feel for what the computation of \\(\\widehat{\\beta}\\) involves: \\(\\widehat{\\beta}=(\\mathbf{X'X})^{-1}\\mathbf{X'y}\\).\n\\[\n\\mathbf{X}= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \n\\end{matrix}  \\right]\n\\] The dimensions of \\(\\mathbf{X}\\) are \\(n x k\\); the sample size by the number of regressors (including the constant)."
  },
  {
    "objectID": "slides/matrix25.html#connecting-data-matrices-to-ols-1",
    "href": "slides/matrix25.html#connecting-data-matrices-to-ols-1",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \n\\end{matrix}  \\right] \\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products."
  },
  {
    "objectID": "slides/matrix25.html#connecting-data-matrices-to-ols-2",
    "href": "slides/matrix25.html#connecting-data-matrices-to-ols-2",
    "title": "Matrix algebra basics",
    "section": "Connecting data matrices to OLS",
    "text": "Connecting data matrices to OLS\n\\[\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i}\n\\end{matrix}  \\right] \\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationships among \\(X_k\\)."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Course overview\nMatrix algebra basics\nProbability basics\nA First Look at OLS\nThinking about data\nBivariate model\nBivariate example\n\nPrediction methods\nPrediction methods video\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/overview25.html",
    "href": "slides/overview25.html",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise\n\n\n\n\nThis course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding.\n\n\n\nIn May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling.\n\n\n\n\nIs not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal.\n\n\n\n\nis aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here.\n\n\n\n\n\nis a collaborative effort among you, and with me and Oguzhan. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together.\n\n\n\n\nWhat do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra\n\n\n\n\nYou should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^{-1}\\) and \\(X'y\\).\n\n\n\n\nwhat are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?\n\n\n\n\nThe class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term.\n\n\n\nPosit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later.\n\n\n\nCan be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "slides/overview25.html#all-models-are-wrong-ldots",
    "href": "slides/overview25.html#all-models-are-wrong-ldots",
    "title": "Course overview",
    "section": "",
    "text": "“All models are wrong, some are useful.” Box & Draper (1987)\nThis course is based on the principle that to build useful models, we need:\n\nan intuitive understanding of regression models\ndata science skills\ncareful and creative thinking about politics\na general skepticism of individual models, but an optimism about the modeling enterprise"
  },
  {
    "objectID": "slides/overview25.html#the-regression-model",
    "href": "slides/overview25.html#the-regression-model",
    "title": "Course overview",
    "section": "",
    "text": "This course focuses on the linear regression model, but with a heavy emphasis on data science skills like data management/wrangling, and coding."
  },
  {
    "objectID": "slides/overview25.html#course-goals",
    "href": "slides/overview25.html#course-goals",
    "title": "Course overview",
    "section": "",
    "text": "In May, you should be able to:\n\ndevelop and test hypotheses about politics.\nestimate, evaluate, and interpret basic linear and nonlinear regression models.\nevaluate, understand, clean, wrangle, join, and reshape data.\nwrite R code, to manage data, implement simulation methods, estimate models, visualize data/predictions.\nunderstand the data generation process as a motivation for modeling."
  },
  {
    "objectID": "slides/overview25.html#this-class",
    "href": "slides/overview25.html#this-class",
    "title": "Course overview",
    "section": "",
    "text": "Is not a math class,\n\n\n\n\n\nbut one that uses basic math toward an applied goal."
  },
  {
    "objectID": "slides/overview25.html#this-class-1",
    "href": "slides/overview25.html#this-class-1",
    "title": "Course overview",
    "section": "",
    "text": "is aimed at application; the more you struggle with code, the more you’ll learn.\nthe more you use other people’s code, the less you’ll learn.\nthe more you look at other people’s code, the more you’ll learn. You should live on stackoverflow etc.\nis applied - if you apply the tools we cover, you’ll learn a lot; if not, you won’t.\nthe students who do best later on in this program challenge themselves here."
  },
  {
    "objectID": "slides/overview25.html#this-class-2",
    "href": "slides/overview25.html#this-class-2",
    "title": "Course overview",
    "section": "",
    "text": "is a collaborative effort among you, and with me and Oguzhan. We learn from each other when we collaborate.\nThis means having honest conversations in class about what we do and do not understand.\nIt means working together to learn R, to wrangle data, and to understand models; working together on your assignments.\nthe most successful cohorts are those that work collaboratively; this does not mean free-riding, but struggling with everything together."
  },
  {
    "objectID": "slides/overview25.html#what-do-you-need-to-know-for-501",
    "href": "slides/overview25.html#what-do-you-need-to-know-for-501",
    "title": "Course overview",
    "section": "",
    "text": "What do you need at the start?\n\nbasic R or this terrific bookdown book written by a graduate student for graduate students.\nbasic probability theory\nbasic matrix algebra"
  },
  {
    "objectID": "slides/overview25.html#matrix-algebra",
    "href": "slides/overview25.html#matrix-algebra",
    "title": "Course overview",
    "section": "",
    "text": "You should be able to make sense of this in terms of dimensions and operations:\n\\[\\beta = (X'X)^{-1} X'y\\] where \\(X\\) is a data matrix of rows and columns; these are the variables in a regression. \\(y\\) is the outcome variable, the same number of rows as \\(X\\). Define the dimensions of \\(X\\), \\(y\\), and \\(\\beta\\), of \\((X'X)^{-1}\\) and \\(X'y\\)."
  },
  {
    "objectID": "slides/overview25.html#probability-theory",
    "href": "slides/overview25.html#probability-theory",
    "title": "Course overview",
    "section": "",
    "text": "what are probability distributions?\nwhat is the PDF of a variable?\nwhat is the CDF of a variable?"
  },
  {
    "objectID": "slides/overview25.html#understanding-regression-models",
    "href": "slides/overview25.html#understanding-regression-models",
    "title": "Course overview",
    "section": "",
    "text": "The class focuses on regression models of the general form (scalar notation):\n\\[ y_i = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 \\ldots + X_k\\beta_k + \\epsilon\\]\nor in matrix notation,\n\\[ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\]\nAn outcome, \\(y\\) is a function of some combination of \\(X\\) variables, their coefficients (\\(\\beta\\), including an intercept), and an error term."
  },
  {
    "objectID": "slides/overview25.html#regression-models",
    "href": "slides/overview25.html#regression-models",
    "title": "Course overview",
    "section": "",
    "text": "Posit a statistical relationship between:\n\nan outcome variable, \\(y\\).\na covariate of interest, \\(x\\).\na set of controls, \\(\\mathbf{X}\\).\n\nThe effect of \\(x\\) is denoted \\(\\beta\\). It is the partial effect of x on y, because removed from that effect are the effects of x -&gt; X -&gt; y. This is what we mean by controlling for the effects of X. Much more on this later."
  },
  {
    "objectID": "slides/overview25.html#regression-models-1",
    "href": "slides/overview25.html#regression-models-1",
    "title": "Course overview",
    "section": "",
    "text": "Can be estimated using different technologies including:\n\nLeast Squares\nMaximum Likelihood\nBayesian methods\n\nThis course is about the technology of least squares; we will deal with one ML regression model (logit) later in the semester."
  },
  {
    "objectID": "slides/overview25.html#models-to-understand-politics",
    "href": "slides/overview25.html#models-to-understand-politics",
    "title": "Course overview",
    "section": "Models to understand politics",
    "text": "Models to understand politics\n\\(~\\)\nThis a methods class, but the methods are only meaningful to us insofar as they help us understand politics. So let’s motivate the class with a question about politics."
  },
  {
    "objectID": "slides/overview25.html#careful-thinking",
    "href": "slides/overview25.html#careful-thinking",
    "title": "Course overview",
    "section": "Careful thinking",
    "text": "Careful thinking\nCorrelation does not imply causation.\n\n\n\nRepression during the COVID-19 Pandemic\nAmong the products of the pandemic is an opportunity for governments, under the guise of protecting public health, to repress citizens.\nThere is some evidence of democratic backsliding prior to the pandemic, but the global public health crisis gave governments a specific and universal opportunity to take advantage of this softened ground and to become more authoritarian.\nWe’re going to examine the covariates of violence against civilians during the COVID period, focusing on how states of emergency create changes in government violence.\n\n\n\nData\nLet’s look at ACLED event data, and data on states of emergency during the COVID period from Lundgren et al (2020).\nThe next figure plots protests and repression since mid-2019, the onset of the pandemic marked at March 15, 2020.\nThe blue lines are local regressions showing trends; while protests return to pre-pandemic levels (similar to the years prior), repression increases since the onset of COVID-19.\n\n\n\nProtests\n\n\ncode\nacled&lt;-read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\")\n\n##Event_date variable has a \"Chr\" format. \n#Need to convert it into date format to calculate 7 day averages for Protests and Violence against Civilians\n\nacled$event_date&lt;-as.Date(acled$event_date,format=\"%d %B %Y\")\n#class(acled$event_date)\n\n##Compute number of events per day\n\nacled$perday&lt;-1\n\nprotests&lt;-filter(acled,event_type==\"Protests\" )\nstateviol&lt;-filter(acled, event_type==\"Violence against civilians\")\nprotests&lt;-aggregate(perday ~ event_date, data = protests, sum)\nstateviol&lt;-aggregate(perday ~ event_date, data = stateviol, sum)\n\n\n##Calculating a 7 day moving average\nprotests$rollmean&lt;-rollmean(protests$perday,k=7, fill=NA)\nstateviol$rollmean&lt;-rollmean(stateviol$perday,k=7, fill=NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\n# protestplot &lt;- ggplot(data=protests, aes(x=event_date, y=rollmean)) +\n#   geom_line()+\n#   geom_smooth() +\n#   geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n#   scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n#   labs(x=\"Date\", y=\"Protests\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") + \n#   ggtitle(\"Protests During the Pandemic\") \n#    \n# \n# protestplot\n\n\nlibrary(highcharter)\nlibrary(dplyr)\n\n# First handle potential NA values and ensure data is properly ordered\nprotests &lt;- protests %&gt;%\n  arrange(event_date) %&gt;%\n  # Optionally remove NA values if present\n  filter(!is.na(rollmean), !is.na(event_date))\n\n# Calculate the smoothed trend line with explicit handling of the data points\nx_vals &lt;- as.numeric(protests$event_date)\nsmooth_fit &lt;- loess(rollmean ~ x_vals, data = protests, span = 0.75)\nprotests$smooth_trend &lt;- predict(smooth_fit, x_vals)\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n# Create the highchart\nhchart &lt;- highchart() %&gt;%\n  hc_add_series(\n    data = protests,\n    type = \"line\",\n    hcaes(x = event_date, y = rollmean),\n    color= \"#000000\",\n    name = \"Rolling Average\"\n  ) %&gt;%\n  hc_add_series(\n    data = protests,\n    type = \"line\",\n    hcaes(x = event_date, y = smooth_trend),\n    name = \"Trend\",\n    dashStyle = \"Solid\",\n    color = \"#005A43\",\n    opacity = 0.7\n  ) %&gt;%\n  hc_xAxis(\n    plotLines = list(list(\n      value = as.numeric(as.POSIXct(\"2020-03-15\")) * 1000,\n      color = \"red\",\n      width = 2,\n      zIndex = 3\n    )),\n    type = \"datetime\",\n    dateTimeLabelFormats = list(month = \"%b %Y\"),\n    tickInterval = 6 * 30 * 24 * 3600 * 1000\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Protests\")\n  ) %&gt;%\n  hc_title(\n    text = \"Protests During the Pandemic\"\n  ) %&gt;%\n  hc_caption(\n    text = \"7 day rolling averages; smoothed loess trend lines; red line indicates 3.15.2020 COVID onset; ACLED data retrieved 7.18.22\",\n    align = \"left\",\n    style = list(fontStyle = \"italic\")\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_credits(enabled = FALSE)\n\n# Display the chart\nhchart\n\n\n\n\n\n\n\n\n\nRepression\n\n\ncode\n# repressionplot &lt;- ggplot(data=stateviol, aes(x=event_date, y=rollmean)) +\n#   geom_line()+\n#   geom_smooth() +\n#   geom_vline(xintercept=as.numeric(as.Date(\"2020-03-15\")), color=\"red\") +\n#   scale_x_date(date_breaks = \"6 months\", date_labels = '%b %Y')  +\n#   labs(x=\"Date\", y=\"Violence against civilians\", caption=\"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") + \n#   ggtitle(\"Violence Against Civilians During the Pandemic\") \n#   \n# repressionplot\n\n# First handle potential NA values and ensure data is properly ordered\nstateviol &lt;- stateviol %&gt;%\n  arrange(event_date) %&gt;%\n  # Optionally remove NA values if present\n  filter(!is.na(rollmean), !is.na(event_date))\n\n# Calculate the smoothed trend line with explicit handling of the data points\nx_vals &lt;- as.numeric(stateviol$event_date)\nsmooth_fit &lt;- loess(rollmean ~ x_vals, data = stateviol, span = 0.75)\nstateviol$smooth_trend &lt;- predict(smooth_fit, x_vals)\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n# Create the highchart\nhchart &lt;- highchart() %&gt;%\n  hc_add_series(\n    data = stateviol,\n    type = \"line\",\n    hcaes(x = event_date, y = rollmean),\n    color= \"#000000\",\n    name = \"Rolling Average\"\n  ) %&gt;%\n  hc_add_series(\n    data = stateviol,\n    type = \"line\",\n    hcaes(x = event_date, y = smooth_trend),\n    name = \"Trend\",\n    dashStyle = \"Solid\",\n    color = \"#005A43\",\n    opacity = 0.7\n  ) %&gt;%\n  hc_xAxis(\n    plotLines = list(list(\n      value = as.numeric(as.POSIXct(\"2020-03-15\")) * 1000,\n      color = \"red\",\n      width = 2,\n      zIndex = 3\n    )),\n    type = \"datetime\",\n    dateTimeLabelFormats = list(month = \"%b %Y\"),\n    tickInterval = 6 * 30 * 24 * 3600 * 1000\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Violence Against Civilians\")\n  ) %&gt;%\n  hc_title(\n    text = \"Violence Against Civilians During the Pandemic\"\n  ) %&gt;%\n  hc_caption(\n    text = \"7 day rolling averages; smoothed loess trend lines; red line indicates 3.15.2020 COVID onset; ACLED data retrieved 7.18.22\",\n    align = \"left\",\n    style = list(fontStyle = \"italic\")\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_credits(enabled = FALSE)\n\n# Display the chart\nhchart\n\n\n\n\n\n\n\n\n\nStates of Emergency during the Pandemic\n\n\ncode\n####################\n#analysis data\n####################\n\n#make analysis data frame\nanalysisdata &lt;- left_join(perday, soe, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"event_date\"))\nanalysisdata &lt;- left_join(analysisdata, covid, by=c(\"ccode\"=\"ccode\", \"event_date\"=\"date\"))\n\n#zeros for NA in soe_s\nanalysisdata &lt;- analysisdata %&gt;% mutate(across(soe_s, ~replace_na(., 0)))\n\n#fill soe series after onset\nanalysisdata &lt;- analysisdata %&gt;% group_by(ccode) %&gt;% mutate(cumSOE = cumsum(soe_s)) %&gt;% ungroup\n\n#plot cumulative states of emergency over time\ntotalsoe &lt;- analysisdata %&gt;% group_by(event_date) %&gt;%\n  summarise(across(cumSOE, sum)) %&gt;%\n  ungroup() \n\ntotalsoe$event_date &lt;- as.Date(totalsoe$event_date)\n\ntotalsoe &lt;- totalsoe %&gt;% filter(event_date &gt;\"2020-01-01\"& event_date &lt;\"2020-07-01\")\n\nggplot() +\n  geom_line(data=totalsoe, aes(x=event_date, y=cumSOE))+\n  scale_x_date(date_breaks = \"1 months\", date_labels = '%b %Y') +\n  labs ( colour = NULL, x = \"Date\", y =  \"Active States of Emergency\" )  \n\n\n\n\n\n\n\n\n\n\n\n\nModeling repression\nLet’s build a simple model predicting violence against civilians. The data here are daily, covering 167 countries between January 2020 and July 2022. We’ll predict the 7 day moving average of repression depending on whether a state of emergency is in place.\n\\(~\\)\nMy expectation is states of emergency will make repression easier to motivate and therefore, more frequent.\n\n\n\nThe model\n\\(y\\) = violence against civilians\n\\(x\\) = states of emergency\nControlling for:\n\nnew daily deaths from COVID (logged)\npercentage of the population over 65 years old\nthe Human Development Index\nthe 7 day average number of protests.\n\n\n\n\nModeling Repression and SoEs\n\n\ncode\n##################\n#models\n#################\n\nanalysisdata$soeprotests &lt;- analysisdata$cumSOE*analysisdata$protest7day\nanalysisdata$lnd =  log(analysisdata$new_deaths+.01)\n#label variables for coef plot\nlibrary(\"labelled\")\nanalysisdata &lt;- analysisdata %&gt;%\n  set_variable_labels(\n    cumSOE = \"State of Emergency\",\n    lnd = \"ln(New COVID deaths)\",\n    aged_65_older = \"% age 65 +\",\n    human_development_index = \"Human Development Index\",\n    protest7day = \"Protests, 7 day avg\",\n    soeprotests = \"SoE * Protests, 7 day avg\", \n    `Violence against civilians` = \"Violence against Civilians\"\n  )\n\n#model predicting repression during covid \n\nm1 &lt;- lm(data=analysisdata, `Violence against civilians` ~ cumSOE  + lnd + aged_65_older+human_development_index + protest7day )\n#summary(m1)  \n\nlibrary(sjPlot)\ntab_model(m1, show.se=TRUE,  p.style=\"stars\", show.ci=FALSE)\n\n\n\n\n\n\n\n\n\n\n \nViolence against\nCivilians\n\n\nPredictors\nEstimates\nstd. Error\n\n\n(Intercept)\n0.93 ***\n0.03\n\n\nState of Emergency\n0.26 ***\n0.01\n\n\nln(New COVID deaths)\n0.02 ***\n0.00\n\n\n%age 65+\n-0.02 ***\n0.00\n\n\nHuman Development Index\n-0.68 ***\n0.04\n\n\nProtests,7 day avg\n0.04 ***\n0.00\n\n\nObservations\n165211\n\n\nR2 / R2 adjusted\n0.057 / 0.057\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n\nAnother look\n\n\ncode\n#coefficient plot\nggcoef_model(\n  m1,show_p_values = FALSE,\n  signif_stars = FALSE, exponentiate =FALSE, intercept=TRUE,\n  colour=NULL, include = c(\"cumSOE\", \"lnd\", \"aged_65_older\", \"human_development_index\", \"protest7day\")\n)+\n  xlab(\"Average Marginal Effects\") +\n  ggtitle(\"Predictors of Violence Against Civilians\") \n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe estimates indicate:\n\nif all the \\(X\\) variables are set to zero, the expected violence against civilians is 0.93 (the intercept). Can we reasonably set all the \\(X\\) variables to zero?\nthe difference between a state with a SoE and without is different from zero, but quite small; 0.26 uses of violence.\nmore deaths from COVID are associated with more violence against civilians; \\(ln(\\beta) = .02\\). Exponentiate that, and the effect on violence is 1.02, so large compared to anything else in the model.\n\n\n\n\nQuantities of interest (simulation)\nLet’s generate predictions from the model by simulating the distribution of \\(\\widehat{\\beta}\\):\n\nsample 1000 times from a multivariate normal distribution with mean \\(\\widehat{\\beta}\\)s and variances of \\(var(\\widehat{\\beta})\\).\nplug those simulated \\(\\widehat{\\beta}\\)s into the equation using mean/median/mode values for the \\(X\\)s.\n\n\n\n\nSimulating quantities\nFirst, set \\(SoE = 0\\); vary \\(covid~deaths = ln(1 \\ldots 28000)\\), \\(\\%~age~ 65 = 6\\) (the median), \\(HDI = .74\\) (the median), and \\(protests = 2\\) (the median).\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*   0 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\nthen, repeat but with \\(SoE = 1\\):\n\\[ \\widehat{y} = \\beta_0 + \\beta_1*1 +\\beta_2*log(p) + \\beta_3*6 +\\beta_4*.74+ \\beta_5*2 \\]\n\n\n\nPredicted Violence against Civilians\n\n\ncode\n#simulated quantities\nsigma &lt;- vcov(m1)\nB &lt;- data.frame(rmvnorm(n=1000, mean=coef(m1), vcov(m1)))\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4', 'b5')\n\npredictions &lt;- data.frame ( lb0 = numeric(0),med0= numeric(0),\n      ub0= numeric(0),lb1= numeric(0),med1= numeric(0),ub1= \n      numeric(0), deaths = numeric(0))\n\nfor (p in seq(0,28000,100)) {\n  \n  xbRA0  &lt;- quantile(B$b0 + B$b1*0 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                       B$b5*2, probs=c(.025,.5,.975))\n  xbRA1 &lt;- quantile(B$b0 + B$b1*1 + B$b2*log(p) + B$b3*6 +B$b4*.74+ \n                      B$b5*2, probs=c(.025,.5,.975))\n  xbRA0&lt;- data.frame(t(xbRA0))\n  xbRA1&lt;- data.frame(t(xbRA1))\n  predictions[p:p,] &lt;- data.frame(xbRA0, xbRA1, p)\n}\npredictions$deaths &lt;- log(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"ln(New COVID Deaths)\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5.5, y = .82, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 6.5, y = .55, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nSoE states have higher levels of violence against civilians, but the difference is very small, making me think we haven’t found anything particularly interesting here.\n\nCovid deaths is associated with more violence; though violence nearly doubles over the range of deaths (x-axis), the range is huge - from 1 to 28,000. If we transform the x-axis to deaths rather than \\(ln(deaths)\\), we can see this effect is pretty isolated.\n\n\n\n\nSimulated effects (again)\n\n\ncode\npredictions$deaths &lt;- exp(predictions$deaths)\n\n#plot \nggplot()+\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb0, ymax=ub0),fill = \"grey70\", alpha = .4, ) +\n  geom_ribbon(data=predictions, aes(x=deaths, ymin=lb1, ymax=ub1), fill= \"grey60\",  alpha = .4, ) +\n  geom_line(data= predictions, aes(x=deaths, y=med0))+\n  geom_line(data= predictions, aes(x=deaths, y=med1))+\n  labs ( colour = NULL, x = \"New COVID Deaths\", y =  \"Expected Repression Episodes\" ) +\n  annotate(\"text\", x = 5000, y = 1, label = \"State of Emergency\", size=3.5, colour=\"gray30\")+\n  annotate(\"text\", x = 10000, y = .65, label = \"No State of Emergency\", size=3.5, colour=\"gray30\")\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThe increase in violence occurs almost entirely between zero and 5000 COVID deaths. We can speculate on why this might be; doing so might lead us to wonder how useful this model is.\n\\(~\\) \\(~\\)\nMuch of our focus this semester will be on evaluating how useful models are, and figuring out ways to build useful models."
  },
  {
    "objectID": "slides/probability25.html",
    "href": "slides/probability25.html",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns.\n\n\n\n\nProbability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value).\n\n\n\n\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita.\n\n\n\n\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely.\n\n\n\n\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war.\n\n\n\n\nThese four levels of measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information.\n\n\n\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "slides/probability25.html#why-probability-distributions",
    "href": "slides/probability25.html#why-probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Inferential models depend on probability distributions:\n\nestimation - is not deterministic, and so admits the unknown via disturbances \\(\\epsilon\\). We characterize those unknowns as following some probability distribution.\ninference - is essential because of the unknowns. Inference is possible because of the probability distribution characterizing the unknowns."
  },
  {
    "objectID": "slides/probability25.html#probability-distributions",
    "href": "slides/probability25.html#probability-distributions",
    "title": "Basic probability",
    "section": "",
    "text": "Probability distributions permit statements about relative scale, frequency, and uncertainty.\n\\(~\\)\nIf the relation between \\(x\\) and \\(y\\) is measured by \\(\\widehat{\\beta}\\), we need to know whether or not to take \\(\\widehat{\\beta}\\) seriously - is it representative of the population relationship between \\(x\\) and \\(y\\)?"
  },
  {
    "objectID": "slides/probability25.html#things-we-want-to-know-about-x",
    "href": "slides/probability25.html#things-we-want-to-know-about-x",
    "title": "Basic probability",
    "section": "",
    "text": "\\(Pr(X = x)\\) - the probability of observing any particular value of \\(x\\); these together comprise the density.\n\\(Pr(X \\leq x)\\) - the probability of observing values up to and including \\(x\\) (or any range of \\(x\\)). (CDF)\ncentral tendency of \\(x\\) - mean, median, mode.\ndispersion - variance, or standard deviation (the average difference of any observation from the expected value)."
  },
  {
    "objectID": "slides/probability25.html#types-of-variables",
    "href": "slides/probability25.html#types-of-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Variables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes \\(-\\infty, +\\infty\\)); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "slides/probability25.html#discrete-variables",
    "href": "slides/probability25.html#discrete-variables",
    "title": "Basic probability",
    "section": "",
    "text": "May be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "slides/probability25.html#continuous-variables",
    "href": "slides/probability25.html#continuous-variables",
    "title": "Basic probability",
    "section": "",
    "text": "Can be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "slides/probability25.html#levels-of-measurement",
    "href": "slides/probability25.html#levels-of-measurement",
    "title": "Basic probability",
    "section": "",
    "text": "These four levels of measurement can be ordered by the amount of information a variable contains, least to most:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "slides/probability25.html#levels-of-measurement-and-models",
    "href": "slides/probability25.html#levels-of-measurement-and-models",
    "title": "Basic probability",
    "section": "",
    "text": "In general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "slides/probability25.html#pdf-and-cdf",
    "href": "slides/probability25.html#pdf-and-cdf",
    "title": "Basic probability",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nProbability distributions can be described by\n\nProbability Density Function (PDF) which maps \\(X\\) onto the probability space describing the probabilities of every value of \\(X\\), \\(x\\).\nCumulative Distribution Function (CDF) which maps \\(X\\) onto the probability space describing the probability \\(X\\) is less than some value, \\(x\\)."
  },
  {
    "objectID": "slides/probability25.html#pdf-density",
    "href": "slides/probability25.html#pdf-density",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nDefinition\n\n\n\nThe Probability Density Function or PDF describes the probability a random variable takes on a particular value, and it does so for all values of that random variable."
  },
  {
    "objectID": "slides/probability25.html#pdf-density-1",
    "href": "slides/probability25.html#pdf-density-1",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\n\n\n\n\n\nExample\n\n\n\nSuppose we toss a fair coin 1 time and want to describe the probability distribution of the outcome, \\(Y\\) which is a random variable. The possible outcomes are heads and tails, and the probability of each is .5. This is the PDF because it describes the probabilities associated with all possible outcomes of \\(Y\\)."
  },
  {
    "objectID": "slides/probability25.html#pdf-density-2",
    "href": "slides/probability25.html#pdf-density-2",
    "title": "Basic probability",
    "section": "PDF (Density)",
    "text": "PDF (Density)\n\nWhile establishing the probability of a value of a discrete variable is possible, establishing the probability of a particular value of a continuous variable is not.\nInstead, the continuous PDF describes the instantaneous rate of change in \\(Pr(X=x)\\) for every value of \\(x\\)."
  },
  {
    "objectID": "slides/probability25.html#pdf-plots",
    "href": "slides/probability25.html#pdf-plots",
    "title": "Basic probability",
    "section": "PDF plots",
    "text": "PDF plots"
  },
  {
    "objectID": "slides/probability25.html#cdf",
    "href": "slides/probability25.html#cdf",
    "title": "Basic probability",
    "section": "CDF",
    "text": "CDF\n\n\n\n\n\n\nDefinition\n\n\n\nFor a random variable, \\(Y\\), the probability \\(Y\\) is less than or equal to some particular value, \\(y\\) in the range of \\(Y\\) defines the cumulative density function.\nFor a continuous variable:\n\\[P(Y \\leq j) =\\int\\limits_{-\\infty}^{j} f(X)dX\\]\nFor a discrete variable:\n\\[P(Y \\leq j) = \\sum\\limits_{y\\leq j} P(Y=y)\n= 1- \\sum\\limits_{y&gt; j} P(Y=y)\\]"
  },
  {
    "objectID": "slides/probability25.html#cdf-plots",
    "href": "slides/probability25.html#cdf-plots",
    "title": "Basic probability",
    "section": "CDF plots",
    "text": "CDF plots"
  },
  {
    "objectID": "slides/probability25.html#notation",
    "href": "slides/probability25.html#notation",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\\(f(x)\\) denotes the PDF of \\(x\\).\n\\(F(x)\\) denotes the CDF of \\(x\\).\nSubstitute either the appropriate name, symbol, or function for \\(F\\) or \\(f\\) to indicate the distribution.\nLower case Greek letters denote PDFs: e.g. \\(f(x)= \\phi(x)\\) denotes the normal PDF.\nUpper case Greek letters denote CDFs: e.g. \\(F(x)=\\Phi(x)\\) denotes the normal CDF."
  },
  {
    "objectID": "slides/probability25.html#notation-1",
    "href": "slides/probability25.html#notation-1",
    "title": "Basic probability",
    "section": "Notation",
    "text": "Notation\n\n\n\n\n\n\nExample\n\n\n\n\\(x\\) is distributed Normal, \\(N(\\mu, \\sigma^{2})\\):\n\\[F(x) = \\Phi_{\\mu, \\sigma^{2}}(x)  =    \\int  \\phi_{\\mu, \\sigma^{2}} (x) f(x)d(x) \\]\n\\[ f(x) = \\phi_{\\mu, \\sigma^{2}}(x) =\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{(x-\\mu)^{2}}{2 \\sigma^{2}} \\right) \\]\nNote the first derivative of the CDF is the PDF; the integral of the PDF is the CDF."
  },
  {
    "objectID": "slides/probability25.html#bernoulli",
    "href": "slides/probability25.html#bernoulli",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nSuppose a binary variable \\(x\\) that takes on only the values of zero and one (\\(x \\in {0, 1}\\)):\n\\[\nPr(X=1)=\\pi\\nonumber \\\\\nPr(x=0)= 1-Pr(x=1) \\\\\n= 1-\\pi   \n\\]"
  },
  {
    "objectID": "slides/probability25.html#bernoulli-1",
    "href": "slides/probability25.html#bernoulli-1",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe PDF is:\n\\[ f(x) = \\\\\n          \\pi    ~~~~~~~~ ~ ~ ~~~~ \\text{if } x=1\\\\\n         1-\\pi   ~ ~ ~ ~~~~\\text{if } x=0\n     \\]\nor\n\\[f(x) = \\pi^{x}(1-\\pi)^{1-x} \\]"
  },
  {
    "objectID": "slides/probability25.html#bernoulli-2",
    "href": "slides/probability25.html#bernoulli-2",
    "title": "Basic probability",
    "section": "Bernoulli",
    "text": "Bernoulli\nThe CDF is:\n\\[F(x) =\\sum_{x} f(x) \\]\nand the expected value is\n\\[\nE(x) =\\sum_{x}x f(x)    \\\\\n= (1)(\\pi)+(0)(1-\\pi)   \\\\\n= \\pi\n\\]"
  },
  {
    "objectID": "slides/probability25.html#binomial",
    "href": "slides/probability25.html#binomial",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nBernoulli is important because it is the foundation for a lot of other distributions, including the binomial distribution. The binomial describes the success probability function (where \\(x=1\\) is a “success”) from a set of \\(n\\) independent Bernoulli trials. So, the binomial is the probability of successes (“ones”) in \\(n\\) independent Bernoulli trials with identical probabilities, \\(\\pi\\).\n\\(~\\)\nThere are two essential parts to the binomial PDF - the probability of success, and the number of ways a success can occur."
  },
  {
    "objectID": "slides/probability25.html#binomial-1",
    "href": "slides/probability25.html#binomial-1",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe probability of success is the Bernoulli probability:\n\\[\nf(x) = \\pi^{x}(1-\\pi)^{1-x}\n\\]\nand the number of ways success can occur (called “n-tuples”) is\n\\[  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} = \\frac{n!}{x!(n-x)!}\n\\]\nNotice the notation for the n-tuple."
  },
  {
    "objectID": "slides/probability25.html#binomial-2",
    "href": "slides/probability25.html#binomial-2",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\nThe PDF combines these:\n\\[\nf(x)=\n  \\begin{pmatrix}\n    n  \\\\\n    x\n  \\end{pmatrix} \\pi^{x}(1-\\pi)^{n-x}\n\\]\nThere are \\(n\\) trials, \\(x\\) is the number of successes, and \\(n-x\\) is the number of failures. Each event in the n-tuple arises with the Bernoulli probability."
  },
  {
    "objectID": "slides/probability25.html#binomial-3",
    "href": "slides/probability25.html#binomial-3",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are going to toss a fair coin 4 times and want to know how many ways we can have heads come up twice.\n\\[\n\\frac{4!}{2!(4-2)!} = 6\n\\] Now, suppose we want to know the probability exactly 2 of those 4 tosses is heads.\n\\[\nPr(x=2) =\\frac{4!}{2!(4-2)!} (.5)^{2}(.5)^{2} \\\\\n= 6(0.0625) \\\\\n= 0.375\n\\]"
  },
  {
    "objectID": "slides/probability25.html#binomial-4",
    "href": "slides/probability25.html#binomial-4",
    "title": "Basic probability",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\nExample\n\n\n\nHere’s another example: suppose we flip 5 fair coins once each; what is the PDF of the number of heads we expect? In other words, what is the probability associated with each possible number of successes (heads), from 0 to 5?\n\\[\nP(x=0)  =  \\begin{pmatrix}\n    5  \\\\\n    0\n  \\end{pmatrix} \\cdot  (.5)^{0} \\cdot (.5) ^{5} = 1 \\cdot 1 \\cdot 0.03125=0.03125\\\\\n  \\] \\[\nP(x=1) =  \\begin{pmatrix}\n    5  \\\\\n    1\n  \\end{pmatrix} \\cdot (.5)^{1}\\cdot (.5)^{4} = 5 \\cdot .5 \\cdot 0.0625=0.15625 \\\\\n  \\] \\[P(x=2) =  \\begin{pmatrix}\n    5  \\\\\n    2\n  \\end{pmatrix} \\cdot (.5)^{2}\\cdot (.5)^{3} = 10 \\cdot .25 \\cdot 0.125=0.3125 \\\\\n\\] \\[\n\\vdots \\vdots \\vdots\n\\]"
  },
  {
    "objectID": "slides/probability25.html#binomial-family-distributions",
    "href": "slides/probability25.html#binomial-family-distributions",
    "title": "Basic probability",
    "section": "Binomial family distributions",
    "text": "Binomial family distributions\n\nthe geometric distribution describes repeated Bernoulli trials with probability of success \\(\\pi\\), until the first success.\nthe negative binomial describes the number of Bernoulli failures prior to the first success - it can be thought of as a counting process up to the first success.\nthe poisson describes Bernoulli trials where \\(\\pi\\) for any particular trial is very small."
  },
  {
    "objectID": "slides/probability25.html#the-normal-distribution",
    "href": "slides/probability25.html#the-normal-distribution",
    "title": "Basic probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe Normal distribution is the most widely used distribution in the social sciences.\n\nThe normal seems to “fit” a lot of the variables social scientists measure and use in models.\nEstimation techniques we often employ assume the disturbance term is normally distributed; one result is the coefficients are either normally distributed or t-distributed."
  },
  {
    "objectID": "slides/probability25.html#normal-pdf",
    "href": "slides/probability25.html#normal-pdf",
    "title": "Basic probability",
    "section": "Normal PDF",
    "text": "Normal PDF\nThe normal PDF: \\[\nPr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]\n\\]\nwhere two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\) describe the location and shape of the distribution, the mean and variance respectively; we indicate a normally distributed variable and its parameters as\n\\[\nY \\sim \\text{Normal}(\\mu,\\sigma^{2}) \\nonumber\n\\]\n\n\nDescribe these dataNormal?\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "slides/probability25.html#standard-normal",
    "href": "slides/probability25.html#standard-normal",
    "title": "Basic probability",
    "section": "Standard Normal",
    "text": "Standard Normal\nA useful special case of the Normal is the standard normal, \\(z \\sim \\text{Normal}(0,1)\\). The PDF for the standard normal is given by\n\\[\n\\phi(z)=\\frac{1}{\\sqrt{2 \\pi }} exp \\left[\\frac{-z^{2}}{2}\\right] \\nonumber\n\\]\nwhere the parameters themselves drop out since we’d be subtracting a mean of zero and dividing/multiplying by a variance of 1. The standard normal CDF is denoted \\(\\Phi(z)\\); the standard normal PDF is denoted \\(\\phi(z)\\)."
  },
  {
    "objectID": "slides/probability25.html#normal-pdfs-different-moments",
    "href": "slides/probability25.html#normal-pdfs-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs different moments",
    "text": "Normal PDFs different moments"
  },
  {
    "objectID": "slides/probability25.html#models",
    "href": "slides/probability25.html#models",
    "title": "Basic probability",
    "section": "Models",
    "text": "Models\n\nProbability models include unobserved disturbances; we make assumptions about the distributions of those errors, \\(\\epsilon\\).\nWhat we assume about the unobservables is always informed by what we know about the observables, mainly \\(y\\).\nA useful way to describe or summarize \\(y\\) is to characterize its observed distribution.\nThe distribution of \\(y\\) informs our assumption about the distribution of \\(\\epsilon\\)."
  },
  {
    "objectID": "slides/probability25.html#why-this-matters-to-ols",
    "href": "slides/probability25.html#why-this-matters-to-ols",
    "title": "Basic probability",
    "section": "Why this matters to OLS",
    "text": "Why this matters to OLS\nLinear regression is such an inferential model; we have a number of sources of uncertainty. We represent that uncertainty in the model via the disturbance term, \\(\\epsilon\\).\n\\(~\\)\nIn order to know things about \\(\\epsilon\\) and other parts of the model, we assume that the disturbances are normally distributed; \\(y\\) should be normal too."
  },
  {
    "objectID": "slides/probability25.html#normality-and-centrality",
    "href": "slides/probability25.html#normality-and-centrality",
    "title": "Basic probability",
    "section": "Normality and Centrality",
    "text": "Normality and Centrality\nWe rely on stuff being normally distributed, and central tendency being meaningful. The Central Limit Theorem facilitates this."
  },
  {
    "objectID": "slides/probability25.html#central-limit-theorem",
    "href": "slides/probability25.html#central-limit-theorem",
    "title": "Basic probability",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nSuppose \\(n\\) random variables - call them \\(X_1, X_2 \\ldots X_n\\). These variables are not identically distributed; some are discrete, some continuous. Each variable, \\(X_i\\) has mean \\(\\bar{X_k}\\).\n\\[ {\\sum\\limits_{n \\rightarrow \\infty} (\\bar{X_i})} / {n}  \\sim N (\\mu, \\sigma^2) \\]\nAs \\(n\\) approaches infinity, the distribution of means, \\(\\bar{X_i}\\) is distributed Normal, with mean \\(\\widetilde{X_i}\\). We could accomplish the same thing by repeated sampling of a single variable."
  },
  {
    "objectID": "slides/probability25.html#clt---why-is-this-valuable",
    "href": "slides/probability25.html#clt---why-is-this-valuable",
    "title": "Basic probability",
    "section": "CLT - Why is this valuable?",
    "text": "CLT - Why is this valuable?\n\nIf we assume repeated sampling and/or infinitely large samples, we can assume normality.\nWe know the properties of the Normal intimately well. We can use this knowledge to evaluate where some value of \\(x\\) lies on the Normal CDF, what the probability less than that value is - we can figure out what the probability of observing that value is (on the PDF).\nAs we’ll see, \\(\\widehat{\\beta}\\) is normally distributed in the OLS model (it is in MLE as well). The CLT and what we know about normality facilitate inference."
  },
  {
    "objectID": "slides/probability25.html#inference",
    "href": "slides/probability25.html#inference",
    "title": "Basic probability",
    "section": "Inference",
    "text": "Inference\nInference is our effort to measure and characterize our uncertainty about the model and its parameters.\n\nuncertainty is the most important thing we estimate in inferential models.\ncharacterizing uncertainty relies on probability theory."
  },
  {
    "objectID": "syllabus25.html#seminar-description",
    "href": "syllabus25.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is about the principles of linear regression models, focusing on the ordinary least squares approach to regression. The course is data science oriented, emphasizing data managment, wrangling, coding in R, and data visualization.\n\nThe class requires some background (provided in preview materials) in probability theory, matrix algebra, and using R. A major goal of the class is to provide the intuition behind how and why we do empirical tests of theories of politics. Anyone can estimate a statistical model, but not everyone can estimate and interpret an informed, thoughful model. Understanding regression and linking regression with theories of political behavior are key steps toward saying insightful things about politics.\nAn important thing to realize is that it takes time and repetition to really understand this stuff intuitively. Exposure to regression in general (whether OLS, MLE, Bayes, etc.) by reading, hearing, and doing over and over will make it stick. So don’t get down or freaked out if you feel like you don’t get it. Instead, ask questions - of me, of the TA, in class, in workshop, in office hours. Asking is essential.\nThe class meets one time per week for three hours. The required workshop meets one time per week for 1 hour. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for your to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "syllabus25.html#course-purpose",
    "href": "syllabus25.html#course-purpose",
    "title": "Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar is a requirement in the Political Science Ph.D. curriculum. It is the second (of three) required quantitative methods courses aimed at giving students a wide set of empirical tools to put to use in testing hypotheses about politics. This seminar focuses on the method of Ordinary Least Squares, and the linear model, emphasizing data science skills."
  },
  {
    "objectID": "syllabus25.html#learning-objectives",
    "href": "syllabus25.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the semester, students will be able to describe the linear model, its assumptions, and interpretation. Students will be able to manage data, estimate linear models, diagnose and correct models, and generate quantities of interest. Students will be able to plot and analyze their estimates and quantities of interest. Students will be able to generate their own research designs, and test hypotheses using the linear model."
  },
  {
    "objectID": "syllabus25.html#reading",
    "href": "syllabus25.html#reading",
    "title": "Syllabus",
    "section": "Reading",
    "text": "Reading\nThe book for the class is:\n\nWooldridge, J.M. 2013. Introductory Econometrics. 5th edition, South-Western/Cengage. ISBN 978-1-111-53104-1.\n\nI’m assigning an older edition of Wooldridge (the 5th) so cheaper copies are easier to come by. If you choose an edition other than that, you’ll need to reconcile chapters, etc. Also, be warned that the cheaper international edition (paperback) is alleged to differ in substantial ways.\nIn addition, I’ll assign a small number of articles over the course of the term.\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it."
  },
  {
    "objectID": "syllabus25.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "syllabus25.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss the readings in seminar. Their purposes are two-fold. First, the technical readings (Wooldridge) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it.\n\n\nHow to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus25.html#course-requirements-and-grades",
    "href": "syllabus25.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 50% total\nReplication/Extension project (including log, poster, etc.) - 50%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work. Ever.\nFor technical and statistics training and help, Oguzhan will lead a required workshop session every week, Thursday 1:15-2:15pm.\nThe paper assignment asks you to replicate some published piece of research (details on selecting this we’ll discuss in seminar), to comment critically on the theory and design as you replicate, and then to build on the paper in a substantive way, extending the research. That extension is the main part of the assignment - this is one of your first major opportunities to develop a novel contribution to the empirical literature. The replication/extension paper is due the Monday of exam week. You’ll keep a research log as you work on the paper during the semester - this is due along with the paper, and is part of your total grade. In addition, we’ll have a poster session to present your extensions at the end of the semester. Details to follow.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "syllabus25.html#course-policies",
    "href": "syllabus25.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies"
  },
  {
    "objectID": "syllabus25.html#course-schedule",
    "href": "syllabus25.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nReferring to Wooldridge, 5th edition, 2013\nWeek 1. 22 Jan Introduction, Regression Discussion\n\nmatrix notes, preview materials\n\nWeek 2, 29 Jan – Matrix/Bivariate Regression\n\nWooldridge, chapter 2, Appendix D, Appendix E\n\nWeek 3, 5 Feb – Implementing the Model\n\nWooldridge, chapter 3, Appendix E\n\nWeek 4, 12 Feb – Multivariate Regression, statistical control\n\nWooldridge, chapter 3, Appendix E\n\nWeek 5, 19 Feb – Inference in Regression\n\nWooldridge, chapter 4\n\nWeek 6, 26 Feb – Specifying the Model\n\nWooldridge, chapters 6 and 9\nKing (1986)\n\nWeek 7, 5 March – Dummy Variables & Multiplicative Interactions\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 8, 12 March – Spring Break\nWeek 9, 19 March – Interactions (continued)\n\nWooldridge, chapter 7\nBrambor, Clark, and Golder (2006)\n\nWeek 10, 26 March – Limited Dependent Variables\n\nWooldridge, chapter 7.5, and 17.1\n\nWeek 11, 2 April – Panels, Fixed & Random Effects\n\nWooldridge, chapters 13, 14\n\nWeek 12, 9 April– Time Series\n\nWooldridge, chapters 10, 12\n\nWeek 13, 16 April – no classes; Monday classes meet\nWeek 14, 23 April – Non-constant Variance\n\nWooldridge, chapter 8\n\nWeek 15, 30 April – IV models\n\nWooldridge, chapter 15\n\nWeek 16, 7 May – Causal Inference\n\nTBA"
  },
  {
    "objectID": "syllabus25.html#instructor",
    "href": "syllabus25.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\n   Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\noffice hours: M 10am-12pm"
  },
  {
    "objectID": "syllabus25.html#course-details",
    "href": "syllabus25.html#course-details",
    "title": "Syllabus",
    "section": "Course details",
    "text": "Course details\n\n   Spring 2025\n   Wednesday\n   9:40-12:40\n   LNG 332 (Bing SSEL)"
  },
  {
    "objectID": "syllabus25.html#ta",
    "href": "syllabus25.html#ta",
    "title": "Syllabus",
    "section": "TA",
    "text": "TA\n\n   Oguzhan Irguren\n   oirgure1@binghamton.edu\noffice hours: R 3pm-4pm"
  },
  {
    "objectID": "syllabus25.html#lab",
    "href": "syllabus25.html#lab",
    "title": "Syllabus",
    "section": "Lab",
    "text": "Lab\n\n   Thursday\n   1:15-2:15\n   CW 326"
  },
  {
    "objectID": "syllabus25.html",
    "href": "syllabus25.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dave Clark\n   LNG 57; LN2430\n   dclark@binghamton.edu\n   clavedark\noffice hours: M 1:30-3:30pm\n\n\n\n\n\n   Spring 2025\n   Wednesday\n   9:40-12:40\n   LNG 332 (Bing SSEL)\n\n\n\n\n\n\n\n\n\n\n\n\n   Oguzhan Irguren\n   oirgure1@binghamton.edu\n\n\n\n\n\n   Thursday\n   1:15-2:15\n   CW 326"
  },
  {
    "objectID": "syllabus25.html#how-to-read",
    "href": "syllabus25.html#how-to-read",
    "title": "Syllabus",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing.\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "syllabus25.html#attendance",
    "href": "syllabus25.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nAttendance is expected, and is essential both in the seminars and lab sessions if you’re to succeed in this class."
  },
  {
    "objectID": "syllabus25.html#academic-integrity",
    "href": "syllabus25.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "syllabus25.html#references",
    "href": "syllabus25.html#references",
    "title": "MLE Syllabus",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus25.html#resources",
    "href": "syllabus25.html#resources",
    "title": "Syllabus",
    "section": " Resources",
    "text": "Resources\nThere are lots of good, free  resources online. Here are a few:\n\nModern Statistics with R\nR for Data Science\nR Markdown: The Definitive Guide\nR Graphics Cookbook\nAdvanced R\nR Markdown Cookbook\nData Science:A First Introduction\nThe Big Book of R"
  },
  {
    "objectID": "syllabus25.html#class-meetings-office-hours-assignments",
    "href": "syllabus25.html#class-meetings-office-hours-assignments",
    "title": "Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this spring entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1 answers\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/probability25.html#normal-pdfs-with-different-moments",
    "href": "slides/probability25.html#normal-pdfs-with-different-moments",
    "title": "Basic probability",
    "section": "Normal PDFs with different moments",
    "text": "Normal PDFs with different moments\nThe following plot shows three normal PDFs with different means and variances. The PDFs are centered at -1, 0, and 1, and have variances of .5, 1, and 1.5 respectively. Thinking in terms of uncertainty (foreshadowing a bit), imagine that larger variance indicates more uncertainty about the mean, and smaller variance indicates less uncertainty."
  },
  {
    "objectID": "slides/probability25.html#simulating-the-central-limit-theorem",
    "href": "slides/probability25.html#simulating-the-central-limit-theorem",
    "title": "Basic probability",
    "section": "Simulating the Central Limit Theorem",
    "text": "Simulating the Central Limit Theorem\nThe following app simulates the Central Limit Theorem. You can select a distribution, sample size, and number of simulations. The app will plot the distribution of the means of the samples. You’ll see that, regardless of the distribution the means are drawn from, the distribution of the means is normal as the sample size increases."
  },
  {
    "objectID": "slides/thinkingdata25.html",
    "href": "slides/thinkingdata25.html",
    "title": "Thinking About Data",
    "section": "",
    "text": "These slides are intended to get you thinking about the data you are using, what it tells us, what it doesn’t etc., and to think carefully about what we’re trying to measure, and what we’re actually measuring.\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "slides/thinkingdata25.html#get-to-know-your-data-explore-etc",
    "href": "slides/thinkingdata25.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "",
    "text": "These slides are intended to get you thinking about the data you are using, what it tells us, what it doesn’t etc., and to think carefully about what we’re trying to measure, and what we’re actually measuring.\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "slides/thinkingdata25.html#data-generating-process",
    "href": "slides/thinkingdata25.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data? That is, do the actors have\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "slides/thinkingdata25.html#data-generating-process-1",
    "href": "slides/thinkingdata25.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources\n\n\nThe data generating process is the complete description of how the observed data arose and how other such data would arise. It includes variables, conditionals, functional forms, mappings from one unit to another, etc."
  },
  {
    "objectID": "slides/thinkingdata25.html#a-terrible-map",
    "href": "slides/thinkingdata25.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "slides/thinkingdata25.html#war-outcomes",
    "href": "slides/thinkingdata25.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "slides/thinkingdata25.html#observability",
    "href": "slides/thinkingdata25.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "slides/thinkingdata25.html#asking-the-right-question",
    "href": "slides/thinkingdata25.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "slides/thinkingdata25.html#data",
    "href": "slides/thinkingdata25.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "slides/thinkingdata25.html#types-of-variables",
    "href": "slides/thinkingdata25.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "slides/thinkingdata25.html#discrete-variables",
    "href": "slides/thinkingdata25.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "slides/thinkingdata25.html#continuous-variables",
    "href": "slides/thinkingdata25.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "slides/thinkingdata25.html#levels-of-measurement",
    "href": "slides/thinkingdata25.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "slides/thinkingdata25.html#levels-of-measurement-and-models",
    "href": "slides/thinkingdata25.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "slides/thinkingdata25.html#describe-these-data",
    "href": "slides/thinkingdata25.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "slides/thinkingdata25.html#describe-these-data-1",
    "href": "slides/thinkingdata25.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "slides/thinkingdata25.html#overstaying-terms",
    "href": "slides/thinkingdata25.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "slides/tdata.html#get-to-know-your-data-explore-etc",
    "href": "slides/tdata.html#get-to-know-your-data-explore-etc",
    "title": "Thinking About Data",
    "section": "Get to know your data, explore etc",
    "text": "Get to know your data, explore etc\n\nAnscombe’s QuartetQuartet \\(\\beta\\)sQuartet Viz\n\n\n\n\ncode\n# anscombes quartet \nlonganscombe &lt;- anscombe %&gt;%\n\npivot_longer(cols = everything(), #pivot all the columns\n             cols_vary = \"slowest\", #keep the datasets together \n              names_to = c(\".value\", \"set\"), #new var names; .value=stem of vars\n              names_pattern = \"(.)(.)\") #to extract var names \n\n\nkable(\n  list(anscombe),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nB &lt;- data.frame(set=numeric(0), b0=numeric(0), b1=numeric(0))\nfor (i in longanscombe$set) {\n    m &lt;- lm(y ~ x, data=longanscombe %&gt;% filter(set==i))\n    B[i:i,] &lt;- data.frame(i, coef(m)[1], coef(m)[2])\n}\n\n\nkable(\n  list(B),\n  caption=\"Anscombe's Quartet\",\n  booktabs = TRUE,\n  valign = 't',\n  row.names = FALSE,\n)\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nset\nb0\nb1\n\n\n\n\n1\n3.000091\n0.5000909\n\n\n2\n3.000909\n0.5000000\n\n\n3\n3.002454\n0.4997273\n\n\n4\n3.001727\n0.4999091\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nggplot(longanscombe, aes(x = x, y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"red\")"
  },
  {
    "objectID": "slides/tdata.html#data-generating-process",
    "href": "slides/tdata.html#data-generating-process",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhat produced the data we observe?\n\npolitical process, actors, etc.\nare those actors purposeful wrt the observed data?\ndata collector; choices, biases, mistakes."
  },
  {
    "objectID": "slides/tdata.html#data-generating-process-1",
    "href": "slides/tdata.html#data-generating-process-1",
    "title": "Thinking About Data",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\nWhy do we observe the data we see and not the data we don’t?\n\nexistence is not randomly determined.\nresearch questions are usually about things that happen, not things that do not or cannot.\nreporting itself is a political process.\nreporting is shaped by resources"
  },
  {
    "objectID": "slides/tdata.html#a-terrible-map",
    "href": "slides/tdata.html#a-terrible-map",
    "title": "Thinking About Data",
    "section": "A Terrible Map",
    "text": "A Terrible Map"
  },
  {
    "objectID": "slides/tdata.html#war-outcomes",
    "href": "slides/tdata.html#war-outcomes",
    "title": "Thinking About Data",
    "section": "War outcomes",
    "text": "War outcomes\n\n\n\nOutcome\nAutocrat\nDemocrat\nTotal\n\n\n\n\nLoser\n42\n9\n51\n\n\nWinner\n32\n38\n70\n\n\nTotal\n74 47\n121\n\n\n\n\nFrom Lake (1992), p. 31"
  },
  {
    "objectID": "slides/tdata.html#observability",
    "href": "slides/tdata.html#observability",
    "title": "Thinking About Data",
    "section": "Observability",
    "text": "Observability"
  },
  {
    "objectID": "slides/tdata.html#asking-the-right-question",
    "href": "slides/tdata.html#asking-the-right-question",
    "title": "Thinking About Data",
    "section": "Asking the right question",
    "text": "Asking the right question"
  },
  {
    "objectID": "slides/tdata.html#data",
    "href": "slides/tdata.html#data",
    "title": "Thinking About Data",
    "section": "Data",
    "text": "Data\nCollections of alike units, their characteristics, features, choice sets, behaviors, etc.\n\nwhat are the units? In what ways are they heterogeneous?\nwhat units are included? Which ones are missing? Why?\nwhat do the variables measure?\nhow are the variables measured?\nwhat observations are missing? Why?\nwhat is the sample; what is the population (sampling frame) from which the sample is drawn?"
  },
  {
    "objectID": "slides/tdata.html#types-of-variables",
    "href": "slides/tdata.html#types-of-variables",
    "title": "Thinking About Data",
    "section": "Types of variables",
    "text": "Types of variables\nVariables are either\n\ndiscrete - observations match to integers; all possible values are clearly distinguishable; not divisible. E.g., number of protests in DC this year; an individual’s sex; Polity score.\ncontinuous - observations can take on any real value between boundaries (sometimes $-, +); infinitely divisible. E.g., household income, GDP per capita."
  },
  {
    "objectID": "slides/tdata.html#discrete-variables",
    "href": "slides/tdata.html#discrete-variables",
    "title": "Thinking About Data",
    "section": "Discrete variables",
    "text": "Discrete variables\nMay be of two types or levels of measurement:\n\nnominal - categories are distinct, but lack order. E.g., religion = Hindu, Muslim, Catholic, Protestent, Jewish. Binary variables are nominal, e.g., Sex = male (0), female (1); do you have blue eyes? yes (0), no (1).\nordinal - take on countable values, increasing/decreasing in some dimension. E.g., Polity -10, -9, \\(\\ldots\\) 0, 1, \\(\\ldots\\) 9, 10 increasing in democracy; survey responses “Do you feel safe traveling abroad?” Not at all; sometimes; yes, completely."
  },
  {
    "objectID": "slides/tdata.html#continuous-variables",
    "href": "slides/tdata.html#continuous-variables",
    "title": "Thinking About Data",
    "section": "Continuous variables",
    "text": "Continuous variables\nCan be of two types (levels of measurement):\n\ninterval - 1 unit increase has same meaning across the scale (i.e., the intervals are the same); e.g., degrees Celsius or Fahrenheit.\nratio - intervals but also has a meaningful absolute zero; e.g., weight in pounds; zero lbs indicates the absence of weight; Venmo balance = zero, means actually no money; degrees Kelvin. Duration of a war in days - zero days means there’s no war."
  },
  {
    "objectID": "slides/tdata.html#levels-of-measurement",
    "href": "slides/tdata.html#levels-of-measurement",
    "title": "Thinking About Data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThese four levels or measurement can be ordered by the amount of information a variable contains:\n\nnominal\nordinal\ninterval\nratio\n\nWe can turn higher levels to lower levels, but not the opposite - doing so sacrifices information."
  },
  {
    "objectID": "slides/tdata.html#levels-of-measurement-and-models",
    "href": "slides/tdata.html#levels-of-measurement-and-models",
    "title": "Thinking About Data",
    "section": "Levels of Measurement and Models",
    "text": "Levels of Measurement and Models\nIn general, the level of measurement of \\(y\\) (so the type and amount of information in a variable) shapes what type of model is appropriate.\n\ndiscrete variables usually require statistics/models in the Binomial family (for our purposes, mostly MLE models like the Logit.)\ncontinuous variables usually require statistics/models in the Normal/Gaussian family (for our purposes, mostly OLS models like the linear regression.)"
  },
  {
    "objectID": "slides/tdata.html#describe-these-data",
    "href": "slides/tdata.html#describe-these-data",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\n\nAlcohol consumptionNormal PDF overlay\n\n\n\n\ncode\ngap &lt;- read.csv(\"/Users/dave/documents/teaching/501/2024/slides/L1-data/data/gapminder.csv\")\n\nggplot(gap, aes(x=alcohol_consumption_per_adult_15plus_litres)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n geom_density(alpha=.2, fill=\"#FF6666\") +\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Density - Alcohol Consumption per Adult (Liters)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\ngap$nalc &lt;- dnorm(gap$alcohol_consumption_per_adult_15plus_litres, mean=mean(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE), sd=sd(gap$alcohol_consumption_per_adult_15plus_litres, na.rm = TRUE))\n\nggplot() + \n  geom_histogram(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=..density..), colour=\"black\", fill=\"white\") +\n  geom_density(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres), alpha=.2, fill=\"#FF6666\")  +\n geom_line(data=gap, aes(x=alcohol_consumption_per_adult_15plus_litres, y=nalc), linetype=\"longdash\", size=1)+\n   labs(x = \"Liters of Booze\", y= \"Density\", caption=\"Alcohol Consumption, Gapminder\")+\n  ggtitle(\"Alcohol Consumption per Adult (Liters), Normal PDF\")"
  },
  {
    "objectID": "slides/tdata.html#describe-these-data-1",
    "href": "slides/tdata.html#describe-these-data-1",
    "title": "Thinking About Data",
    "section": "Describe these data",
    "text": "Describe these data\nPolity Scores\n\n\ncode\npolity &lt;- polity %&gt;% mutate(era = ifelse(year==1980, 1980, ifelse(year==2018, 2018, 0)))\n\np &lt;- ggplot(data=polity %&gt;% filter(era!=0), aes(y=polity2), colour=\"black\", fill=\"white\")+\n  geom_bar()+\n  geom_text(aes(label = ..count..),stat=\"count\", hjust = -.2, colour = \"black\", size=2.5, \n            position = position_dodge(0.9)) \n\np + facet_wrap(era ~ .) +\n  labs(y = \"Polity score\", x= \"Countries\", caption=\"Polity project\")+\n  ggtitle(\"Polity Scores, 1980 and 2018\")"
  },
  {
    "objectID": "slides/tdata.html#overstaying-terms",
    "href": "slides/tdata.html#overstaying-terms",
    "title": "Thinking About Data",
    "section": "Overstaying Terms",
    "text": "Overstaying Terms\n\n\nexpand for full code\n#polity &lt;- read_dta(\"/Users/dave/Documents/2023/PITF/slides/polity5.dta\")\noverpolity &lt;- left_join(tl, polity, by=c(\"ccode\"=\"ccode\", \"year\"=\"year\"))\n\nsuccess &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_success\"), sum) %&gt;%\n  filter(!is.na(polity2)) %&gt;%\n  mutate(outcome = \"succeeded\") %&gt;%\n  mutate(events=tl_success)%&gt;%\n  subset(select = -c(tl_success))\n\nfail &lt;- overpolity %&gt;%\n  group_by(polity2) %&gt;%\n  summarise_at(c(\"tl_failed\"), sum) %&gt;%\n  filter(!is.na(polity2))  %&gt;%\n  mutate(outcome = \"failed\") %&gt;%\n  mutate(events=tl_failed) %&gt;%\n  subset(select = -c(tl_failed))\n\ntlpolity &lt;- rbind(success, fail)\n\nggplot(tlpolity, aes(fill=outcome, y=events, x=polity2)) +\n  geom_bar(position=\"stack\", stat=\"identity\",)+\n  scale_fill_manual(values=c(\"dark green\", \"light green\")) +\n  labs(x = \"Polity\", y= \"Frequency\", caption=\"Overstaying data (Versteeg et al. 2020)\") +\n  ggtitle(\"Overstay Attempts over Regime\") +\n  scale_x_continuous(breaks=seq(-10, 10, 1))"
  },
  {
    "objectID": "slides/thinkingdata25.html#thinking-about-the-dgp",
    "href": "slides/thinkingdata25.html#thinking-about-the-dgp",
    "title": "Thinking About Data",
    "section": "Thinking about the DGP",
    "text": "Thinking about the DGP\n\nWhat are the units of observation? Who is taking action or having action done to them? Are the units heterogeneous, and if so, how?\nWhat circumstances are the units in?\nWhat are the units’ choice sets?\nWhat relates the units circumstances to the outcomes?\nHow are the units related to each other?\nWhat don’t we observe?"
  },
  {
    "objectID": "slides/thinkingdata25.html#can-the-dgp-exist",
    "href": "slides/thinkingdata25.html#can-the-dgp-exist",
    "title": "Thinking About Data",
    "section": "Can the DGP exist?",
    "text": "Can the DGP exist?\n\ncan the units experience the causal claim in question?\ndoes the data represent the enter DGP or just part of it?"
  },
  {
    "objectID": "code 2.html",
    "href": "code 2.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1 answers\nexercise #2 answers\nexercise #3 answers\nexercise #4 answers\nsimulation shiny app\nsimulation code\ncentral limit theorem shiny app\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/linearOLS.html",
    "href": "slides/linearOLS.html",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "This document demonstrates the process of linear regression, starting with data simulation, estimation using R’s lm() function, and then replicating the process using matrix algebra. We’ll work with both simulated and real-world data (Boston housing dataset).\n\n\n\nWe’ll simulate data based on the following model:\ny = b_0 + b_1*x1 + b_2*x2 + error\nwhere:\n\ny is a continuous, positive outcome variable.\nx1 is a binary variable.\nx2 is a continuous variable.\n\n\nset.seed(8675309)  # For reproducibility\nn &lt;- 100 # Number of observations\n\n# Simulate data\nx1 &lt;- rbinom(n, 1, 0.5)\nx2 &lt;- rnorm(n, 10, 2)\nerror &lt;- rnorm(n, 0, 3)  # Error term\nb0 &lt;- 5\nb1 &lt;- 2\nb2 &lt;- -0.5\ny &lt;- b0 + b1 * x1 + b2 * x2 + error\n\nsim_data &lt;- data.frame(y, x1, x2)\n\n\n\n\nmodel_lm &lt;- lm(y ~ x1 + x2, data = sim_data)\nsummary(model_lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0134 -2.2345  0.3141  2.3522  8.4468 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.7208     1.8970   1.961   0.0527 .  \nx1            2.7952     0.6710   4.166 6.73e-05 ***\nx2           -0.4278     0.1837  -2.329   0.0219 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.353 on 97 degrees of freedom\nMultiple R-squared:  0.1874,    Adjusted R-squared:  0.1707 \nF-statistic: 11.19 on 2 and 97 DF,  p-value: 4.254e-05\n\n# Create a table of coefficients\nlm_coef_table &lt;- knitr::kable(coef(summary(model_lm)), digits = 3, caption = \"Coefficients from `lm()`\")\nlm_coef_table\n\n\nCoefficients from lm()\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3.721\n1.897\n1.961\n0.053\n\n\nx1\n2.795\n0.671\n4.166\n0.000\n\n\nx2\n-0.428\n0.184\n-2.329\n0.022\n\n\n\n\n\n\n\ncode\n#plot predictions in highcharter \n\n# Prediction Plot\n\npred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data, \"scatter\", hcaes(x = x2, y = y), name = \"Actual Data (x1=0)\") %&gt;%\n  hc_add_series(data.frame(x2 = x2_pred, pred = predictions), \"line\", hcaes(x=x2, y = pred), name=\"Predicted\") %&gt;%\n  hc_title(text = \"Regression Predictions (lm)\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"y\"))\npred_plot\n\n\n\n\n\n\n\n\ncode\n# Residual Plot\nresid_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data, \"column\", hcaes(x = x2, y = pred - y), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (lm)\")\n\nresid_plot\n\n\n\n\n\n\n\n\n\n\n# Create design matrix X and outcome vector y\nX &lt;- cbind(1, sim_data$x1, sim_data$x2)\ny &lt;- sim_data$y\n\n# Calculate coefficients using matrix algebra\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\n# Calculate standard errors\nresiduals &lt;- y - X %*% b\nsigma_sq &lt;- sum(residuals^2) / (n - ncol(X))\nvar_b &lt;- sigma_sq * solve(t(X) %*% X)\nse_b &lt;- sqrt(diag(var_b))\n\n# Create a table of coefficients\nmatrix_coef_table &lt;- knitr::kable(data.frame(Estimate = b, `Std. Error` = se_b, row.names = c(\"Intercept\", \"x1\", \"x2\")), digits = 3, caption = \"Coefficients from Matrix Algebra\")\n\nmatrix_coef_table\n\n\nCoefficients from Matrix Algebra\n\n\n\nEstimate\nStd..Error\n\n\n\n\nIntercept\n3.721\n1.897\n\n\nx1\n2.795\n0.671\n\n\nx2\n-0.428\n0.184\n\n\n\n\n\n\n# Predictions and plots (matrix)\nX_pred &lt;- cbind(1, x1_pred, x2_pred)\npredictions_matrix &lt;- X_pred %*% b\n\n# Prediction Plot\npred_plot_matrix &lt;- highchart() %&gt;%\n hc_add_series(plot_data, \"scatter\", hcaes(x = x2, y = y), name = \"Actual Data (x1=0)\") %&gt;%\n  hc_add_series(data.frame(x2 = x2_pred, pred = predictions_matrix), \"line\", hcaes(x=x2, y = pred), name=\"Predicted\") %&gt;%\n    hc_title(text = \"Regression Predictions (Matrix)\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"y\"))\n\npred_plot_matrix\n\n\n\n\n\n\nresid_plot_matrix &lt;- highchart() %&gt;%\n    hc_add_series(plot_data, \"column\", hcaes(x = x2, y = pred - y), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (Matrix)\")\n\nresid_plot_matrix\n\n\n\n\n\n\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nWe estimate this model: medv ~ crim + age + tax + black + dis + ptratio\n\nboston_model &lt;- lm(medv ~ crim + age + tax + black + dis + ptratio, data = Boston)\nsummary(boston_model)\n\n\nCall:\nlm(formula = medv ~ crim + age + tax + black + dis + ptratio, \n    data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.671  -4.151  -1.315   2.270  33.143 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 58.773732   3.721963  15.791  &lt; 2e-16 ***\ncrim        -0.140667   0.046639  -3.016 0.002691 ** \nage         -0.094768   0.017458  -5.428 8.89e-08 ***\ntax         -0.007048   0.002845  -2.477 0.013576 *  \nblack        0.014560   0.003972   3.666 0.000273 ***\ndis         -0.922736   0.238417  -3.870 0.000123 ***\nptratio     -1.519762   0.166843  -9.109  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.179 on 499 degrees of freedom\nMultiple R-squared:  0.398, Adjusted R-squared:  0.3907 \nF-statistic: 54.98 on 6 and 499 DF,  p-value: &lt; 2.2e-16\n\nboston_lm_coef_table &lt;- knitr::kable(coef(summary(boston_model)), digits = 3, caption = \"Boston Model Coefficients (lm)\")\nboston_lm_coef_table\n\n\nBoston Model Coefficients (lm)\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n58.774\n3.722\n15.791\n0.000\n\n\ncrim\n-0.141\n0.047\n-3.016\n0.003\n\n\nage\n-0.095\n0.017\n-5.428\n0.000\n\n\ntax\n-0.007\n0.003\n-2.477\n0.014\n\n\nblack\n0.015\n0.004\n3.666\n0.000\n\n\ndis\n-0.923\n0.238\n-3.870\n0.000\n\n\nptratio\n-1.520\n0.167\n-9.109\n0.000\n\n\n\n\n\n\n# Predictions for Boston data (lm)\ncentral_tendencies &lt;- apply(Boston[, c(\"crim\", \"age\", \"tax\", \"black\", \"dis\")], 2, median)\nptratio_pred &lt;- seq(min(Boston$ptratio), max(Boston$ptratio), length.out = 100)\n\nboston_new_data &lt;- data.frame(crim = central_tendencies[\"crim\"], \n                          age = central_tendencies[\"age\"],\n                          tax = central_tendencies[\"tax\"],\n                          black = central_tendencies[\"black\"],\n                          dis = central_tendencies[\"dis\"],\n                          ptratio = ptratio_pred)\n\nboston_predictions &lt;- predict(boston_model, newdata = boston_new_data)\n\n# Plotting data (using all observations for scatter)\nplot_data_boston_lm &lt;- Boston\nplot_data_boston_lm$pred &lt;- predict(boston_model)\n\n# Prediction plot\nboston_lm_pred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"scatter\", hcaes(x = ptratio, y = medv), name = \"Actual Data\") %&gt;%\n  hc_add_series(data.frame(ptratio = ptratio_pred, pred = boston_predictions), \"line\", hcaes(x=ptratio, y = pred), name=\"Predicted\") %&gt;%\n  hc_title(text = \"Boston Housing Predictions (lm)\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"medv\"))\n\nboston_lm_pred_plot\n\n\n\n\n# Residuals plot\nboston_lm_resid_plot &lt;- highchart() %&gt;%\n hc_add_series(plot_data_boston_lm, \"column\", hcaes(x = ptratio, y = pred - medv), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (lm)\")\n\nboston_lm_resid_plot\n\n\n\n\n\n\n\n\nX_boston &lt;- model.matrix(boston_model) \ny_boston &lt;- Boston$medv\n\nb_boston &lt;- solve(t(X_boston) %*% X_boston) %*% t(X_boston) %*% y_boston\n\n# ... (Calculate standard errors, t-values, p-values similarly to the simulated data example.  Include this code if needed).\n\n\nboston_matrix_coef_table &lt;- knitr::kable(data.frame(Estimate=b_boston), digits = 3, caption = \"Boston Model Coefficients (Matrix)\")\n\nboston_matrix_coef_table\n\n\nBoston Model Coefficients (Matrix)\n\n\n\nEstimate\n\n\n\n\n(Intercept)\n58.774\n\n\ncrim\n-0.141\n\n\nage\n-0.095\n\n\ntax\n-0.007\n\n\nblack\n0.015\n\n\ndis\n-0.923\n\n\nptratio\n-1.520\n\n\n\n\n\n\n# Predictions for Boston data (Matrix)\nX_boston_pred &lt;- cbind(1, central_tendencies[\"crim\"], central_tendencies[\"age\"], central_tendencies[\"tax\"],\n                         central_tendencies[\"black\"], central_tendencies[\"dis\"], ptratio_pred)\n\nboston_predictions_matrix &lt;- X_boston_pred %*% b_boston\n\n# Prediction Plot\nboston_matrix_pred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"scatter\", hcaes(x = ptratio, y = medv), name = \"Actual Data\") %&gt;%\n  hc_add_series(data.frame(ptratio = ptratio_pred, pred = boston_predictions_matrix), \"line\", hcaes(x = ptratio, y = pred), name = \"Predicted\") %&gt;%\n  hc_title(text = \"Boston Housing Predictions (Matrix)\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"medv\"))\n\nboston_matrix_pred_plot\n\n\n\n\n# Residual plot (from lm predictions - for fair comparison)\nboston_matrix_resid_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"column\", hcaes(x = ptratio, y = pred - medv), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (Matrix)\")\n\nboston_matrix_resid_plot"
  },
  {
    "objectID": "slides/linearOLS.html#introduction",
    "href": "slides/linearOLS.html#introduction",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "This document demonstrates the process of linear regression, starting with data simulation, estimation using R’s lm() function, and then replicating the process using matrix algebra. We’ll work with both simulated and real-world data (Boston housing dataset)."
  },
  {
    "objectID": "slides/linearOLS.html#simulated-data-example",
    "href": "slides/linearOLS.html#simulated-data-example",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "We’ll simulate data based on the following model:\ny = b_0 + b_1*x1 + b_2*x2 + error\nwhere:\n\ny is a continuous, positive outcome variable.\nx1 is a binary variable.\nx2 is a continuous variable.\n\n\nset.seed(8675309)  # For reproducibility\nn &lt;- 100 # Number of observations\n\n# Simulate data\nx1 &lt;- rbinom(n, 1, 0.5)\nx2 &lt;- rnorm(n, 10, 2)\nerror &lt;- rnorm(n, 0, 3)  # Error term\nb0 &lt;- 5\nb1 &lt;- 2\nb2 &lt;- -0.5\ny &lt;- b0 + b1 * x1 + b2 * x2 + error\n\nsim_data &lt;- data.frame(y, x1, x2)\n\n\n\n\nmodel_lm &lt;- lm(y ~ x1 + x2, data = sim_data)\nsummary(model_lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0134 -2.2345  0.3141  2.3522  8.4468 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.7208     1.8970   1.961   0.0527 .  \nx1            2.7952     0.6710   4.166 6.73e-05 ***\nx2           -0.4278     0.1837  -2.329   0.0219 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.353 on 97 degrees of freedom\nMultiple R-squared:  0.1874,    Adjusted R-squared:  0.1707 \nF-statistic: 11.19 on 2 and 97 DF,  p-value: 4.254e-05\n\n# Create a table of coefficients\nlm_coef_table &lt;- knitr::kable(coef(summary(model_lm)), digits = 3, caption = \"Coefficients from `lm()`\")\nlm_coef_table\n\n\nCoefficients from lm()\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n3.721\n1.897\n1.961\n0.053\n\n\nx1\n2.795\n0.671\n4.166\n0.000\n\n\nx2\n-0.428\n0.184\n-2.329\n0.022\n\n\n\n\n\n\n\ncode\n#plot predictions in highcharter \n\n# Prediction Plot\n\npred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data, \"scatter\", hcaes(x = x2, y = y), name = \"Actual Data (x1=0)\") %&gt;%\n  hc_add_series(data.frame(x2 = x2_pred, pred = predictions), \"line\", hcaes(x=x2, y = pred), name=\"Predicted\") %&gt;%\n  hc_title(text = \"Regression Predictions (lm)\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"y\"))\npred_plot\n\n\n\n\n\n\n\n\ncode\n# Residual Plot\nresid_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data, \"column\", hcaes(x = x2, y = pred - y), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (lm)\")\n\nresid_plot\n\n\n\n\n\n\n\n\n\n\n# Create design matrix X and outcome vector y\nX &lt;- cbind(1, sim_data$x1, sim_data$x2)\ny &lt;- sim_data$y\n\n# Calculate coefficients using matrix algebra\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\n# Calculate standard errors\nresiduals &lt;- y - X %*% b\nsigma_sq &lt;- sum(residuals^2) / (n - ncol(X))\nvar_b &lt;- sigma_sq * solve(t(X) %*% X)\nse_b &lt;- sqrt(diag(var_b))\n\n# Create a table of coefficients\nmatrix_coef_table &lt;- knitr::kable(data.frame(Estimate = b, `Std. Error` = se_b, row.names = c(\"Intercept\", \"x1\", \"x2\")), digits = 3, caption = \"Coefficients from Matrix Algebra\")\n\nmatrix_coef_table\n\n\nCoefficients from Matrix Algebra\n\n\n\nEstimate\nStd..Error\n\n\n\n\nIntercept\n3.721\n1.897\n\n\nx1\n2.795\n0.671\n\n\nx2\n-0.428\n0.184\n\n\n\n\n\n\n# Predictions and plots (matrix)\nX_pred &lt;- cbind(1, x1_pred, x2_pred)\npredictions_matrix &lt;- X_pred %*% b\n\n# Prediction Plot\npred_plot_matrix &lt;- highchart() %&gt;%\n hc_add_series(plot_data, \"scatter\", hcaes(x = x2, y = y), name = \"Actual Data (x1=0)\") %&gt;%\n  hc_add_series(data.frame(x2 = x2_pred, pred = predictions_matrix), \"line\", hcaes(x=x2, y = pred), name=\"Predicted\") %&gt;%\n    hc_title(text = \"Regression Predictions (Matrix)\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"y\"))\n\npred_plot_matrix\n\n\n\n\n\n\nresid_plot_matrix &lt;- highchart() %&gt;%\n    hc_add_series(plot_data, \"column\", hcaes(x = x2, y = pred - y), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"x2\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (Matrix)\")\n\nresid_plot_matrix"
  },
  {
    "objectID": "slides/linearOLS.html#boston-housing-data-example",
    "href": "slides/linearOLS.html#boston-housing-data-example",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "library(MASS)\ndata(Boston)\n\nWe estimate this model: medv ~ crim + age + tax + black + dis + ptratio\n\nboston_model &lt;- lm(medv ~ crim + age + tax + black + dis + ptratio, data = Boston)\nsummary(boston_model)\n\n\nCall:\nlm(formula = medv ~ crim + age + tax + black + dis + ptratio, \n    data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.671  -4.151  -1.315   2.270  33.143 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 58.773732   3.721963  15.791  &lt; 2e-16 ***\ncrim        -0.140667   0.046639  -3.016 0.002691 ** \nage         -0.094768   0.017458  -5.428 8.89e-08 ***\ntax         -0.007048   0.002845  -2.477 0.013576 *  \nblack        0.014560   0.003972   3.666 0.000273 ***\ndis         -0.922736   0.238417  -3.870 0.000123 ***\nptratio     -1.519762   0.166843  -9.109  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.179 on 499 degrees of freedom\nMultiple R-squared:  0.398, Adjusted R-squared:  0.3907 \nF-statistic: 54.98 on 6 and 499 DF,  p-value: &lt; 2.2e-16\n\nboston_lm_coef_table &lt;- knitr::kable(coef(summary(boston_model)), digits = 3, caption = \"Boston Model Coefficients (lm)\")\nboston_lm_coef_table\n\n\nBoston Model Coefficients (lm)\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n58.774\n3.722\n15.791\n0.000\n\n\ncrim\n-0.141\n0.047\n-3.016\n0.003\n\n\nage\n-0.095\n0.017\n-5.428\n0.000\n\n\ntax\n-0.007\n0.003\n-2.477\n0.014\n\n\nblack\n0.015\n0.004\n3.666\n0.000\n\n\ndis\n-0.923\n0.238\n-3.870\n0.000\n\n\nptratio\n-1.520\n0.167\n-9.109\n0.000\n\n\n\n\n\n\n# Predictions for Boston data (lm)\ncentral_tendencies &lt;- apply(Boston[, c(\"crim\", \"age\", \"tax\", \"black\", \"dis\")], 2, median)\nptratio_pred &lt;- seq(min(Boston$ptratio), max(Boston$ptratio), length.out = 100)\n\nboston_new_data &lt;- data.frame(crim = central_tendencies[\"crim\"], \n                          age = central_tendencies[\"age\"],\n                          tax = central_tendencies[\"tax\"],\n                          black = central_tendencies[\"black\"],\n                          dis = central_tendencies[\"dis\"],\n                          ptratio = ptratio_pred)\n\nboston_predictions &lt;- predict(boston_model, newdata = boston_new_data)\n\n# Plotting data (using all observations for scatter)\nplot_data_boston_lm &lt;- Boston\nplot_data_boston_lm$pred &lt;- predict(boston_model)\n\n# Prediction plot\nboston_lm_pred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"scatter\", hcaes(x = ptratio, y = medv), name = \"Actual Data\") %&gt;%\n  hc_add_series(data.frame(ptratio = ptratio_pred, pred = boston_predictions), \"line\", hcaes(x=ptratio, y = pred), name=\"Predicted\") %&gt;%\n  hc_title(text = \"Boston Housing Predictions (lm)\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"medv\"))\n\nboston_lm_pred_plot\n\n\n\n\n# Residuals plot\nboston_lm_resid_plot &lt;- highchart() %&gt;%\n hc_add_series(plot_data_boston_lm, \"column\", hcaes(x = ptratio, y = pred - medv), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (lm)\")\n\nboston_lm_resid_plot\n\n\n\n\n\n\n\n\nX_boston &lt;- model.matrix(boston_model) \ny_boston &lt;- Boston$medv\n\nb_boston &lt;- solve(t(X_boston) %*% X_boston) %*% t(X_boston) %*% y_boston\n\n# ... (Calculate standard errors, t-values, p-values similarly to the simulated data example.  Include this code if needed).\n\n\nboston_matrix_coef_table &lt;- knitr::kable(data.frame(Estimate=b_boston), digits = 3, caption = \"Boston Model Coefficients (Matrix)\")\n\nboston_matrix_coef_table\n\n\nBoston Model Coefficients (Matrix)\n\n\n\nEstimate\n\n\n\n\n(Intercept)\n58.774\n\n\ncrim\n-0.141\n\n\nage\n-0.095\n\n\ntax\n-0.007\n\n\nblack\n0.015\n\n\ndis\n-0.923\n\n\nptratio\n-1.520\n\n\n\n\n\n\n# Predictions for Boston data (Matrix)\nX_boston_pred &lt;- cbind(1, central_tendencies[\"crim\"], central_tendencies[\"age\"], central_tendencies[\"tax\"],\n                         central_tendencies[\"black\"], central_tendencies[\"dis\"], ptratio_pred)\n\nboston_predictions_matrix &lt;- X_boston_pred %*% b_boston\n\n# Prediction Plot\nboston_matrix_pred_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"scatter\", hcaes(x = ptratio, y = medv), name = \"Actual Data\") %&gt;%\n  hc_add_series(data.frame(ptratio = ptratio_pred, pred = boston_predictions_matrix), \"line\", hcaes(x = ptratio, y = pred), name = \"Predicted\") %&gt;%\n  hc_title(text = \"Boston Housing Predictions (Matrix)\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"medv\"))\n\nboston_matrix_pred_plot\n\n\n\n\n# Residual plot (from lm predictions - for fair comparison)\nboston_matrix_resid_plot &lt;- highchart() %&gt;%\n  hc_add_series(plot_data_boston_lm, \"column\", hcaes(x = ptratio, y = pred - medv), name = \"Residuals\", color=\"grey\") %&gt;%\n  hc_xAxis(title = list(text = \"ptratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Plot of Residuals (Matrix)\")\n\nboston_matrix_resid_plot"
  },
  {
    "objectID": "linearmodelfirstlook.html",
    "href": "linearmodelfirstlook.html",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "These slides are aimed at estimating and interpreting the linear model in OLS by walking through all the various pieces and parts of the model. The goal here is to start with some data and to use those data to manually generate OLS estimates of \\(\\beta\\) and \\(\\sigma^2\\) and to compare those estimates to the output from R’s lm() function. I want to show you how the estimates are generated and how they relate to the data, and to start the semester demystifying what lm() is doing.\nThink of these slides as describing much of what we’ll study in the next few weeks - I don’t expect you to “get” all this right now. But I do want you to see where we’re going and to start to get the intuition of OLS estimation.\n\n\n\nLet’s create a very small data set (n=6) - just so we have some nouns to use, we’re going to call the \\(y\\) variable “social insurance spending” as a percent of total spending, and the \\(x\\) variable “gdp”. We’ll also include a dummy variable for democracy.\n\n\ncode\ndata &lt;- c(.18, 1, 28, .05, 0, 11, .23, 1, 45, .07, 0, 20, .12, 1, 30, .12, 1, 5)\ndm &lt;- matrix(data, nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(dm) &lt;- c('socins', 'dem', 'gdp')\ndf &lt;- as.data.frame(dm)\n\n#print the data in a table using kableextra; add a title to the table; add bootstrap styling to the table; make the table responsive.Title the table \"Data\".\n\nlibrary(kableExtra)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\n\n\n\n\n0.18\n1\n28\n\n\n0.05\n0\n11\n\n\n0.23\n1\n45\n\n\n0.07\n0\n20\n\n\n0.12\n1\n30\n\n\n0.12\n1\n5"
  },
  {
    "objectID": "linearmodelfirstlook.html#multivariate-model-and-quantities",
    "href": "linearmodelfirstlook.html#multivariate-model-and-quantities",
    "title": "Linear Regression in OLS",
    "section": "Multivariate Model and Quantities",
    "text": "Multivariate Model and Quantities\nLet’s now estimate a multivariate model. We’ll include both “democracy” and “gdp” as predictors of social insurance spending. The model is:\n\n\ncode\n#refresh data\ndf &lt;- as.data.frame(dm)\n\nols3 &lt;- lm(socins ~ dem + gdp, data = df)\n#summary(ols2)\n\nmodelsummary(\n  models = ols3,\n  stars = TRUE,\n  format=\"pipe\",\n#  output = \"html\",\n  title = \"OLS Estimates\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.022  \n                \n                \n                             \n                  (0.030)\n                \n                \n                  dem        \n                  0.074  \n                \n                \n                             \n                  (0.032)\n                \n                \n                  gdp        \n                  0.002  \n                \n                \n                             \n                  (0.001)\n                \n                \n                  R2         \n                  0.851  \n                \n                \n                  R2 Adj.    \n                  0.751  \n                \n        \n      \n    \n\n\n\nTo generate predictions in the multivariate model, we need to hold variables other than our variable of interest constant at some meaningful value (often, central tendency). In this case, I’m going to vary GDP (my x-axis representing my variable of interest), and compute predictions at both values of “democracy” (0 and 1).\n\n\ncode\n#compute average predictions for dem=0 and dem=1\n\ndfpreds &lt;- df\ndfpreds$dem &lt;- 0\nfit &lt;- data.frame(predict(ols3, newdata = dfpreds, interval = \"confidence\", se.fit = TRUE))\ndf$nondemxb &lt;- fit$fit.fit\ndfpreds$dem &lt;- 1\nfit &lt;- data.frame(predict(ols3, newdata = dfpreds, interval = \"confidence\", se.fit = TRUE))\ndf$demxb &lt;- fit$fit.fit\n\n\n#plot predictions\nhighchart() %&gt;%\n  hc_add_series(df, type = \"line\", hcaes(x = gdp, y = demxb), name = \"Dem = 1\", color=\"#005A43\") %&gt;%\n  hc_add_series(df, type = \"line\", hcaes(x = gdp, y = nondemxb), name = \"Dem = 0\", color=\"#6CC24A\") %&gt;%\n  #hc_add_series(df, type = \"scatter\", hcaes(x = gdp, y = socins), name = \"Actual\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Social Spending\"))\n\n\n\n\n\n\nThe upper line reports the predicted social insurance spending for democracies over GDP; the lower line reports the predicted social insurance spending for non-democracies over GDP. The distance between the two lines is the difference in the mean of social insurance spending between democracies and non-democracies; it’s the value of the “democracy” coefficient, 0.074. Also, note that the lines are parellel - this is by construction since we’ve only estimated one slope (GDP) despite having two regime groups. This is the structural stability assumption, something we’ll explore in the coming weeks."
  },
  {
    "objectID": "linearmodelfirstlook.html#real-data-boston-housing-values",
    "href": "linearmodelfirstlook.html#real-data-boston-housing-values",
    "title": "Linear Regression in OLS",
    "section": "Real Data: Boston Housing Values",
    "text": "Real Data: Boston Housing Values\nLet’s now turn to a real data set - the Boston housing data set. We’ll estimate a model of median home value as a function of crime rate, age of the house, tax rate, black population, distance to employment centers, and pupil-teacher ratio. In the end, I want to plot predictions and confidence intervals over the range of pupil-teacher ratios.\nLet’s present the results two different ways: in a table and in a coefficient plot.\n\n\ncode\nlibrary(MASS)\nboston &lt;- data.frame(Boston)\n\ninmodel &lt;- subset(boston, select = c(medv, crim, age, tax, black, dis, ptratio))\n\nhvmodel1 &lt;- lm(medv ~ crim + age + tax + black + dis + ptratio, data = boston)\n\nmodelsummary(\n  models = hvmodel1,\n  stars = TRUE,\n  title = \"Boston Housing Values, OLS estimates\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Boston Housing Values, OLS estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  58.774***\n                \n                \n                             \n                  (3.722)  \n                \n                \n                  crim       \n                  -0.141** \n                \n                \n                             \n                  (0.047)  \n                \n                \n                  age        \n                  -0.095***\n                \n                \n                             \n                  (0.017)  \n                \n                \n                  tax        \n                  -0.007*  \n                \n                \n                             \n                  (0.003)  \n                \n                \n                  black      \n                  0.015*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  dis        \n                  -0.923***\n                \n                \n                             \n                  (0.238)  \n                \n                \n                  ptratio    \n                  -1.520***\n                \n                \n                             \n                  (0.167)  \n                \n                \n                  R2         \n                  0.398    \n                \n                \n                  R2 Adj.    \n                  0.391    \n                \n        \n      \n    \n\n\n\ncode\nmodelplot(hvmodel1, stars = TRUE, title = \"Boston Housing Values, OLS estimates\", coef_omit = c(\"Intercept\"))\n\n\n\n\n\n\n\n\n\nLet’s generate predicted housing values across the values of pupil-teacher ratio. We’ll hold the other variables at means, medians, or modes, and compute the predictions and confidence intervals.\n\n\ncode\n# make some data for at-means predictions \n#summary(boston$ptratio)\n\noos1 &lt;- data.frame(Intercept=1, crim=.25, age=77.5, tax=330,black=391.44, dis=3.2, ptratio= c(seq(12,22,1)))\n\nboston.predict &lt;- data.frame(oos1, predict(hvmodel1, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=oos1))\n\n# confidence bounds by end point transformation\nboston.predict &lt;- boston.predict %&gt;% mutate(ub=fit.fit+1.96*se.fit) %&gt;% mutate(lb=fit.fit-1.96*se.fit)\n\n\nhighchart() %&gt;%\n  hc_add_series(boston.predict, type = \"arearange\", hcaes(x = ptratio, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.2) %&gt;%\n  hc_add_series(boston.predict, type = \"line\", hcaes(x = ptratio, y = fit.fit), name = \"Predicted\", color=\"#6CC24A\") %&gt;%\n  hc_xAxis(title = list(text = \"Pupil-teacher ratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Predicted Housing Value\"))"
  },
  {
    "objectID": "linearmodelfirstlook.html#repression-example",
    "href": "linearmodelfirstlook.html#repression-example",
    "title": "Linear Regression in OLS",
    "section": "Repression Example",
    "text": "Repression Example\n\nacled &lt;- read.csv(\"/Users/dave/Documents/2013/PITF/analysis/protests2021/data/2022analysis/acled2022.csv\") # Replace with your actual file path\n\nacled &lt;- acled %&gt;% distinct()\nacled$event_date &lt;- as.Date(acled$event_date, format = \"%d %B %Y\")\nacled$perday &lt;- 1\n\nprotests &lt;- filter(acled, event_type == \"Protests\")\nstateviol &lt;- filter(acled, event_type == \"Violence against civilians\")\nprotests &lt;- aggregate(perday ~ event_date, data = protests, sum)\nstateviol &lt;- aggregate(perday ~ event_date, data = stateviol, sum)\n\nprotests$rollmean &lt;- rollmean(protests$perday, k = 7, fill = NA)\nstateviol$rollmean &lt;- rollmean(stateviol$perday, k = 7, fill = NA)\n\nprotests &lt;- protests %&gt;% filter(protests$event_date &lt; \"2022-06-25\")\nstateviol &lt;- stateviol %&gt;% filter(stateviol$event_date &lt; \"2022-06-25\")\n\n# ... (Rest of the data preparation code)\n\n\n# Convert event_date to numeric for highcharter\nprotests$event_date_numeric &lt;- as.numeric(protests$event_date)*86400000\nstateviol$event_date_numeric &lt;- as.numeric(stateviol$event_date)*86400000\n\nprotestplot &lt;- highchart() %&gt;%\n  hc_add_series(protests, type = \"line\", hcaes(x = event_date_numeric, y = rollmean), name = \"7-Day Avg\") %&gt;% # fix series name here\n  hc_add_series(protests, type = \"spline\", hcaes(x = event_date_numeric, y = rollmean), name = \"Trend\", smoothing = TRUE, method = \"loess\") %&gt;%\n   hc_xAxis(type = \"datetime\", dateTimeLabelFormats = list(month = \"%b %Y\"), tickInterval = 3600000 * 24 * 30 * 6) %&gt;%\n  hc_yAxis(title = list(text = \"Protests\")) %&gt;%\n  hc_title(text = \"Protests During the Pandemic\") %&gt;% # fix caption\n    hc_caption(text = \"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; 'ACLED data retrieved 7.18.22, http://acleddata.com\") %&gt;%\n  hc_xAxis(plotLines = list(list(color = \"red\", value = as.numeric(as.Date(\"2020-03-15\")) * 86400000, width = 2, zIndex = 5)))\n\n\nrepressionplot &lt;- highchart() %&gt;% # fix series name here\n  hc_add_series(stateviol, type = \"line\", hcaes(x = event_date_numeric, y = rollmean), name = \"7-Day Avg\") %&gt;%\n  hc_add_series(stateviol, type = \"spline\", hcaes(x = event_date_numeric, y = rollmean), name = \"Trend\", smoothing = TRUE, method = \"loess\") %&gt;%\n hc_xAxis(type = \"datetime\", dateTimeLabelFormats = list(month = \"%b %Y\"), tickInterval = 3600000 * 24 * 30 * 6) %&gt;%\n  hc_yAxis(title = list(text = \"Violence against civilians\")) %&gt;%\n hc_title(text = \"Violence Against Civilians During the Pandemic\") %&gt;%  # fix caption\n   hc_caption(text = \"7 day rolling averages; smoothed loess trend lines; red lines indicate 3.15.2020 COVID onset; ACLED data retrieved 7.18.22, http://acleddata.com\") %&gt;%\n hc_xAxis(plotLines = list(list(color = \"red\", value = as.numeric(as.Date(\"2020-03-15\")) * 86400000, width = 2, zIndex = 5)))\n\n\n\nprotestplot\n\n\n\n\nrepressionplot"
  },
  {
    "objectID": "linearmodelfirstlook.html#bivariate-model",
    "href": "linearmodelfirstlook.html#bivariate-model",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "The goal here is to start with some data and to use those data to manually generate OLS estimates of \\(\\beta\\) and \\(\\sigma^2\\). We’ll then compare those estimates to the output from R’s lm() function. I want to show you how the estimates are generated and how they relate to the data, and to start the semester demystifying what lm() is doing.\nThink of these slides as describing much of what we’ll study in the next few weeks - I don’t expect you to “get” all this right now. But I do want you to see where we’re going and to start to get the intuition of OLS estimation.\n\n\n\nLet’s create a very small data set (n=6) - just so we have some nouns to use, we’re going to call the \\(y\\) variable “social insurance spending” and the \\(x\\) variable “gdp”. We’ll also include a dummy variable for democracy.\n\n\ncode\ndata &lt;- c(.18, 1, 28, .05, 0, 11, .23, 1, 45, .07, 0, 20, .12, 1, 30, .12, 1, 5)\ndm &lt;- matrix(data, nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(dm) &lt;- c('socins', 'dem', 'gdp')\ndf &lt;- as.data.frame(dm)\n\n#print the data in a table using kableextra; add a title to the table; add bootstrap styling to the table; make the table responsive.Title the table \"Data\".\n\nlibrary(kableExtra)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\n\n\n\n\n0.18\n1\n28\n\n\n0.05\n0\n11\n\n\n0.23\n1\n45\n\n\n0.07\n0\n20\n\n\n0.12\n1\n30\n\n\n0.12\n1\n5\n\n\n\n\n\n\n\n\n\n\n\nLet’s estimate three models using R’s lm() function. The first model is a simple constant-only model, and the second model includes GDP as a predictor of social insurance spending. The third includes democracy.\n\n\ncode\nols0 &lt;- lm(socins ~ 1, data = df)\n\nols1 &lt;- lm(socins ~ gdp, data = df)\n\nols2 &lt;- lm(socins ~ dem , data = df)\n\nmodelsummary(\n  models = list(ols0, ols1, ols2),\n  stars = TRUE,\n  output = \"html\",\n  title = \"OLS Estimates\",\n  #estimate = \"std.error\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.128**\n                  0.045  \n                  0.060  \n                \n                \n                             \n                  (0.027)\n                  (0.040)\n                  (0.033)\n                \n                \n                  gdp        \n                         \n                  0.004+ \n                         \n                \n                \n                             \n                         \n                  (0.002)\n                         \n                \n                \n                  dem        \n                         \n                         \n                  0.103+ \n                \n                \n                             \n                         \n                         \n                  (0.040)\n                \n                \n                  R2         \n                  0.000  \n                  0.584  \n                  0.618  \n                \n                \n                  R2 Adj.    \n                  0.000  \n                  0.480  \n                  0.522  \n                \n        \n      \n    \n\n\n\nThe constant-only model (or null model) only estimates an intercept. The value of the intercept is the mean of the \\(y\\) variable as you can see below:\n\n# mean of social insurance \n\nmean(df$socins)\n\n[1] 0.1283333\n\n\nIn the null model, the estimate of the intercept is our best guess about \\(y\\) absent any other information - the sample mean of \\(y\\).\nLet’s look at the bivariate model including “democracy.” The coefficient on “democracy” is the difference in the mean of social insurance spending between democracies and non-democracies. Recall the “democracy” variable is binary, so the coefficient is the difference in the mean of social insurance spending between democracies and non-democracies - the difference between democracies and non-democracies in social insurance spending is 0.103, so 10%.\nWe can also compute predictions or expected values of social insurance spending. If “democracy” is zero (so the country is a non-democracy), the expected value of social insurance spending is 0.06 - so just the intercept, or 6%.\nIf “democracy” is one (so the country is a democracy), the expected value of social insurance spending is 0.16. So our best guess about non-democratic social spending is 16%; for democratic countries, it’s 16% (the intercept, .06 plus the democracy coefficient, .10).\n\n\n\n\n\n\nDifferential Intercepts\n\n\n\nWe can call the coefficient on a dummy variable (like “democracy”) a differential intercept - it’s measuring the difference in the y-axis between two groups.\n\n\n\n\n\nThe predictions from this model we’ll call \\(\\widehat{y}\\) - the predicted value of social insurance spending given a value of GDP. The residuals are the difference between the actual value of social insurance spending and the predicted value, \\(y-\\widehat{y}\\).\nFirst, let’s generate and plot the predictions from the bivariate model. We’ll have \\(N\\) predicted values, one for each observation in the model, compuated as \\(\\widehat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\). So let’s just plug the values of “gdp” in for \\(x\\) and compute the predicted values of social insurance spending. You can see the predictions in the table below and the plot of the predictions and actual values in the figure below.\n\n\ncode\nfit1 &lt;- predict(ols1, interval = \"confidence\", se.fit = TRUE)\npredictions &lt;- cbind(df, fit1)\npredictions$res &lt;- predictions$socins - predictions$fit.fit\n\n#table of predictions and the y variable social insurance spending; only include these two columns, exclude row numbers.\n\nkable(predictions[,c('socins', 'fit.fit')], \n      \"html\", \n      row.names = FALSE,\n      col.names = c(\"Social Insurance\", \"Predicted Values\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\n\n\nSocial Insurance\nPredicted Values\n\n\n\n\n0.18\n0.1456225\n\n\n0.05\n0.0848124\n\n\n0.23\n0.2064326\n\n\n0.07\n0.1170060\n\n\n0.12\n0.1527766\n\n\n0.12\n0.0633500\n\n\n\n\n\n\n\n\n\n\ncode\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\nhighchart() %&gt;%\n  hc_add_series(predictions, type = \"line\", hcaes(x = gdp, y = fit.fit), name = \"Predicted\", color=\"#005A43\") %&gt;%\n  hc_add_series(predictions, type = \"scatter\", hcaes(x = gdp, y = socins), name = \"Actual\", color=\"#6CC24A\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Social Spending\"))\n\n\n\n\n\n\nThe regression line (line of best fit) is the line that minimizes the sum of the squared residuals. The residuals are the vertical distances between the actual values and the predicted values. The residuals are shown in the plot below as the vertical bars between the actual values and the predicted values."
  },
  {
    "objectID": "linearmodelfirstlook.html#residual-plot",
    "href": "linearmodelfirstlook.html#residual-plot",
    "title": "Linear Regression in OLS",
    "section": "Residual Plot",
    "text": "Residual Plot\n\n\ncode\n#plot prediction line, scatter of actuals, and residuals as segment bars between prediction line and actual\n\n# First, create a helper function to create segment data\ncreate_segment_data &lt;- function(df) {\n  segments &lt;- lapply(1:nrow(df), function(i) {\n    list(\n      data = list(\n        list(x = df$gdp[i], y = df$socins[i]),\n        list(x = df$gdp[i], y = df$fit.fit[i])\n      )\n    )\n  })\n  return(segments)\n}\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n# Create the chart\nhighchart() %&gt;%\n  # Add the fitted line\n  hc_add_series(\n    data = predictions,\n    type = \"line\",\n    hcaes(x = gdp, y = fit.fit),\n    name = \"Fitted Line\",\n    color = \"#005A43\"\n  ) %&gt;%\n  # Add the actual points\n  hc_add_series(\n    data = predictions,\n    type = \"scatter\",\n    hcaes(x = gdp, y = socins),\n    name = \"Actual Values\",\n    color = \"#6CC24A\"\n  ) %&gt;%\n  # Add the connecting segments\n  hc_add_series_list(\n    create_segment_data(predictions) %&gt;%\n      lapply(function(x) {\n        x$type &lt;- \"line\"\n        x$dashStyle &lt;- \"Dash\"\n        x$color &lt;- \"#005A43\"\n        x$showInLegend &lt;- FALSE\n        x$enableMouseTracking &lt;- FALSE\n        x$linkedTo &lt;- \":previous\"\n        return(x)\n      })\n  ) %&gt;%\n  # Customize the chart\n  hc_xAxis(\n    title = list(text = \"GDP\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Social Insurance Spending\")\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) \n\n\n\n\n\n\nThe distances between observed and predicted values of \\(y\\) are the residuals. Since the regression line minimizes the sum of these squared distances, it might be apparent the positive and negative residuals are balanced around the line, and so cancel each other out. Here’s another way to visualize the residuals emphasizing that intuition. You’ll see the residuals sum to zero."
  },
  {
    "objectID": "linearmodelfirstlook.html#residuals-sum-to-zero",
    "href": "linearmodelfirstlook.html#residuals-sum-to-zero",
    "title": "Linear Regression in OLS",
    "section": "Residuals sum to zero",
    "text": "Residuals sum to zero\n\n\ncode\n# plot residuals as bars around zero with a reference line at zero. Compute the sum of the residuals. Round to 3 decimals and print this on the plot space.\n\ne &lt;- sum(predictions$res)\ne &lt;- round(e, 3)\n\n\nhighchart() %&gt;%\n  hc_add_series(predictions, type = \"column\", hcaes(x = gdp, y = res), name = \"Residuals\", color=\"#005A43\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Residuals\") %&gt;%\n  hc_plotOptions(column = list(stacking = \"normal\")) %&gt;%\n  hc_yAxis(plotLines = list(list(value = 0, color = \"#000000\", width = 2, zIndex = 4))) %&gt;%\n  hc_add_annotation(\n    labels = list(\n      list(\n        point = list(x = 0, y = 0),\n        text = paste(\"Sum of residuals: \", e)\n      )\n    )\n  )"
  },
  {
    "objectID": "linearmodelfirstlook.html#measuring-uncertainty",
    "href": "linearmodelfirstlook.html#measuring-uncertainty",
    "title": "Linear Regression in OLS",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\nThe most important quantities we get from any regression model are those measuring uncertainty. For all sorts of reasons, we should doubt our estimates - the model is probably specified incorrectly in a number of ways; the variables are mismeasured; the assumptions we make in the model are unreasonable; and so on. The residuals which we now know how to compute are the key to measuring uncertainty about the estimates of \\(\\widehat{\\beta}\\).\n\nResiduals\nLet’s sum the residuals and verify they sum to zero:\n\ne &lt;- sum(predictions$res)\nprint(e)\n\n[1] -2.220446e-16\n\n\nIf the sum is zero, so is the mean. This is a property of the OLS estimator, and the reason we sum the squared residuals - this gives us the sum squared error:\n\nSSE &lt;- sum(predictions$res^2)\nprint(SSE)\n\n[1] 0.009442229\n\n\nIf we average the sum squared residuals over the degrees of freedom, we get the variance of the residuals. This is our estimate of the variance of the error term, \\(\\sigma^2\\). If we take the square root of this, we get the residual standard error or RSE (also called the root mean squared error). This is the average distance between the observed values and the predicted values.\n\n#variance of the residuals or sigma^2\nsigma2 &lt;- SSE / (6 - 2)\nprint(sigma2)\n\n[1] 0.002360557\n\n#residual standard error or root mean squared error\nrse &lt;- sqrt(sigma2)\nprint(rse)\n\n[1] 0.04858557\n\n#verify same as lm() output\nsummary(ols1) # Compare RSE\n\n\nCall:\nlm(formula = socins ~ gdp, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n 0.03438 -0.03481  0.02357 -0.04701 -0.03278  0.05665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.045465   0.040220   1.130    0.322  \ngdp         0.003577   0.001510   2.368    0.077 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04859 on 4 degrees of freedom\nMultiple R-squared:  0.5837,    Adjusted R-squared:  0.4797 \nF-statistic: 5.609 on 1 and 4 DF,  p-value: 0.07696\n\n\nThe RSE is a measure of the typical size of a residual, and it’s in the units of the \\(y\\) variable, so useful in telling us how large or small our average error is.\nNow that we know where residuals come from, let’s move backwards and repeat what we’ve just done but in matrix notation - this will give us the same estimates as lm() but we’ll see how they’re generated, and it’ll give us some insight into where our standard errors come from."
  },
  {
    "objectID": "linearmodelfirstlook.html#matrix-estimation",
    "href": "linearmodelfirstlook.html#matrix-estimation",
    "title": "Linear Regression in OLS",
    "section": "Matrix Estimation",
    "text": "Matrix Estimation\nUsing the same data as above, let’s think about this in matrix terms - so we have a column vector, \\(y\\) (social insurance), and a matrix \\(X\\) of predictors (a column of ones for the intercept and a column for GDP). For the bivariate model, we’ll exclude “democracy” for now.\n\n\ncode\ndf &lt;- df %&gt;% mutate(cons = 1)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\", \"Constant\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\nConstant\n\n\n\n\n0.18\n1\n28\n1\n\n\n0.05\n0\n11\n1\n\n\n0.23\n1\n45\n1\n\n\n0.07\n0\n20\n1\n\n\n0.12\n1\n30\n1\n\n\n0.12\n1\n5\n1\n\n\n\n\n\n\n\n\nWe can estimate the coefficients of the bivariate model using matrix algebra. The formula for the OLS estimator is\n\\[\\widehat{\\beta} = (X'X)^{-1}X'y\\]\nFirst, let’s arrange the data into a matrix \\(X\\) and a vector \\(y\\) - note that our \\(X\\) matrix includes a column of ones to estimate the intercept.\n\n#make the data matrix X and the outcome vector y\ny &lt;- df$socins\ncons &lt;- rep(1, length(y))\nX &lt;- cbind(cons, df$gdp)\ncolnames(X) &lt;- c('cons', 'gdp')\n\n\n# Calculate coefficients using matrix algebra\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nprint(b)\n\n            [,1]\ncons 0.045464648\ngdp  0.003577066\n\n# Compare with lm() coefficients\ncoef(ols1)  \n\n(Intercept)         gdp \n0.045464648 0.003577066 \n\n\nYou can see our matrix estimates are the same as the lm() estimates.\nLet’s continue by computing the residuals - the difference between the observed values of \\(y\\) and the predicted values of \\(y\\) - so we’ll need to compute the predictions too:\n\n# Calculate residuals\ne &lt;- y - X %*% b\nprint(e)\n\n            [,1]\n[1,]  0.03437752\n[2,] -0.03481237\n[3,]  0.02356740\n[4,] -0.04700596\n[5,] -0.03277661\n[6,]  0.05665002\n\n# compute the sum of the residuals squared, compare to above\n\nSSEm &lt;- t(e) %*% e\nprint(SSEm)\n\n            [,1]\n[1,] 0.009442229\n\nprint(SSE)\n\n[1] 0.009442229\n\n#compute sigma^2 and compare to above\n\nsigma2m &lt;- (t(e) %*% e) * 1 / (6 - 2)\nprint(sigma2m)\n\n            [,1]\n[1,] 0.002360557\n\nprint(sigma2)\n\n[1] 0.002360557\n\n\nSo we’ve just replicated in matrix form what we did above in scalar form. Let’s now compute the standard errors of the coefficients. To do that, we need to compute the variance-covariance matrix of the coefficients. This is given by:\n\\[V(\\widehat{\\beta}) = \\sigma^2(X'X)^{-1}\\]\nThe estimated error variance multiplied by the inverse of the \\(X'X\\) matrix gives us the variance-covariance matrix of the coefficients. We’re effectively weighting error variance, \\(\\sigma^2\\), by the covariance of the \\(X\\) variables.\nCompute the variance-covariance matrix of the coefficients and compare to the lm() output:\n\n# variance-covariance matrix of the coefficients\nvcb &lt;- drop(sigma2) * solve(t(X) %*% X)\nprint(vcb)\n\n              cons           gdp\ncons  1.617679e-03 -5.284546e-05\ngdp  -5.284546e-05  2.281099e-06\n\n# Compare with lm() vcov\nvcov(ols1)  \n\n              (Intercept)           gdp\n(Intercept)  1.617679e-03 -5.284546e-05\ngdp         -5.284546e-05  2.281099e-06\n\n\nThe main diagonal elements of the variance-covariance matrix are the variances of the coefficients. The square roots of these are the standard errors of the coefficients. Let’s compute the standard errors and compare to the lm() output:\n\n# Calculate standard errors\nse_b &lt;- sqrt(diag(vcb))\nprint(se_b)\n\n       cons         gdp \n0.040220385 0.001510331 \n\n# Compare with lm() standard errors\nsummary(ols1)\n\n\nCall:\nlm(formula = socins ~ gdp, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n 0.03438 -0.03481  0.02357 -0.04701 -0.03278  0.05665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.045465   0.040220   1.130    0.322  \ngdp         0.003577   0.001510   2.368    0.077 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04859 on 4 degrees of freedom\nMultiple R-squared:  0.5837,    Adjusted R-squared:  0.4797 \nF-statistic: 5.609 on 1 and 4 DF,  p-value: 0.07696\n\n\nSo we’ve just replicated the lm() output using matrix algebra. We’ve seen how the estimates are generated and how the standard errors are computed."
  },
  {
    "objectID": "linearmodelfirstlook.html#ols-estimation",
    "href": "linearmodelfirstlook.html#ols-estimation",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "These slides are aimed at estimating and interpreting the linear model in OLS by walking through all the various pieces and parts of the model. The goal here is to start with some data and to use those data to manually generate OLS estimates of \\(\\beta\\) and \\(\\sigma^2\\) and to compare those estimates to the output from R’s lm() function. I want to show you how the estimates are generated and how they relate to the data, and to start the semester demystifying what lm() is doing.\nThink of these slides as describing much of what we’ll study in the next few weeks - I don’t expect you to “get” all this right now. But I do want you to see where we’re going and to start to get the intuition of OLS estimation.\n\n\n\nLet’s create a very small data set (n=6) - just so we have some nouns to use, we’re going to call the \\(y\\) variable “social insurance spending” as a percent of total spending, and the \\(x\\) variable “gdp”. We’ll also include a dummy variable for democracy.\n\n\ncode\ndata &lt;- c(.18, 1, 28, .05, 0, 11, .23, 1, 45, .07, 0, 20, .12, 1, 30, .12, 1, 5)\ndm &lt;- matrix(data, nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(dm) &lt;- c('socins', 'dem', 'gdp')\ndf &lt;- as.data.frame(dm)\n\n#print the data in a table using kableextra; add a title to the table; add bootstrap styling to the table; make the table responsive.Title the table \"Data\".\n\nlibrary(kableExtra)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\n\n\n\n\n0.18\n1\n28\n\n\n0.05\n0\n11\n\n\n0.23\n1\n45\n\n\n0.07\n0\n20\n\n\n0.12\n1\n30\n\n\n0.12\n1\n5"
  },
  {
    "objectID": "linearmodelfirstlook.html#interpreting-the-linear-model",
    "href": "linearmodelfirstlook.html#interpreting-the-linear-model",
    "title": "Linear Regression in OLS",
    "section": "Interpreting the Linear Model",
    "text": "Interpreting the Linear Model\nLet’s estimate three models using R’s lm() function. The first model is a simple constant-only model, and the second model includes GDP as a predictor of social insurance spending. The third includes democracy.\n\n\ncode\nols0 &lt;- lm(socins ~ 1, data = df)\n\nols1 &lt;- lm(socins ~ gdp, data = df)\n\nols2 &lt;- lm(socins ~ dem , data = df)\n\nmodelsummary(\n  models = list(ols0, ols1, ols2),\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.128**\n                  0.045  \n                  0.060  \n                \n                \n                             \n                  (0.027)\n                  (0.040)\n                  (0.033)\n                \n                \n                  gdp        \n                         \n                  0.004+ \n                         \n                \n                \n                             \n                         \n                  (0.002)\n                         \n                \n                \n                  dem        \n                         \n                         \n                  0.103+ \n                \n                \n                             \n                         \n                         \n                  (0.040)\n                \n                \n                  R2         \n                  0.000  \n                  0.584  \n                  0.618  \n                \n                \n                  R2 Adj.    \n                  0.000  \n                  0.480  \n                  0.522  \n                \n        \n      \n    \n\n\n\nThe constant-only model (or null model) only estimates an intercept. The value of the intercept is the mean of the \\(y\\) variable as you can see below:\n\n# mean of social insurance \n\nmean(df$socins)\n\n[1] 0.1283333\n\n\nIn the null model, the estimate of the intercept is our best guess about \\(y\\) absent any other information - the sample mean of \\(y\\).\nLet’s look at the bivariate model including “democracy.” The coefficient on “democracy” is the difference in the mean of social insurance spending between democracies and non-democracies. Recall the “democracy” variable is binary, so the coefficient is the difference in the mean of social insurance spending between democracies and non-democracies - the difference between democracies and non-democracies in social insurance spending is 0.103, so 10%.\nWe can also compute predictions or expected values of social insurance spending. If “democracy” is zero (so the country is a non-democracy), the expected value of social insurance spending is 0.06 - so just the intercept, or 6%.\nIf “democracy” is one (so the country is a democracy), the expected value of social insurance spending is 0.16. So our best guess about non-democratic social spending is 16%; for democratic countries, it’s 16% (the intercept, .06 plus the democracy coefficient, .10).\n\n\n\n\n\n\nDifferential Intercepts\n\n\n\nWe can call the coefficient on a dummy variable (like “democracy”) a differential intercept - it’s measuring the difference in the y-axis between two groups.\n\n\n\n\ncode\nlibrary(kableExtra)\n\ninterpret_df &lt;- data.frame(\n  Coefficient = c(\n    \"Intercept (β₀ = 0.06)\",\n    \"Democracy Coefficient (β₁ = 0.1025)\",\n    \"Predicted Value when Democracy = 0\",\n    \"Predicted Value when Democracy = 1\"\n  ),\n  Interpretation = c(\n    \"When Democracy = 0, the predicted social insurance spending is 0.06 (6% of GDP)\",\n    \"Moving from non-democracy (0) to democracy (1) is associated with an increase of 0.1025 (10.25 percentage points) in social insurance spending as a share of GDP\",\n    \"0.06 or 6% of GDP\",\n    \"0.1625 or 16.25% of GDP\"\n  )\n)\n\nkable(interpret_df, \n      \"html\",\n      col.names = c(\"Component\", \"Interpretation\"),\n      caption = \"Interpreting Differential Intercepts\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F) %&gt;%\n  column_spec(1, border_right = TRUE)\n\n\n\n\nInterpreting Differential Intercepts\n\n\nComponent\nInterpretation\n\n\n\n\nIntercept (β₀ = 0.06)\nWhen Democracy = 0, the predicted social insurance spending is 0.06 (6% of GDP)\n\n\nDemocracy Coefficient (β₁ = 0.1025)\nMoving from non-democracy (0) to democracy (1) is associated with an increase of 0.1025 (10.25 percentage points) in social insurance spending as a share of GDP\n\n\nPredicted Value when Democracy = 0\n0.06 or 6% of GDP\n\n\nPredicted Value when Democracy = 1\n0.1625 or 16.25% of GDP"
  },
  {
    "objectID": "linearmodelfirstlook.html#predictions-and-residuals",
    "href": "linearmodelfirstlook.html#predictions-and-residuals",
    "title": "Linear Regression in OLS",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\nThe predictions from this model we’ll call \\(\\widehat{y}\\) - the predicted value of social insurance spending given a value of GDP. The residuals are the difference between the actual value of social insurance spending and the predicted value, \\(y-\\widehat{y}\\).\nFirst, let’s generate and plot the predictions from the bivariate model. We’ll have \\(N\\) predicted values, one for each observation in the model, compuated as \\(\\widehat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\). So let’s just plug the values of “gdp” in for \\(x\\) and compute the predicted values of social insurance spending. You can see the predictions in the table below and the plot of the predictions and actual values in the figure below.\n\nBivariate model predictions\n\n\ncode\nfit1 &lt;- predict(ols1, interval = \"confidence\", se.fit = TRUE)\npredictions &lt;- cbind(df, fit1)\npredictions$res &lt;- predictions$socins - predictions$fit.fit\n\n#table of predictions and the y variable social insurance spending; only include these two columns, exclude row numbers.\n\nkable(predictions[,c('socins', 'fit.fit')], \n      \"html\", \n      row.names = FALSE,\n      col.names = c(\"Social Insurance\", \"Predicted Values\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\n\n\nSocial Insurance\nPredicted Values\n\n\n\n\n0.18\n0.1456225\n\n\n0.05\n0.0848124\n\n\n0.23\n0.2064326\n\n\n0.07\n0.1170060\n\n\n0.12\n0.1527766\n\n\n0.12\n0.0633500\n\n\n\n\n\n\n\n\nThe regression line (line of best fit) is the line that minimizes the sum of the squared residuals. The residuals are the vertical distances between the actual values and the predicted values. The residuals are shown in the plot below as the vertical bars between the actual values and the predicted values.\n\n\nBivariate model predictions and residuals\n\n\ncode\n#plot prediction line, scatter of actuals, and residuals as segment bars between prediction line and actual\n\n# First, create a helper function to create segment data\ncreate_segment_data &lt;- function(df) {\n  segments &lt;- lapply(1:nrow(df), function(i) {\n    list(\n      data = list(\n        list(x = df$gdp[i], y = df$socins[i]),\n        list(x = df$gdp[i], y = df$fit.fit[i])\n      )\n    )\n  })\n  return(segments)\n}\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n# Create the chart\nhighchart() %&gt;%\n  # Add the fitted line\n  hc_add_series(\n    data = predictions,\n    type = \"line\",\n    hcaes(x = gdp, y = fit.fit),\n    name = \"Fitted Line\",\n    color = \"#005A43\"\n  ) %&gt;%\n  # Add the actual points\n  hc_add_series(\n    data = predictions,\n    type = \"scatter\",\n    hcaes(x = gdp, y = socins),\n    name = \"Actual Values\",\n    color = \"#6CC24A\"\n  ) %&gt;%\n  # Add the connecting segments\n  hc_add_series_list(\n    create_segment_data(predictions) %&gt;%\n      lapply(function(x) {\n        x$type &lt;- \"line\"\n        x$dashStyle &lt;- \"Dash\"\n        x$color &lt;- \"#005A43\"\n        x$showInLegend &lt;- FALSE\n        x$enableMouseTracking &lt;- FALSE\n        x$linkedTo &lt;- \":previous\"\n        return(x)\n      })\n  ) %&gt;%\n  # Customize the chart\n  hc_xAxis(\n    title = list(text = \"GDP\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Social Insurance Spending\")\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) \n\n\n\n\n\n\nThe distances between observed and predicted values of \\(y\\) are the residuals. Since the regression line minimizes the sum of these squared distances, it might be apparent the positive and negative residuals are balanced around the line, and so cancel each other out. Here’s another way to visualize the residuals emphasizing that intuition. You’ll see the residuals sum to zero."
  },
  {
    "objectID": "linearmodelfirstlook.html#predictions-with-confidence-intervals",
    "href": "linearmodelfirstlook.html#predictions-with-confidence-intervals",
    "title": "Linear Regression in OLS",
    "section": "Predictions with Confidence Intervals",
    "text": "Predictions with Confidence Intervals\nLet’s return to the predictions from the bivariate model. In the Section 3.2 above, we plotted \\(\\widehat{y}\\) against \\(x\\) and the actual values of \\(y\\) but we didn’t express any uncertainty about those predictions - we did not compute standard error of the \\(\\widehat{y}\\) values, or compute confidence intervals. Let’s do that now.\nWe can compute the confidence intervals for the predictions using the formula:\n\\[\\widehat{y} \\pm t_{\\alpha/2, n-2} \\times \\text{SE}(\\widehat{y})\\]\nWhere do we get the standard errors of the predictions? We can compute these using the formula:\n\\[\\text{SE}(\\widehat{y}) = \\sqrt{diag(\\text{X}V\\text{X}')}\\].\nThis is the square root of the diagonal elements of the matrix product of the \\(X\\) matrix, the variance-covariance matrix of the coefficients, and the transpose of the \\(X\\) matrix.\n\n\ncode\npredictions &lt;- predictions %&gt;% mutate(ub = fit.fit + 1.96 * se.fit, lb = fit.fit - 1.96 * se.fit)\n#order predictions by gdp\npredictions &lt;- predictions[order(predictions$gdp),]\nhighchart() %&gt;%\n  hc_add_series(predictions, type = \"arearange\", hcaes(x = gdp, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.5) %&gt;%\n  hc_add_series(predictions, type = \"line\", hcaes(x = gdp, y = fit.fit), name = \"Predicted\", color=\"black\") %&gt;%\n  #hc_add_series(predictions, type = \"scatter\", hcaes(x = gdp, y = socins), name = \"Actual\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Social Spending\"))\n\n\n\n\n\n\nThe confidence intervals are wider where the data are more dispersed, and narrower where the data are more tightly clustered. The confidence intervals are a measure of the uncertainty in our predictions. One thing to notice is that, scanning from left-to-right, we could draw a horizontal line from the left-most upper bound to the right-most lower bound - which is to say we can’t say the change in social insurance spending is statistically significant across the range of GDP because the interval contains a line with slope=0."
  },
  {
    "objectID": "linearmodelfirstlook.html#predictions-using-matrix-algebra",
    "href": "linearmodelfirstlook.html#predictions-using-matrix-algebra",
    "title": "Linear Regression in OLS",
    "section": "Predictions using matrix algebra",
    "text": "Predictions using matrix algebra\nLet’s do this one last time computing the predictions and confidence intervals for the Boston housing data set using matrix algebra.\n\n\ncode\n# generate xb and st. errs of predictions by matrix: \n\n# get b vector\nb &lt;- coef(hvmodel1)\n\n# set up out of sample data, making sure vars in same order as in model\noos1 &lt;- data.frame(Intercept=1, crim=.25, age=77.5, tax=330,black=391.44, dis=3.2, ptratio= c(seq(12,22,1)))\nX &lt;- as.matrix(oos1)\n\nxb &lt;- X%*%b\n\n# now, get the var-cov matrix of b : var(p) = XVX'\nV &lt;- vcov(hvmodel1)\nvcp = X%*%V%*%t(X)\nvarp&lt;- diag(vcp, names = TRUE)\n\n# compute upper and lower bounds by end point transformation\noos1 &lt;-data.frame(oos1, sep= sqrt(varp), xb, ub=xb+1.96*sqrt(varp), lb=xb-1.96*sqrt(varp))\n\n# plot\nhighchart() %&gt;%\n  hc_add_series(oos1, type = \"arearange\", hcaes(x = ptratio, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.2) %&gt;%\n  hc_add_series(oos1, type = \"line\", hcaes(x = ptratio, y = xb), name = \"Predicted\", color=\"#6CC24A\") %&gt;%\n  hc_xAxis(title = list(text = \"Pupil/Teacher Ratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Predicted Home Value\"))"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html",
    "href": "slides/linearmodelfirstlook.html",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "These slides are aimed at estimating and interpreting the linear model in OLS by walking through all the various pieces and parts of the model. The goal here is to start with some data and to use those data to manually generate OLS estimates of \\(\\beta\\) and \\(\\sigma^2\\) and to compare those estimates to the output from R’s lm() function. I want to show you how the estimates are generated and how they relate to the data, and to start the semester demystifying what lm() is doing.\nThink of these slides as describing much of what we’ll study in the next few weeks - I don’t expect you to “get” all this right now. But I do want you to see where we’re going and to start to get the intuition of OLS estimation.\n\n\n\nLet’s create a very small data set (n=6) - just so we have some nouns to use, we’re going to call the \\(y\\) variable “social insurance spending” as a percent of total spending, and the \\(x\\) variable “gdp”. We’ll also include a dummy variable for democracy.\n\n\ncode\ndata &lt;- c(.18, 1, 28, .05, 0, 11, .23, 1, 45, .07, 0, 20, .12, 1, 30, .12, 1, 5)\ndm &lt;- matrix(data, nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(dm) &lt;- c('socins', 'dem', 'gdp')\ndf &lt;- as.data.frame(dm)\n\n#print the data in a table using kableextra; add bootstrap styling to the table; make the table responsive.Title the table \"Data\".\n\nlibrary(kableExtra)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\n\n\n\n\n0.18\n1\n28\n\n\n0.05\n0\n11\n\n\n0.23\n1\n45\n\n\n0.07\n0\n20\n\n\n0.12\n1\n30\n\n\n0.12\n1\n5"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#ols-estimation",
    "href": "slides/linearmodelfirstlook.html#ols-estimation",
    "title": "Linear Regression in OLS",
    "section": "",
    "text": "These slides are aimed at estimating and interpreting the linear model in OLS by walking through all the various pieces and parts of the model. The goal here is to start with some data and to use those data to manually generate OLS estimates of \\(\\beta\\) and \\(\\sigma^2\\) and to compare those estimates to the output from R’s lm() function. I want to show you how the estimates are generated and how they relate to the data, and to start the semester demystifying what lm() is doing.\nThink of these slides as describing much of what we’ll study in the next few weeks - I don’t expect you to “get” all this right now. But I do want you to see where we’re going and to start to get the intuition of OLS estimation.\n\n\n\nLet’s create a very small data set (n=6) - just so we have some nouns to use, we’re going to call the \\(y\\) variable “social insurance spending” as a percent of total spending, and the \\(x\\) variable “gdp”. We’ll also include a dummy variable for democracy.\n\n\ncode\ndata &lt;- c(.18, 1, 28, .05, 0, 11, .23, 1, 45, .07, 0, 20, .12, 1, 30, .12, 1, 5)\ndm &lt;- matrix(data, nrow = 6, ncol = 3, byrow = TRUE)\ncolnames(dm) &lt;- c('socins', 'dem', 'gdp')\ndf &lt;- as.data.frame(dm)\n\n#print the data in a table using kableextra; add bootstrap styling to the table; make the table responsive.Title the table \"Data\".\n\nlibrary(kableExtra)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\n\n\n\n\n0.18\n1\n28\n\n\n0.05\n0\n11\n\n\n0.23\n1\n45\n\n\n0.07\n0\n20\n\n\n0.12\n1\n30\n\n\n0.12\n1\n5"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#interpreting-the-linear-model",
    "href": "slides/linearmodelfirstlook.html#interpreting-the-linear-model",
    "title": "Linear Regression in OLS",
    "section": "Interpreting the Linear Model",
    "text": "Interpreting the Linear Model\nLet’s estimate three models using R’s lm() function. The first model is a simple constant-only model, and the second model includes GDP as a predictor of social insurance spending. The third includes democracy.\n\n\ncode\nols0 &lt;- lm(socins ~ 1, data = df)\n\nols1 &lt;- lm(socins ~ gdp, data = df)\n\nols2 &lt;- lm(socins ~ dem , data = df)\n\nmodelsummary(\n  models = list(ols0, ols1, ols2),\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.128**\n                  0.045  \n                  0.060  \n                \n                \n                             \n                  (0.027)\n                  (0.040)\n                  (0.033)\n                \n                \n                  gdp        \n                         \n                  0.004+ \n                         \n                \n                \n                             \n                         \n                  (0.002)\n                         \n                \n                \n                  dem        \n                         \n                         \n                  0.103+ \n                \n                \n                             \n                         \n                         \n                  (0.040)\n                \n                \n                  R2         \n                  0.000  \n                  0.584  \n                  0.618  \n                \n                \n                  R2 Adj.    \n                  0.000  \n                  0.480  \n                  0.522  \n                \n        \n      \n    \n\n\n\nThe constant-only model (or null model) only estimates an intercept. The value of the intercept is the mean of the \\(y\\) variable as you can see below:\n\n# mean of social insurance \n\nmean(df$socins)\n\n[1] 0.1283333\n\n\nIn the null model, the estimate of the intercept is our best guess about \\(y\\) absent any other information - the sample mean of \\(y\\).\nLet’s look at the bivariate model including “democracy.” The coefficient on “democracy” is the difference in the mean of social insurance spending between democracies and non-democracies. Recall the “democracy” variable is binary, so the coefficient is the difference in the mean of social insurance spending between democracies and non-democracies - the difference between democracies and non-democracies in social insurance spending is 0.103, so 10%.\nWe can also compute predictions or expected values of social insurance spending. If “democracy” is zero (so the country is a non-democracy), the expected value of social insurance spending is 0.06 - so just the intercept, or 6%.\nIf “democracy” is one (so the country is a democracy), the expected value of social insurance spending is 0.16. So our best guess about non-democratic social spending is 16%; for democratic countries, it’s 16% (the intercept, .06 plus the democracy coefficient, .10).\n\n\n\n\n\n\nDifferential Intercepts\n\n\n\nWe can call the coefficient on a dummy variable (like “democracy”) a differential intercept - it’s measuring the difference in the y-axis between two groups.\n\n\n\n\ncode\nlibrary(kableExtra)\n\ninterpret_df &lt;- data.frame(\n  Coefficient = c(\n    \"Intercept (β₀ = 0.06)\",\n    \"Democracy Coefficient (β₁ = 0.1025)\",\n    \"Predicted Value when Democracy = 0\",\n    \"Predicted Value when Democracy = 1\"\n  ),\n  Interpretation = c(\n    \"When Democracy = 0, the predicted social insurance spending is 0.06 (6% of GDP)\",\n    \"Moving from non-democracy (0) to democracy (1) is associated with an increase of 0.1025 (10.25 percentage points) in social insurance spending as a share of GDP\",\n    \"0.06 or 6% of GDP\",\n    \"0.1625 or 16.25% of GDP\"\n  )\n)\n\nkable(interpret_df, \n      \"html\",\n      col.names = c(\"Component\", \"Interpretation\"),\n      caption = \"Interpreting Differential Intercepts\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F) %&gt;%\n  column_spec(1, border_right = TRUE)\n\n\n\n\nInterpreting Differential Intercepts\n\n\nComponent\nInterpretation\n\n\n\n\nIntercept (β₀ = 0.06)\nWhen Democracy = 0, the predicted social insurance spending is 0.06 (6% of GDP)\n\n\nDemocracy Coefficient (β₁ = 0.1025)\nMoving from non-democracy (0) to democracy (1) is associated with an increase of 0.1025 (10.25 percentage points) in social insurance spending as a share of GDP\n\n\nPredicted Value when Democracy = 0\n0.06 or 6% of GDP\n\n\nPredicted Value when Democracy = 1\n0.1625 or 16.25% of GDP"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#predictions-and-residuals",
    "href": "slides/linearmodelfirstlook.html#predictions-and-residuals",
    "title": "Linear Regression in OLS",
    "section": "Predictions and Residuals",
    "text": "Predictions and Residuals\nThe predictions from this model we’ll call \\(\\widehat{y}\\) - the predicted value of social insurance spending given a value of GDP. The residuals are the difference between the actual value of social insurance spending and the predicted value, \\(y-\\widehat{y}\\).\nFirst, let’s generate and plot the predictions from the bivariate model. We’ll have \\(N\\) predicted values, one for each observation in the model, compuated as \\(\\widehat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\). So let’s just plug the values of “gdp” in for \\(x\\) and compute the predicted values of social insurance spending. You can see the predictions in the table below and the plot of the predictions and actual values in the figure below.\n\nBivariate model predictions\n\n\ncode\nfit1 &lt;- predict(ols1, interval = \"confidence\", se.fit = TRUE)\npredictions &lt;- cbind(df, fit1)\npredictions$res &lt;- predictions$socins - predictions$fit.fit\n\n#table of predictions and the y variable social insurance spending; only include these two columns, exclude row numbers.\n\nkable(predictions[,c('socins', 'fit.fit')], \n      \"html\", \n      row.names = FALSE,\n      col.names = c(\"Social Insurance\", \"Predicted Values\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\n\n\nSocial Insurance\nPredicted Values\n\n\n\n\n0.18\n0.1456225\n\n\n0.05\n0.0848124\n\n\n0.23\n0.2064326\n\n\n0.07\n0.1170060\n\n\n0.12\n0.1527766\n\n\n0.12\n0.0633500\n\n\n\n\n\n\n\n\nThe regression line (line of best fit) is the line that minimizes the sum of the squared residuals. The residuals are the vertical distances between the actual values and the predicted values. The residuals are shown in the plot below as the vertical bars between the actual values and the predicted values.\n\n\nBivariate model predictions and residuals\n\n\ncode\n#plot prediction line, scatter of actuals, and residuals as segment bars between prediction line and actual\n\n# First, create a helper function to create segment data\ncreate_segment_data &lt;- function(df) {\n  segments &lt;- lapply(1:nrow(df), function(i) {\n    list(\n      data = list(\n        list(x = df$gdp[i], y = df$socins[i]),\n        list(x = df$gdp[i], y = df$fit.fit[i])\n      )\n    )\n  })\n  return(segments)\n}\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n# Create the chart\nhighchart() %&gt;%\n  # Add the fitted line\n  hc_add_series(\n    data = predictions,\n    type = \"line\",\n    hcaes(x = gdp, y = fit.fit),\n    name = \"Fitted Line\",\n    color = \"#005A43\"\n  ) %&gt;%\n  # Add the actual points\n  hc_add_series(\n    data = predictions,\n    type = \"scatter\",\n    hcaes(x = gdp, y = socins),\n    name = \"Actual Values\",\n    color = \"#6CC24A\"\n  ) %&gt;%\n  # Add the connecting segments\n  hc_add_series_list(\n    create_segment_data(predictions) %&gt;%\n      lapply(function(x) {\n        x$type &lt;- \"line\"\n        x$dashStyle &lt;- \"Dash\"\n        x$color &lt;- \"#005A43\"\n        x$showInLegend &lt;- FALSE\n        x$enableMouseTracking &lt;- FALSE\n        x$linkedTo &lt;- \":previous\"\n        return(x)\n      })\n  ) %&gt;%\n  # Customize the chart\n  hc_xAxis(\n    title = list(text = \"GDP\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Social Insurance Spending\")\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) \n\n\n\n\n\n\nThe distances between observed and predicted values of \\(y\\) are the residuals. Since the regression line minimizes the sum of these squared distances, it might be apparent the positive and negative residuals are balanced around the line, and so cancel each other out. Here’s another way to visualize the residuals emphasizing that intuition. You’ll see the residuals sum to zero."
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#residuals-sum-to-zero",
    "href": "slides/linearmodelfirstlook.html#residuals-sum-to-zero",
    "title": "Linear Regression in OLS",
    "section": "Residuals sum to zero",
    "text": "Residuals sum to zero\n\n\ncode\n# plot residuals as bars around zero with a reference line at zero. Compute the sum of the residuals. Round to 3 decimals and print this on the plot space.\n\ne &lt;- sum(predictions$res)\ne &lt;- round(e, 3)\n\n\nhighchart() %&gt;%\n  hc_add_series(predictions, type = \"column\", hcaes(x = gdp, y = res), name = \"Residuals\", color=\"#005A43\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\")) %&gt;%\n  hc_title(text = \"Residuals\") %&gt;%\n  hc_plotOptions(column = list(stacking = \"normal\")) %&gt;%\n  hc_yAxis(plotLines = list(list(value = 0, color = \"#000000\", width = 2, zIndex = 4))) %&gt;%\n  hc_add_annotation(\n    labels = list(\n      list(\n        point = list(x = 0, y = 0),\n        text = paste(\"Sum of residuals: \", e)\n      )\n    )\n  )"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#measuring-uncertainty",
    "href": "slides/linearmodelfirstlook.html#measuring-uncertainty",
    "title": "Linear Regression in OLS",
    "section": "Measuring Uncertainty",
    "text": "Measuring Uncertainty\nThe most important quantities we get from any regression model are those measuring uncertainty. For all sorts of reasons, we should doubt our estimates - the model is probably specified incorrectly in a number of ways; the variables are mismeasured; the assumptions we make in the model are unreasonable; and so on. The residuals which we now know how to compute are the key to measuring uncertainty about the estimates of \\(\\widehat{\\beta}\\).\n\nResiduals\nLet’s sum the residuals and verify they sum to zero:\n\ne &lt;- sum(predictions$res)\nprint(e)\n\n[1] -2.220446e-16\n\n\nIf the sum is zero, so is the mean. This is a property of the OLS estimator, and the reason we sum the squared residuals - this gives us the sum squared error:\n\nSSE &lt;- sum(predictions$res^2)\nprint(SSE)\n\n[1] 0.009442229\n\n\nIf we average the sum squared residuals over the degrees of freedom, we get the variance of the residuals. This is our estimate of the variance of the error term, \\(\\sigma^2\\). If we take the square root of this, we get the residual standard error or RSE. This is the average distance between the observed values and the predicted values.\n\n#variance of the residuals or sigma^2\nsigma2 &lt;- SSE / (6 - 2)\nprint(sigma2)\n\n[1] 0.002360557\n\n#residual standard error\nrse &lt;- sqrt(sigma2)\nprint(rse)\n\n[1] 0.04858557\n\n#verify same as lm() output\nsummary(ols1) # Compare RSE\n\n\nCall:\nlm(formula = socins ~ gdp, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n 0.03438 -0.03481  0.02357 -0.04701 -0.03278  0.05665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.045465   0.040220   1.130    0.322  \ngdp         0.003577   0.001510   2.368    0.077 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04859 on 4 degrees of freedom\nMultiple R-squared:  0.5837,    Adjusted R-squared:  0.4797 \nF-statistic: 5.609 on 1 and 4 DF,  p-value: 0.07696\n\n\nThe RSE is a measure of the typical size of a residual, and it’s in the units of the \\(y\\) variable, so useful in telling us how large or small our average error is.\nNow that we know where residuals come from, let’s move backwards and repeat what we’ve just done but in matrix notation - this will give us the same estimates as lm() but we’ll see how they’re generated, and it’ll give us some insight into where our standard errors come from."
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#matrix-estimation",
    "href": "slides/linearmodelfirstlook.html#matrix-estimation",
    "title": "Linear Regression in OLS",
    "section": "Matrix Estimation",
    "text": "Matrix Estimation\nUsing the same data as above, let’s think about this in matrix terms - so we have a column vector, \\(y\\) (social insurance), and a matrix \\(X\\) of predictors (a column of ones for the intercept and a column for GDP). For the bivariate model, we’ll exclude “democracy” for now.\n\n\ncode\ndf &lt;- df %&gt;% mutate(cons = 1)\nkable(df, \n      \"html\", \n      row.names = FALSE,\n      caption = \"Data\",\n      col.names = c(\"Social Insurance\", \"Democracy\", \"GDP\", \"Constant\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\nData\n\n\nSocial Insurance\nDemocracy\nGDP\nConstant\n\n\n\n\n0.18\n1\n28\n1\n\n\n0.05\n0\n11\n1\n\n\n0.23\n1\n45\n1\n\n\n0.07\n0\n20\n1\n\n\n0.12\n1\n30\n1\n\n\n0.12\n1\n5\n1\n\n\n\n\n\n\n\n\nWe can estimate the coefficients of the bivariate model using matrix algebra. The formula for the OLS estimator is\n\\[\\widehat{\\beta} = (X'X)^{-1}X'y\\]\nFirst, let’s arrange the data into a matrix \\(X\\) and a vector \\(y\\) - note that our \\(X\\) matrix includes a column of ones to estimate the intercept.\n\n#make the data matrix X and the outcome vector y\ny &lt;- df$socins\ncons &lt;- rep(1, length(y))\nX &lt;- cbind(cons, df$gdp)\ncolnames(X) &lt;- c('cons', 'gdp')\n\n\n# Calculate coefficients using matrix algebra\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nprint(b)\n\n            [,1]\ncons 0.045464648\ngdp  0.003577066\n\n# Compare with lm() coefficients\ncoef(ols1)  \n\n(Intercept)         gdp \n0.045464648 0.003577066 \n\n\nYou can see our matrix estimates are the same as the lm() estimates.\nLet’s continue by computing the residuals - the difference between the observed values of \\(y\\) and the predicted values of \\(y\\) - so we’ll need to compute the predictions too:\n\n# Calculate residuals\ne &lt;- y - X %*% b\nprint(e)\n\n            [,1]\n[1,]  0.03437752\n[2,] -0.03481237\n[3,]  0.02356740\n[4,] -0.04700596\n[5,] -0.03277661\n[6,]  0.05665002\n\n# compute the sum of the residuals squared, compare to above\n\nSSEm &lt;- t(e) %*% e\nprint(SSEm)\n\n            [,1]\n[1,] 0.009442229\n\nprint(SSE)\n\n[1] 0.009442229\n\n#compute sigma^2 and compare to above\n\nsigma2m &lt;- (t(e) %*% e) * 1 / (6 - 2)\nprint(sigma2m)\n\n            [,1]\n[1,] 0.002360557\n\nprint(sigma2)\n\n[1] 0.002360557\n\n\nSo we’ve just replicated in matrix form what we did above in scalar form. Let’s now compute the standard errors of the coefficients. To do that, we need to compute the variance-covariance matrix of the coefficients. This is given by:\n\\[V(\\widehat{\\beta}) = \\sigma^2(X'X)^{-1}\\]\nThe estimated error variance multiplied by the inverse of the \\(X'X\\) matrix gives us the variance-covariance matrix of the coefficients. We’re effectively weighting error variance, \\(\\sigma^2\\), by the covariance of the \\(X\\) variables.\nCompute the variance-covariance matrix of the coefficients and compare to the lm() output:\n\n# variance-covariance matrix of the coefficients\nvcb &lt;- drop(sigma2) * solve(t(X) %*% X)\nprint(vcb)\n\n              cons           gdp\ncons  1.617679e-03 -5.284546e-05\ngdp  -5.284546e-05  2.281099e-06\n\n# Compare with lm() vcov\nvcov(ols1)  \n\n              (Intercept)           gdp\n(Intercept)  1.617679e-03 -5.284546e-05\ngdp         -5.284546e-05  2.281099e-06\n\n\nThe main diagonal elements of the variance-covariance matrix are the variances of the coefficients. The square roots of these are the standard errors of the coefficients. Let’s compute the standard errors and compare to the lm() output:\n\n# Calculate standard errors\nse_b &lt;- sqrt(diag(vcb))\nprint(se_b)\n\n       cons         gdp \n0.040220385 0.001510331 \n\n# Compare with lm() standard errors\nsummary(ols1)\n\n\nCall:\nlm(formula = socins ~ gdp, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n 0.03438 -0.03481  0.02357 -0.04701 -0.03278  0.05665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.045465   0.040220   1.130    0.322  \ngdp         0.003577   0.001510   2.368    0.077 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04859 on 4 degrees of freedom\nMultiple R-squared:  0.5837,    Adjusted R-squared:  0.4797 \nF-statistic: 5.609 on 1 and 4 DF,  p-value: 0.07696\n\n\nSo we’ve just replicated the lm() output using matrix algebra. We’ve seen how the estimates are generated and how the standard errors are computed."
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#predictions-with-confidence-intervals",
    "href": "slides/linearmodelfirstlook.html#predictions-with-confidence-intervals",
    "title": "Linear Regression in OLS",
    "section": "Predictions with Confidence Intervals",
    "text": "Predictions with Confidence Intervals\nLet’s return to the predictions from the bivariate model. In the Section 3.2 above, we plotted \\(\\widehat{y}\\) against \\(x\\) and the actual values of \\(y\\) but we didn’t express any uncertainty about those predictions - we did not compute standard error of the \\(\\widehat{y}\\) values, or compute confidence intervals. Let’s do that now.\nWe can compute the confidence intervals for the predictions using the formula:\n\\[\\widehat{y} \\pm t_{\\alpha/2, n-2} \\times \\text{SE}(\\widehat{y})\\]\nWhere do we get the standard errors of the predictions? We can compute these using the formula:\n\\[\\text{SE}(\\widehat{y}) = \\sqrt{diag(\\text{X}V\\text{X}')}\\].\nThis is the square root of the diagonal elements of the matrix product of the \\(X\\) matrix, the variance-covariance matrix of the coefficients, and the transpose of the \\(X\\) matrix.\n\n\ncode\npredictions &lt;- predictions %&gt;% mutate(ub = fit.fit + 1.96 * se.fit, lb = fit.fit - 1.96 * se.fit)\n#order predictions by gdp\npredictions &lt;- predictions[order(predictions$gdp),]\nhighchart() %&gt;%\n  hc_add_series(predictions, type = \"arearange\", hcaes(x = gdp, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.5) %&gt;%\n  hc_add_series(predictions, type = \"line\", hcaes(x = gdp, y = fit.fit), name = \"Predicted\", color=\"black\") %&gt;%\n  #hc_add_series(predictions, type = \"scatter\", hcaes(x = gdp, y = socins), name = \"Actual\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Social Spending\"))\n\n\n\n\n\n\nThe confidence intervals are wider where the data are more dispersed, and narrower where the data are more tightly clustered. The confidence intervals are a measure of the uncertainty in our predictions. One thing to notice is that, scanning from left-to-right, we could draw a horizontal line from the left-most upper bound to the right-most lower bound - which is to say we can’t say the change in social insurance spending is statistically significant across the range of GDP because the interval contains a line with slope=0."
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#multivariate-model-and-quantities",
    "href": "slides/linearmodelfirstlook.html#multivariate-model-and-quantities",
    "title": "Linear Regression in OLS",
    "section": "Multivariate Model and Quantities",
    "text": "Multivariate Model and Quantities\nLet’s now estimate a multivariate model. We’ll include both “democracy” and “gdp” as predictors of social insurance spending. The model is:\n\n\ncode\n#refresh data\ndf &lt;- as.data.frame(dm)\n\nols3 &lt;- lm(socins ~ dem + gdp, data = df)\n#summary(ols2)\n\nmodelsummary(\n  models = ols3,\n  stars = TRUE,\n  format=\"pipe\",\n#  output = \"html\",\n  title = \"OLS Estimates\",\n  gof_map = c(\"n\", \"r.squared\", \"adj.r.squared\", \"fstatistic\", \"p.value\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.022  \n                \n                \n                             \n                  (0.030)\n                \n                \n                  dem        \n                  0.074  \n                \n                \n                             \n                  (0.032)\n                \n                \n                  gdp        \n                  0.002  \n                \n                \n                             \n                  (0.001)\n                \n                \n                  R2         \n                  0.851  \n                \n                \n                  R2 Adj.    \n                  0.751  \n                \n        \n      \n    \n\n\n\nTo generate predictions in the multivariate model, we need to hold variables other than our variable of interest constant at some meaningful value (often, central tendency). In this case, I’m going to vary GDP (my x-axis representing my variable of interest), and compute predictions at both values of “democracy” (0 and 1).\n\n\ncode\n#compute average predictions for dem=0 and dem=1\n\ndfpreds &lt;- df\ndfpreds$dem &lt;- 0\nfit &lt;- data.frame(predict(ols3, newdata = dfpreds, interval = \"confidence\", se.fit = TRUE))\ndf$nondemxb &lt;- fit$fit.fit\ndfpreds$dem &lt;- 1\nfit &lt;- data.frame(predict(ols3, newdata = dfpreds, interval = \"confidence\", se.fit = TRUE))\ndf$demxb &lt;- fit$fit.fit\n\n\n#plot predictions\nhighchart() %&gt;%\n  hc_add_series(df, type = \"line\", hcaes(x = gdp, y = demxb), name = \"Dem = 1\", color=\"#005A43\") %&gt;%\n  hc_add_series(df, type = \"line\", hcaes(x = gdp, y = nondemxb), name = \"Dem = 0\", color=\"#6CC24A\") %&gt;%\n  #hc_add_series(df, type = \"scatter\", hcaes(x = gdp, y = socins), name = \"Actual\") %&gt;%\n  hc_xAxis(title = list(text = \"GDP\")) %&gt;%\n  hc_yAxis(title = list(text = \"Social Spending\"))\n\n\n\n\n\n\nThe upper line reports the predicted social insurance spending for democracies over GDP; the lower line reports the predicted social insurance spending for non-democracies over GDP. The distance between the two lines is the difference in the mean of social insurance spending between democracies and non-democracies; it’s the value of the “democracy” coefficient, 0.074. Also, note that the lines are parellel - this is by construction since we’ve only estimated one slope (GDP) despite having two regime groups. This is the structural stability assumption, something we’ll explore in the coming weeks."
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#real-data-boston-housing-values",
    "href": "slides/linearmodelfirstlook.html#real-data-boston-housing-values",
    "title": "Linear Regression in OLS",
    "section": "Real Data: Boston Housing Values",
    "text": "Real Data: Boston Housing Values\nLet’s now turn to a real data set - the Boston housing data set. We’ll estimate a model of median home value as a function of crime rate, age of the house, tax rate, black population, distance to employment centers, and pupil-teacher ratio. In the end, I want to plot predictions and confidence intervals over the range of pupil-teacher ratios.\nLet’s present the results two different ways: in a table and in a coefficient plot.\n\n\ncode\nlibrary(MASS)\nboston &lt;- data.frame(Boston)\n\ninmodel &lt;- subset(boston, select = c(medv, crim, age, tax, black, dis, ptratio))\n\nhvmodel1 &lt;- lm(medv ~ crim + age + tax + black + dis + ptratio, data = boston)\n\nstargazer(hvmodel1, type = \"html\", title = \"Boston Housing Values, OLS estimates\")\n\n\n\nBoston Housing Values, OLS estimates\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nmedv\n\n\n\n\n\n\n\n\ncrim\n\n\n-0.141***\n\n\n\n\n\n\n(0.047)\n\n\n\n\n\n\n\n\n\n\nage\n\n\n-0.095***\n\n\n\n\n\n\n(0.017)\n\n\n\n\n\n\n\n\n\n\ntax\n\n\n-0.007**\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\nblack\n\n\n0.015***\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\ndis\n\n\n-0.923***\n\n\n\n\n\n\n(0.238)\n\n\n\n\n\n\n\n\n\n\nptratio\n\n\n-1.520***\n\n\n\n\n\n\n(0.167)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n58.774***\n\n\n\n\n\n\n(3.722)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n506\n\n\n\n\nR2\n\n\n0.398\n\n\n\n\nAdjusted R2\n\n\n0.391\n\n\n\n\nResidual Std. Error\n\n\n7.179 (df = 499)\n\n\n\n\nF Statistic\n\n\n54.981*** (df = 6; 499)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\ncode\n# sse &lt;- sum(hvmodel1$residuals^2)\n# rse &lt;- sqrt(sse / (nrow(boston) - 7))\n# rmse &lt;- mean(hvmodel1$residuals^2)\n\n\n\n\ncode\nmodelplot(hvmodel1, stars = TRUE, title = \"Boston Housing Values, OLS estimates\", coef_omit = c(\"Intercept\"))\n\n\n\n\n\n\n\n\n\nLet’s generate predicted housing values across the values of pupil-teacher ratio. We’ll hold the other variables at means, medians, or modes, and compute the predictions and confidence intervals.\n\n\ncode\n# make some data for at-means predictions \n#summary(boston$ptratio)\n\noos1 &lt;- data.frame(Intercept=1, crim=.25, age=77.5, tax=330,black=391.44, dis=3.2, ptratio= c(seq(12,22,1)))\n\nboston.predict &lt;- data.frame(oos1, predict(hvmodel1, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=oos1))\n\n# confidence bounds by end point transformation\nboston.predict &lt;- boston.predict %&gt;% mutate(ub=fit.fit+1.96*se.fit) %&gt;% mutate(lb=fit.fit-1.96*se.fit)\n\n\nhighchart() %&gt;%\n  hc_add_series(boston.predict, type = \"arearange\", hcaes(x = ptratio, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.2) %&gt;%\n  hc_add_series(boston.predict, type = \"line\", hcaes(x = ptratio, y = fit.fit), name = \"Predicted\", color=\"#6CC24A\") %&gt;%\n  hc_xAxis(title = list(text = \"Pupil-teacher ratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Predicted Housing Value\"))"
  },
  {
    "objectID": "slides/linearmodelfirstlook.html#predictions-using-matrix-algebra",
    "href": "slides/linearmodelfirstlook.html#predictions-using-matrix-algebra",
    "title": "Linear Regression in OLS",
    "section": "Predictions using matrix algebra",
    "text": "Predictions using matrix algebra\nLet’s do this one last time computing the predictions and confidence intervals for the Boston housing data set using matrix algebra.\n\n\ncode\n# generate xb and st. errs of predictions by matrix: \n\n# get b vector\nb &lt;- coef(hvmodel1)\n\n# set up out of sample data, making sure vars in same order as in model\noos1 &lt;- data.frame(Intercept=1, crim=.25, age=77.5, tax=330,black=391.44, dis=3.2, ptratio= c(seq(12,22,1)))\nX &lt;- as.matrix(oos1)\n\nxb &lt;- X%*%b\n\n# now, get the var-cov matrix of b : var(p) = XVX'\nV &lt;- vcov(hvmodel1)\nvcp = X%*%V%*%t(X)\nvarp&lt;- diag(vcp, names = TRUE)\n\n# compute upper and lower bounds by end point transformation\noos1 &lt;-data.frame(oos1, sep= sqrt(varp), xb, ub=xb+1.96*sqrt(varp), lb=xb-1.96*sqrt(varp))\n\n# plot\nhighchart() %&gt;%\n  hc_add_series(oos1, type = \"arearange\", hcaes(x = ptratio, low = lb, high = ub), name = \"Confidence Interval\", color = \"#005A43\", fillOpacity = 0.2) %&gt;%\n  hc_add_series(oos1, type = \"line\", hcaes(x = ptratio, y = xb), name = \"Predicted\", color=\"#6CC24A\") %&gt;%\n  hc_xAxis(title = list(text = \"Pupil/Teacher Ratio\")) %&gt;%\n  hc_yAxis(title = list(text = \"Predicted Home Value\"))"
  },
  {
    "objectID": "syllabus25.html#artificial-intelligence",
    "href": "syllabus25.html#artificial-intelligence",
    "title": "Syllabus",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\nAny and all use of AI in work for this course must be documented and cited. Documentation minimally includes reporting prompts used to generate text, code, data, etc. Citation includes a reference to the AI tool used, model, date, etc.\nI encourage you to use AI to troubleshoot; I discourage your using it otherwise for coding at this point in your graduate careers for two reasons. First, the more you can code on your own, the better you’ll be able to use AI - so you have to learn to code first. Coding implies both the technical, syntactical aspects of writing in a language, and the logical, structural aspects of writing a program. Second, the more you can code independently, the better you’ll be able to connect your data and data structures to statistical models - your intuitions will be deeper this way.\nIt will be tempting to use AI to finish assignments and to get things “right.” For this semester, getting them is much more important than getting them right. I’d rather you turn in a wrong answer you’ve worked through than a right answer you’ve not."
  },
  {
    "objectID": "slides/olsmatrix25.html",
    "href": "slides/olsmatrix25.html",
    "title": "Deriving the OLS Estimator",
    "section": "",
    "text": "Start with: \\[y = X\\beta + \\epsilon\\]\nMinimize sum of squared errors: \\[\\min_{\\beta} \\epsilon'\\epsilon = \\min_{\\beta} (y - X\\beta)'(y - X\\beta)\\]\nExpand: \\[(y - X\\beta)'(y - X\\beta) = y'y - y'X\\beta - \\beta'X'y + \\beta'X'X\\beta\\]\nSimplify using symmetry (\\(y'X\\beta = \\beta'X'y\\) as they’re scalars): \\[= y'y - 2\\beta'X'y + \\beta'X'X\\beta\\]\nTake derivative with respect to \\(\\beta\\) and set to zero: \\[\\frac{\\partial}{\\partial\\beta}(y'y - 2\\beta'X'y + \\beta'X'X\\beta) = 0\\] \\[-2X'y + 2X'X\\beta = 0\\]\nSolve for \\(\\beta\\): \\[X'X\\beta = X'y\\] \\[\\hat{\\beta} = (X'X)^{-1}X'y\\]\n\n\n\nStart with \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) and substitute \\(y = X\\beta + \\epsilon\\):\n\\[\\hat{\\beta} = (X'X)^{-1}X'(X\\beta + \\epsilon)\\] \\[= \\beta + (X'X)^{-1}X'\\epsilon\\]\nTherefore: \\[\\hat{\\beta} - \\beta = (X'X)^{-1}X'\\epsilon\\]\nThe variance-covariance matrix is: \\[Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)']\\] \\[= E[(X'X)^{-1}X'\\epsilon\\epsilon'X(X'X)^{-1}]\\]\nUnder homoskedasticity (\\(E[\\epsilon\\epsilon'] = \\sigma^2I\\)): \\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\]"
  },
  {
    "objectID": "slides/olsmatrix25.html#deriving-hatbeta",
    "href": "slides/olsmatrix25.html#deriving-hatbeta",
    "title": "Deriving the OLS Estimator",
    "section": "",
    "text": "Start with: \\[y = X\\beta + \\epsilon\\]\nMinimize sum of squared errors: \\[\\min_{\\beta} \\epsilon'\\epsilon = \\min_{\\beta} (y - X\\beta)'(y - X\\beta)\\]\nExpand: \\[(y - X\\beta)'(y - X\\beta) = y'y - y'X\\beta - \\beta'X'y + \\beta'X'X\\beta\\]\nSimplify using symmetry (\\(y'X\\beta = \\beta'X'y\\) as they’re scalars): \\[= y'y - 2\\beta'X'y + \\beta'X'X\\beta\\]\nTake derivative with respect to \\(\\beta\\) and set to zero: \\[\\frac{\\partial}{\\partial\\beta}(y'y - 2\\beta'X'y + \\beta'X'X\\beta) = 0\\] \\[-2X'y + 2X'X\\beta = 0\\]\nSolve for \\(\\beta\\): \\[X'X\\beta = X'y\\] \\[\\hat{\\beta} = (X'X)^{-1}X'y\\]"
  },
  {
    "objectID": "slides/olsmatrix25.html#variance-covariance-matrix",
    "href": "slides/olsmatrix25.html#variance-covariance-matrix",
    "title": "Deriving the OLS Estimator",
    "section": "",
    "text": "Start with \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) and substitute \\(y = X\\beta + \\epsilon\\):\n\\[\\hat{\\beta} = (X'X)^{-1}X'(X\\beta + \\epsilon)\\] \\[= \\beta + (X'X)^{-1}X'\\epsilon\\]\nTherefore: \\[\\hat{\\beta} - \\beta = (X'X)^{-1}X'\\epsilon\\]\nThe variance-covariance matrix is: \\[Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)']\\] \\[= E[(X'X)^{-1}X'\\epsilon\\epsilon'X(X'X)^{-1}]\\]\nUnder homoskedasticity (\\(E[\\epsilon\\epsilon'] = \\sigma^2I\\)): \\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\]"
  },
  {
    "objectID": "slides/bivariateexample25.html",
    "href": "slides/bivariateexample25.html",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "",
    "text": "These slides introduce the basics of bivariate OLS regression using cross-sectional data. We’ll analyze how infant mortality rates (IMR) vary across countries based on their political systems and economic development. Until the end, we’ll focus on bivariate models. The goal is just to think through some of the elements of the model and questions we face when modeling - you’ll benefit from having reviewed the matrix derivation slides, and the thinking about data slides."
  },
  {
    "objectID": "slides/bivariateexample25.html#setup-and-data-loading",
    "href": "slides/bivariateexample25.html#setup-and-data-loading",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Setup and Data Loading",
    "text": "Setup and Data Loading\nFirst, we’ll load required packages and our dataset.\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(dplyr)\nlibrary(modelsummary)\n\ncensor &lt;- read.csv(\"/Users/dave/Documents/teaching/501/2023/data/censorcy.csv\", header=TRUE)"
  },
  {
    "objectID": "slides/bivariateexample25.html#data-exploration",
    "href": "slides/bivariateexample25.html#data-exploration",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nLet’s examine the dataset’s structure and summary statistics. The data include one observation for each country, and measures of infant mortality, GDP per capit and its natural log, deaths from natural disasters, whether there’s systematic censorship, and the polity scale. Here are a few ways to look at the data.\n\ntibble(censor)\n\n# A tibble: 155 × 8\n   ctry        ccode    IMR  gdppc deaths censorship polity lngdp\n   &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 Afghanistan   700 177.    1272.   7308          1     -7  7.15\n 2 Albania       339  45.7   2833.    125          1      7  7.95\n 3 Algeria       615  56.5   4538.    921          1     -2  8.42\n 4 Angola        540 192.    1164.    766          1     -1  7.06\n 5 Argentina     160  24.2   8616.    112          1      8  9.06\n 6 Armenia       371  26.7   3231.      4          1      5  8.08\n 7 Australia     900   7.18 18623.     75          1     10  9.83\n 8 Austria       305   7.84 19533.     53          1     10  9.88\n 9 Azerbaijan    373  81.5   2955.     42          1     -6  7.99\n10 Bahrain       692  31.8  11702.      0          1     -9  9.37\n# ℹ 145 more rows\n\ndatasummary_skim(censor)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ccode\n                  154\n                  0\n                  461.6\n                  234.9\n                  2.0\n                  452.0\n                  950.0\n                  \n                \n                \n                  IMR\n                  154\n                  0\n                  55.9\n                  44.4\n                  3.4\n                  45.9\n                  191.8\n                  \n                \n                \n                  gdppc\n                  154\n                  0\n                  6050.3\n                  6092.3\n                  371.1\n                  3667.9\n                  26653.5\n                  \n                \n                \n                  deaths\n                  130\n                  0\n                  6499.1\n                  30349.9\n                  0.0\n                  180.0\n                  300000.0\n                  \n                \n                \n                  censorship\n                  2\n                  0\n                  0.9\n                  0.2\n                  0.0\n                  1.0\n                  1.0\n                  \n                \n                \n                  polity\n                  21\n                  1\n                  4.3\n                  6.2\n                  -10.0\n                  7.0\n                  10.0\n                  \n                \n                \n                  lngdp\n                  154\n                  0\n                  8.2\n                  1.1\n                  5.9\n                  8.2\n                  10.2\n                  \n                \n        \n      \n    \n\n\ncensor %&gt;%\n   summarise_if(is.numeric, median, na.rm=TRUE)\n\n  ccode    IMR    gdppc deaths censorship polity    lngdp\n1   452 45.865 3667.933    180          1      7 8.207383\n\nggpairs(censor, columns = 3:7)"
  },
  {
    "objectID": "slides/bivariateexample25.html#constant-only-model",
    "href": "slides/bivariateexample25.html#constant-only-model",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Constant-Only Model",
    "text": "Constant-Only Model\nLet’s start with the constant-only, or null model:\n\n\ncode\nm0 &lt;- lm(IMR ~ 1, data = censor)\n\nmodelsummary(\n  models = m0,\n  stars = TRUE,\n  title = \"Null Model\",\n  gof_omit = 'AIC|Log.Lik|R2|BIC|RMSE',\n  #gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"f.statistic\", \"p.value\", \"sigma\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Null Model\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  55.946***\n                \n                \n                             \n                  (3.566)  \n                \n                \n                  Num.Obs.   \n                  155      \n                \n        \n      \n    \n\n\n\nThe estimate of \\(\\beta_0\\) is the mean of the dependent variable, IMR. The constant-only model is a simple way to understand the data’s central tendency.\n\nsummary(censor$IMR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.388  14.934  45.865  55.946  90.438 191.787"
  },
  {
    "objectID": "slides/bivariateexample25.html#political-system-and-infant-mortality",
    "href": "slides/bivariateexample25.html#political-system-and-infant-mortality",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Political System and Infant Mortality",
    "text": "Political System and Infant Mortality\nNow we’ll examine how a country’s political system (measured by the Polity score) affects infant mortality. The Polity score ranges from -10 (most autocratic) to +10 (most democratic). Let’s look at the Polity variable:\n\n\ncode\n# Calculate counts by polity score\npolity_counts &lt;- table(censor$polity)\npolity_df &lt;- data.frame(\n  polity = as.numeric(names(polity_counts)),\n  count = as.numeric(polity_counts)\n)\n\n# Create highchart\nhighchart() %&gt;%\n  hc_chart(type = \"column\") %&gt;%\n  hc_xAxis(categories = polity_df$polity, title = list(text = \"Polity Score\")) %&gt;%\n  hc_yAxis(title = list(text = \"Count\")) %&gt;%\n  hc_add_series(\n    data = polity_df$count,\n    name = \"Countries\",\n    color = \"#005A43\",\n    dataLabels = list(\n      enabled = TRUE,\n      format = \"{y}\"\n    )\n  ) %&gt;%\n  hc_title(text = \"Distribution of Polity Scores\") %&gt;%\n  hc_plotOptions(column = list(\n    borderRadius = 3,\n    borderWidth = 0\n  ))\n\n\n\n\n\n\ncode\n#     \n#     \n\n\nPolity is categorical and ordered. It’s commonly (mis)treated as a continuous variable in the literature - we’ll start there:\n\n\ncode\nm1 &lt;- lm(IMR ~ polity, data=censor)\n\nsummary(m1)\n\n\n\nCall:\nlm(formula = IMR ~ polity, data = censor)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-76.97 -31.95 -11.67  35.30 120.50 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  68.4898     4.0820  16.778  &lt; 2e-16 ***\npolity       -2.7999     0.5431  -5.155 7.84e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.22 on 151 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.1496,    Adjusted R-squared:  0.144 \nF-statistic: 26.57 on 1 and 151 DF,  p-value: 7.836e-07\n\n\nThe estimated coefficient of -2.7999 suggests that, on average, each unit increase in the Polity score is associated with a 2.7999 decrease in the IMR; the estimate is statistically different from zero.\nLet’s take a brief detour to think about the errors in our model. First, let’s compute the sum of squared residuals, \\(\\sigma^2\\), and the residual standard error. Remember the residuals are given by \\(y - \\hat{y}\\); \\(\\sigma^2\\) is the sum of squared residuals divided by \\(n-k-1\\); and the residual standard error is the square root of \\(\\sigma^2\\).\n\nsse &lt;- sum(residuals(m1)^2)\nsigma2 &lt;- sse / m1$df.residual\nrse &lt;- sqrt(sigma2)\n\ncat(\"Sum of Squared Residuals:\", sse, \"\\n\")\n\nSum of Squared Residuals: 256597.8 \n\ncat(\"Sigma^2:\", sigma2, \"\\n\")\n\nSigma^2: 1699.323 \n\ncat(\"Residual Standard Error:\", rse, \"\\n\")\n\nResidual Standard Error: 41.22285 \n\n\nComparing this to the estimates above, you’ll see our estimate of the RSE is the same as R’s estimate. This is the average residual in the model in terms of the \\(y\\) variable, IMR - so on average, we’re off by about 41 infant deaths per 1000 live births."
  },
  {
    "objectID": "slides/bivariateexample25.html#economic-development-and-infant-mortality",
    "href": "slides/bivariateexample25.html#economic-development-and-infant-mortality",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Economic Development and Infant Mortality",
    "text": "Economic Development and Infant Mortality\nLet’s examine how GDP per capita affects infant mortality rates, exploring different functional forms.\n\nLinear Relationship\n\n\ncode\nm2 &lt;- lm(IMR ~ gdppc, data=censor)\n\nmodelsummary(\n  models = m2,\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_omit = 'AIC|Log.Lik|R2|BIC|RMSE',\n  #gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"f.statistic\", \"p.value\", \"sigma\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  86.710***\n                \n                \n                             \n                  (3.618)  \n                \n                \n                  gdppc      \n                  -0.005***\n                \n                \n                             \n                  (0.000)  \n                \n                \n                  Num.Obs.   \n                  155      \n                \n                \n                  F          \n                  145.103  \n                \n        \n      \n    \n\n\n\nLooking at the predictions, it’s pretty clear the scatterplot of IMR and GDP per capita doesn’t suggest a linear relationship.\n\n\ncode\n# Generate predictions\nanalysisdata &lt;- censor %&gt;%\n  mutate(xb = coef(m2)[1] + coef(m2)[2]*gdppc)\n\npred &lt;- predict(m2, interval=\"confidence\", se.fit=TRUE)\nanalysisdata &lt;- cbind(analysisdata, pred)\n# \n# # Plot with predictions\n# ggplot(analysisdata, aes(x=gdppc, y=IMR)) +\n#   geom_point(color=\"green\") + \n#   geom_text(label=censor$ctry, size=2) +\n#   geom_line(aes(x=gdppc, y=xb)) +\n#   labs(x=\"GDP per capita\", y=\"Infant Mortality Rate\")\n\n# same as above using highcharter \n\nhighchart() %&gt;%\n  hc_add_series(\n    data = censor,\n    name = \"Observed IMR\",\n    type = \"scatter\",\n    hcaes(x = gdppc, y = IMR),\n    color = \"#005A43\",\n    dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n  ) %&gt;%\n  hc_add_series(\n    data = analysisdata,\n    name = \"Predicted IMR\",\n    type = \"line\", \n    color=\"#6CC24A\",\n    hcaes(x = gdppc, y = xb)\n  ) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Infant Mortality Rate\"))\n\n\n\n\n\n\n\n\nResiduals\nLet’s examine the residuals from this model - recall the residuals are the differences between the observed and predicted values of \\(y\\).\n\n\ncode\n# Calculate residuals\nanalysisdata &lt;- analysisdata %&gt;%\n  mutate(res = IMR - xb)\n# \n# ggplot(analysisdata, aes(x=gdppc, y=res)) +\n#   geom_point(color=\"green\") + \n#   geom_text(label=censor$ctry, size=3) +\n#   geom_abline(slope=0, intercept=0) +\n#   labs(x=\"GDP per capita\", y=\"Residuals\")\n\n# same plot in highcharter\n\nhighchart() %&gt;%\n hc_add_series(\n   data = analysisdata,\n   name = \"Residuals\",\n   type = \"scatter\",\n   hcaes(x = gdppc, y = res),\n   color = \"#005A43\",\n   dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n ) %&gt;%\n hc_add_series(\n   data = list(list(x = min(analysisdata$gdppc), y = 0), \n               list(x = max(analysisdata$gdppc), y = 0)),\n   type = \"line\",\n   color = \"red\",\n   enableMouseTracking = FALSE,\n   showInLegend = FALSE\n ) %&gt;%\n hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n hc_yAxis(title = list(text = \"Residuals\"))\n\n\n\n\n\n\nBoth the predictions and the residuals suggest the relationship between GDP and IMR is not linear. Let’s consider some alternatives that might fit the data better.\n\n\nLog-transformed GDP\nLet’s try a log transformation of GDP per capita \\(ln(GDP_{pc})\\) and see if that improves the model.\n\n\ncode\nanalysisdata2 &lt;- censor %&gt;%\n  mutate(lngdp = log(gdppc))\n\nm3 &lt;- lm(IMR ~ lngdp, data=analysisdata2)\n\nmodelsummary(\n  models = m3,\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_omit = 'AIC|Log.Lik|R2|BIC|RMSE',\n  #gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"f.statistic\", \"p.value\", \"sigma\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  332.552***\n                \n                \n                             \n                  (14.657)  \n                \n                \n                  lngdp      \n                  -33.846***\n                \n                \n                             \n                  (1.777)   \n                \n                \n                  Num.Obs.   \n                  155       \n                \n                \n                  F          \n                  362.586   \n                \n        \n      \n    \n\n\n\nand take a look at the predictions from this model:\n\n\ncode\n# Generate predictions\nanalysisdata2 &lt;- analysisdata2 %&gt;%\n  mutate(xb = coef(m3)[1] + coef(m3)[2]*lngdp)\n#sort by lngdp\nanalysisdata2 &lt;- analysisdata2[order(analysisdata2$lngdp),]\n\n\n# plot using highcharter\n\nhighchart() %&gt;%\n  hc_add_series(\n    data = analysisdata2,\n    name = \"Observed IMR\",\n    type = \"scatter\",\n    hcaes(x = gdppc, y = IMR),\n    color = \"#005A43\",\n    dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n  ) %&gt;%\n  hc_add_series(\n    data = analysisdata2,\n    name = \"Predicted IMR\",\n    type = \"line\", \n    color=\"#6CC24A\",\n    hcaes(x = gdppc, y = xb)\n  ) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Infant Mortality Rate\"))\n\n\n\n\n\n\nThe log-transformed model seems to fit the data better than the linear model. The residuals are also more evenly distributed around zero, though still appear correlated with GDP per capita.\n\n\ncode\n# Calculate residuals\nanalysisdata2 &lt;- analysisdata2 %&gt;%\n  mutate(res = IMR - xb)\n\n# Plot using highcharter\n\nhighchart() %&gt;%\n  hc_add_series(\n    data = analysisdata2,\n    name = \"Residuals\",\n    type = \"scatter\",\n    hcaes(x = gdppc, y = res),\n    color = \"#005A43\",\n    dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n  ) %&gt;%\n  hc_add_series(\n    data = list(list(x = min(analysisdata2$gdppc), y = 0), \n                list(x = max(analysisdata2$gdppc), y = 0)),\n    type = \"line\",\n    color = \"red\",\n    enableMouseTracking = FALSE,\n    showInLegend = FALSE\n  ) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\"))\n\n\n\n\n\n\n\n\nPolynomials\nFinally, let’s try a polynomial model to see if we can improve the fit further. We’ll try a quadratic model, \\(IMR = \\beta_0 + \\beta_1 \\times GDP_{pc} + \\beta_2 \\times GDP_{pc}^2\\).\n\n\ncode\ncensor$gdp2 = censor$gdppc^2\n\nm4 &lt;- lm(IMR ~ gdppc + gdp2, data=censor, na.action = na.omit)\n\nmodelsummary(\n  models = m4,\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_omit = 'AIC|Log.Lik|R2|BIC|RMSE',\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  110.123***\n                \n                \n                             \n                  (4.016)   \n                \n                \n                  gdppc      \n                  -0.015*** \n                \n                \n                             \n                  (0.001)   \n                \n                \n                  gdp2       \n                  0.000***  \n                \n                \n                             \n                  (0.000)   \n                \n                \n                  Num.Obs.   \n                  155       \n                \n                \n                  F          \n                  145.206   \n                \n        \n      \n    \n\n\n\nGDP per capita is negatively related to IMR, then positively related at higher values. Here are predictions from the polynomial model (including a 95% confidence interval) and the actual data points:\n\n\ncode\nfit2 &lt;- predict(m4, interval=\"confidence\", se.fit=TRUE)\n\npredictions2 &lt;- cbind(censor, fit2)\n\n# sort predictions2 by gdppc\npredictions2 &lt;- predictions2[order(predictions2$gdppc),]\n\n# ggplot(data=predictions2, aes(x=gdppc, y=IMR))+\n#   geom_point(color=\"green\") +   geom_text(label=predictions2$ctry, size=2) +\n#   geom_line(aes(x=gdppc, y=fit.fit)) +\n#   geom_ribbon(aes(ymin = fit.lwr, ymax = fit.upr), alpha = 0.2) \n\n#plot using highcharter\n\nhighchart() %&gt;%\n  hc_add_series(\n    data = predictions2,\n    name = \"Observed IMR\",\n    type = \"scatter\",\n    hcaes(x = gdppc, y = IMR),\n    color = \"#005A43\",\n    dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n  ) %&gt;%\n  hc_add_series(\n    data = predictions2,\n    name = \"Predicted IMR\",\n    type = \"line\", \n    color=\"#6CC24A\",\n    hcaes(x = gdppc, y = fit.fit)\n  ) %&gt;%\n  hc_add_series(\n    data = predictions2,\n    type = \"arearange\",\n    name = \"95% CI\",\n    hcaes(x = gdppc, low = fit.lwr, high = fit.upr),\n    color = \"#005A43\",\n    fillOpacity = 0.2\n  ) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Infant Mortality Rate\"))\n\n\n\n\n\n\nAnd let’s look at the residuals from the polynomial model:\n\n\ncode\n# Calculate residuals\n\npredictions2 &lt;- predictions2 %&gt;%\n  mutate(res = IMR - fit.fit)\n\n# Plot using highcharter\n\nhighchart() %&gt;%\n  hc_add_series(\n    data = predictions2,\n    name = \"Residuals\",\n    type = \"scatter\",\n    hcaes(x = gdppc, y = res),\n    color = \"#005A43\",\n    dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n  ) %&gt;%\n  hc_add_series(\n    data = list(list(x = min(predictions2$gdppc), y = 0), \n                list(x = max(predictions2$gdppc), y = 0)),\n    type = \"line\",\n    color = \"red\",\n    enableMouseTracking = FALSE,\n    showInLegend = FALSE\n  ) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Residuals\"))\n\n\n\n\n\n\nThe residuals exhibit less pattern than the linear model, but seem to overfit some of the nonlinearity in the sense that the residuals appear nonlinear and perhaps in the opposite direction of the observed data."
  },
  {
    "objectID": "slides/bivariateexample25.html#multivariate-model-combined-effects-of-economics-and-politics",
    "href": "slides/bivariateexample25.html#multivariate-model-combined-effects-of-economics-and-politics",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Multivariate Model: Combined Effects of Economics and Politics",
    "text": "Multivariate Model: Combined Effects of Economics and Politics\nFinally, let’s examine how both economic development and political system jointly affect infant mortality rates. This multivariate model is somewhat more realistic insofar as we don’t believe either of these variables acts alone in shaping infant mortality rates. Here, we’ll use the logged GDP per capita measure as that functional form seemed to fit the data best (informally).\n\n\ncode\n# Fit multivariate model\nm5 &lt;- lm(IMR ~ lngdp + polity, data=censor)\nmodelsummary(m4)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  110.123 \n                \n                \n                             \n                  (4.016) \n                \n                \n                  gdppc      \n                  -0.015  \n                \n                \n                             \n                  (0.001) \n                \n                \n                  gdp2       \n                  0.000   \n                \n                \n                             \n                  (0.000) \n                \n                \n                  Num.Obs.   \n                  155     \n                \n                \n                  R2         \n                  0.656   \n                \n                \n                  R2 Adj.    \n                  0.652   \n                \n                \n                  AIC        \n                  1457.2  \n                \n                \n                  BIC        \n                  1469.3  \n                \n                \n                  Log.Lik.   \n                  -724.588\n                \n                \n                  F          \n                  145.206 \n                \n                \n                  RMSE       \n                  25.94   \n                \n        \n      \n    \n\n\n\n\n\ncode\n# Generate predictions for different political systems\ngdp_range &lt;- seq(min(censor$gdppc), max(censor$gdppc), length.out=100)\npredictions_df &lt;- data.frame(\n  gdppc = rep(gdp_range, 2),\n  lngdp = log(rep(gdp_range, 2)),\n  polity = c(rep(-10, 100), rep(10, 100))\n)\n\n\n# Calculate predicted values\npredictions_df$predicted &lt;- predict(m5, newdata=predictions_df)\n\n\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Generate predictions for different political systems\ngdp_range &lt;- seq(min(censor$gdppc), max(censor$gdppc), length.out=100)\npredictions_df &lt;- data.frame(\n  gdppc = rep(gdp_range, 2),\n  lngdp = log(rep(gdp_range, 2)),\n  polity = c(rep(-10, 100), rep(10, 100))\n)\n\n# Calculate predicted values\npredictions_df$predicted &lt;- predict(m5, newdata=predictions_df)\n\n# Create highchart\nhighchart() %&gt;%\n  hc_add_series(data = predictions_df[predictions_df$polity == -10,],\n                hcaes(x = gdppc, y = predicted),\n                type = \"line\",\n                color = \"#005A43\",\n                name = \"Polity = -10\") %&gt;%\n  hc_add_series(data = predictions_df[predictions_df$polity == 10,],\n                hcaes(x = gdppc, y = predicted),\n                type = \"line\",\n                color = \"black\",\n                name = \"Polity = 10\") %&gt;%\n  hc_add_series(data = censor,\n                hcaes(x = gdppc, y = IMR),\n                type = \"scatter\",\n                name = \"Observed Values\",\n                marker = list(symbol = \"circle\"),\n                color = \"#6CC24A\",\n                dataLabels = list(enabled = TRUE, format = \"{point.ctry}\"))%&gt;%\n  # hc_add_series(data = censor,\n  #               hcaes(x = gdppc, y = IMR),\n  #               type = \"scatter\",\n  #               name = \"Country Labels\",\n  #               dataLabels = list(\n  #                 enabled = TRUE,\n  #                 format = \"{point.ctry}\",\n  #                 style = list(fontSize = \"8px\")\n  #               )) %&gt;%\n  hc_xAxis(title = list(text = \"GDP per capita\")) %&gt;%\n  hc_yAxis(title = list(text = \"Predicted Infant Mortality Rate\")) %&gt;%\n  hc_title(text = \"Predicted IMR by GDP and Political System\") %&gt;%\n  hc_legend(title = list(text = \"Polity Score\")) %&gt;%\n  hc_tooltip(shared = FALSE)\n\n\n\n\n\n\nThis final plot shows how infant mortality rates are predicted to vary with GDP per capita for both highly autocratic (Polity = -10) and highly democratic (Polity = +10) countries. The actual observed values are shown as green points with country labels."
  },
  {
    "objectID": "slides/bivariateexample25.html#rethinking-polity",
    "href": "slides/bivariateexample25.html#rethinking-polity",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Rethinking Polity",
    "text": "Rethinking Polity\nSince Polity is categorical, we don’t know the intervals between categories, so interpreting this coefficient (-2.79) as an effect of a 1 unit change on \\(y\\) really doesn’t make much sense. We are assuming the effect of a change in Polity is the same across all unit changes in the Polity scale - not only are we assuming the change is the same magnitude, but that it’s in the same direction - that the effect is linear.\nAnother way to treat Polity in this model is to include it as a factor variable. This allows us to estimate the effect of each level of Polity on IMR, relative to a reference level. In effect, we are including a dummy variable for all but one of the cetegories of Polity. Let’s look at such a model:\n\n\ncode\nm1f &lt;- lm(IMR ~ factor(polity), data=censor)\n\nmodelsummary(\n  models = m1f,\n  stars = TRUE,\n  title = \"OLS Estimates\",\n  gof_omit = 'AIC|Log.Lik|R2|BIC|RMSE',\n  #gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\", \"f.statistic\", \"p.value\", \"sigma\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        OLS Estimates\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)     \n                  44.411  \n                \n                \n                                  \n                  (34.102)\n                \n                \n                  factor(polity)-9\n                  13.518  \n                \n                \n                                  \n                  (37.356)\n                \n                \n                  factor(polity)-8\n                  35.063  \n                \n                \n                                  \n                  (39.377)\n                \n                \n                  factor(polity)-7\n                  21.136  \n                \n                \n                                  \n                  (36.834)\n                \n                \n                  factor(polity)-6\n                  33.683  \n                \n                \n                                  \n                  (36.834)\n                \n                \n                  factor(polity)-5\n                  17.644  \n                \n                \n                                  \n                  (41.766)\n                \n                \n                  factor(polity)-4\n                  49.030  \n                \n                \n                                  \n                  (39.377)\n                \n                \n                  factor(polity)-3\n                  1.577   \n                \n                \n                                  \n                  (39.377)\n                \n                \n                  factor(polity)-2\n                  14.589  \n                \n                \n                                  \n                  (37.356)\n                \n                \n                  factor(polity)-1\n                  93.026* \n                \n                \n                                  \n                  (39.377)\n                \n                \n                  factor(polity)0 \n                  79.959* \n                \n                \n                                  \n                  (36.834)\n                \n                \n                  factor(polity)2 \n                  64.582  \n                \n                \n                                  \n                  (39.377)\n                \n                \n                  factor(polity)3 \n                  41.135  \n                \n                \n                                  \n                  (41.766)\n                \n                \n                  factor(polity)4 \n                  25.645  \n                \n                \n                                  \n                  (38.127)\n                \n                \n                  factor(polity)5 \n                  49.484  \n                \n                \n                                  \n                  (36.834)\n                \n                \n                  factor(polity)6 \n                  35.730  \n                \n                \n                                  \n                  (35.494)\n                \n                \n                  factor(polity)7 \n                  7.281   \n                \n                \n                                  \n                  (35.389)\n                \n                \n                  factor(polity)8 \n                  8.292   \n                \n                \n                                  \n                  (34.987)\n                \n                \n                  factor(polity)9 \n                  -2.346  \n                \n                \n                                  \n                  (35.036)\n                \n                \n                  factor(polity)10\n                  -30.601 \n                \n                \n                                  \n                  (34.614)\n                \n                \n                  Num.Obs.        \n                  153     \n                \n                \n                  F               \n                  6.657   \n                \n        \n      \n    \n\n\n\nLet’s generate predictions from the factor model. Each bar represents the 95% confidence interval for the predicted IMR at each level of Polity; the dots are the predictions.\n\n\ncode\n# Predictions for factor model\nnew_data &lt;- data.frame(polity = factor(seq(-10, 10), \n                                     levels = sort(unique(censor$polity))))\npredictions &lt;- predict(m1f, newdata = new_data, interval = \"confidence\")\nresults &lt;- data.frame(\n  polity = as.numeric(as.character(new_data$polity)),\n  fit = predictions[,\"fit\"],\n  lwr = predictions[,\"lwr\"],\n  upr = predictions[,\"upr\"]\n)\n\nresults &lt;- results[!is.na(results$fit), ]\nresults &lt;- results[order(results$polity), ]\n\nhighchart() %&gt;%\n  hc_chart(type = \"errorbar\") %&gt;%\n  hc_xAxis(\n    categories = results$polity,\n    title = list(text = \"Polity Score\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Predicted IMR\")\n  ) %&gt;%\n  hc_add_series(\n    type = \"errorbar\",\n    data = list_parse(\n      data.frame(\n        low = results$lwr,\n        high = results$upr\n      )\n    ),\n    name = \"95% CI\",\n    color = \"#005A43\",\n    stemWidth = 3,\n    whiskerLength = 10\n  ) %&gt;%\n  hc_add_series(\n    type = \"scatter\",\n    data = results$fit,\n    name = \"Predicted IMR\",\n    color = \"#005A43\",\n    marker = list(\n      symbol = \"circle\",\n      radius = 6\n    ),\n    dataLabels = list(\n      enabled = TRUE,\n      format = \"{point.y:.1f}\",\n      verticalAlign = \"bottom\",\n      y = -10\n    )\n  ) %&gt;%\n  hc_title(text = \"Predicted Infant Mortality Rates by Polity Score\")\n\n\n\n\n\n\nA couple of things to note regarding these predictions. Because Polity is categorical, we cannot draw a continuous line across the categories. You’ll notice that it appears from the coefficients and from the predictions that the effect of Polity on IMR is not linear. The common practice of treating Polity as a continuous variable would be problematic in this model (and in most). Also, notice the ranges of the confidence intervals vary quite a lot. The variability within Polity category raises potentially interesting questions about development and regime."
  },
  {
    "objectID": "slides/bivariateexample25.html#generate-predictions",
    "href": "slides/bivariateexample25.html#generate-predictions",
    "title": "Introduction to Bivariate Linear Regression",
    "section": "Generate predictions",
    "text": "Generate predictions\nLet’s compute \\(\\widehat{y}\\) for each category of Polity and plot along with the actual observed value of IMR for each country.\n\n\ncode\npredictions &lt;- data.frame(polity=numeric(0), xb=numeric(0))\n\nfor (i in seq(1,21,1)){\n  xb = 68.4898 + -2.7999*(i-11)\n  predictions[i:i,] &lt;- data.frame(i-11, xb)\n}\n\n# # Visualize predictions with actual data\n# ggplot(data=censor, aes(x=polity, y=IMR)) +\n#   geom_point(color=\"green\") + \n#   geom_text(label=censor$ctry, size=3) +\n#   geom_line(data=predictions, aes(x=polity, y=xb)) +\n#   labs(x=\"Polity\", y=\"Infant Mortality Rate\")\n\nlibrary(highcharter)\n\n# Generate predictions dataframe\npredictions &lt;- data.frame(\n polity = seq(-10, 10, 1),\n xb = 68.4898 + -2.7999 * (seq(1, 21, 1) - 11)\n)\n\nhighchart() %&gt;%\n hc_add_series(\n   data = censor,\n   name = \"Observed IMR\",\n   type = \"scatter\",\n   hcaes(x = polity, y = IMR),\n   color = \"#005A43\",\n   dataLabels = list(enabled = TRUE, format = \"{point.ctry}\")\n ) %&gt;%\n hc_add_series(\n   data = predictions,\n   name = \"Predicted IMR\",\n   type = \"line\", \n   color=\"#6CC24A\",\n   hcaes(x = polity, y = xb)\n ) %&gt;%\n hc_xAxis(title = list(text = \"Polity\")) %&gt;%\n hc_yAxis(title = list(text = \"Infant Mortality Rate\"))"
  },
  {
    "objectID": "slides/bivariate25.html",
    "href": "slides/bivariate25.html",
    "title": "The Bivariate Model",
    "section": "",
    "text": "These slides aim to describe the elements of OLS regression - they attempt to connect the data structure to the matrix algebra that underlies the estimation of the coefficients.\nRegression in any form is based on the conditional expectation of \\(y\\) - the expected value of $Yy is conditional on some \\(X\\)s, but we don’t know the actual conditions or effects of those \\(X\\)s. So we can write the regression like this:\n\\[E[y|X_{1}, \\ldots,X_{k}]= \\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\]\n\n\nLet \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances.\n\n\n\nThe predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\).\n\n\n\nThe differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\).\n\n\n\nRestating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]\n\n\n\nBegin with the estimating equation:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nWe can solve for \\(\\widehat{\\beta}\\) by minimizing the sum of the squared errors and arrive at:\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\] We’ll do this below.\n\n\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]\n\n\n\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products.\n\n\n\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "slides/bivariate25.html#regression-1",
    "href": "slides/bivariate25.html#regression-1",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Let \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "slides/bivariate25.html#linear-predictions",
    "href": "slides/bivariate25.html#linear-predictions",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The predicted points that form the line are \\(\\widehat{y_{i}}\\)\n\\[\\widehat{y_{i}}=\\widehat{\\beta_{0}}+\\widehat{\\beta_{1}}X_{1,i}\\]\n\\(\\widehat{y_{i}}\\) is the sum of the \\(\\beta\\)s multiplied by each value of the appropriate \\(X_i\\), for \\(i=1 \\ldots N\\).\n\\(\\widehat{y_{i}}\\) is referred to as the “linear prediction” or as \\(x\\hat{\\beta}\\)."
  },
  {
    "objectID": "slides/bivariate25.html#residuals",
    "href": "slides/bivariate25.html#residuals",
    "title": "The Bivariate Model",
    "section": "",
    "text": "The differences between those predicted points, \\(\\widehat{y_{i}}\\) and the observed values \\(y_i\\) are:\n\\[\n\\begin{align}\n  \\widehat{u} = y_{i}-\\widehat{y_{i}} \\\\\n= y_{i}-\\widehat{\\beta_{0}}-\\widehat{\\beta_{1}}X_{1,i}\\\\\n= y_{i}-\\mathbf{x_i\\widehat{\\beta}}  \\nonumber\n\\end{align}\n\\]\nThese are the residuals, \\(\\widehat{u}\\) - the observed measure of the unobserved disturbances, \\(\\epsilon\\)."
  },
  {
    "objectID": "slides/bivariate25.html#in-matrix-notation",
    "href": "slides/bivariate25.html#in-matrix-notation",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Restating in matrix notation:\n\\[\n\\begin{align}\ny_{i} = \\mathbf{X_i}  \\beta_{k}  +\\varepsilon_i \\nonumber \\\\\n\\widehat{y_{i}}= \\mathbf{X_i} \\widehat{\\beta_{k}}   \\nonumber \\\\\n\\widehat{u_{i}} = y_i - \\mathbf{X_i} \\beta_k  \\nonumber \\\\\n\\widehat{u_i} = y_i - \\widehat{y_{i}}  \\nonumber\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/bivariate25.html#matrix-components",
    "href": "slides/bivariate25.html#matrix-components",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Begin with the estimating equation:\n\\[\ny_{i} = \\mathbf{X_i}  \\widehat{\\beta}  +\\epsilon \\nonumber\n\\]\nWe can solve for \\(\\widehat{\\beta}\\) by minimizing the sum of the squared errors and arrive at:\n\\[ \\widehat{\\beta} = \\mathbf{X'X}^{-1} \\mathbf{X'}y\\] We’ll do this below.\n\n\n\\[\n\\left[\n\\begin{matrix}\n  y_1 \\\\\n  y_2\\\\\n  y_3  \\\\\n  \\vdots \\\\\n  y_n    \\nonumber\n\\end{matrix}  \\right]\n= \\left[\n\\begin{matrix}\n  1& X_{1,2} & X_{1,3} & \\cdots & X_{1,k} \\\\\n  1 & X_{2,2} & X_{2,3} &\\cdots & X_{2,k} \\\\\n  1 & X_{3,2} & X_{3,3} &\\cdots & X_{3,k} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & X_{n,2} & X_{n,3} & \\cdots & X_{n,k}  \\nonumber\n\\end{matrix}  \\right]\n\\left[\n\\begin{matrix}\n  \\beta_1 \\\\\n  \\beta_2\\\\\n  \\beta_3  \\\\\n  \\vdots \\\\\n  \\beta_k    \\nonumber\n\\end{matrix}  \\right]\n+\n\\left[\n\\begin{matrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2\\\\\n  \\epsilon_3  \\\\\n  \\vdots \\\\\n  \\epsilon_n    \\nonumber\n\\end{matrix}  \\right]\n\\]\n\n\n\n\\[\n\\mathbf{X'X}= \\left[\n\\begin{matrix}\n  N& \\sum X_{2,i} & \\sum X_{3,i} & \\cdots & \\sum X_{k,i} \\\\\n  \\sum X_{2,i}&\\sum X_{2,2}^{2} & \\sum X_{2,i} X_{3,i} &\\cdots & \\sum X_{2,i}X_{k,i} \\\\\n  \\sum X_{3,i} & \\sum X_{3,i}X_{2,i}& \\sum X_{3,i}^{2} &\\cdots & \\sum X_{3,i}X_{k,i} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\sum X_{k,i} & \\sum X_{k,i}X_{2,i} & \\sum X_{k,i}X_{3,i} & \\cdots & \\sum X_{k,i}^{2}  \\nonumber\n\\end{matrix}  \\right]\n\\]\nIn \\(\\mathbf{X'X}\\), the main diagonal is the sums of squares and the offdiagonals are the cross-products.\n\n\n\n\\[\n\\mathbf{X'y}= \\left[\n\\begin{matrix}\n\\sum Y_{i}\\\\\n\\sum X_{2,i}Y_{i}\\\\\n\\sum X_{3,i}Y_{i}\\\\\n\\vdots \\\\\n\\sum X_{k,i}Y_{i} \\nonumber\n\\end{matrix}  \\right]\n\\]\nWhen we compute \\(\\widehat{\\beta}\\), \\(\\mathbf{X'y}\\) is the covariation of \\(X\\) and \\(Y\\), and we pre-multiply by the inverse of \\(\\mathbf{(X'X)^{-1}}\\) to control for the relationship between \\(X_{1}\\), \\(X_{2}\\), etc."
  },
  {
    "objectID": "slides/bivariate25.html#deriving-hatbeta",
    "href": "slides/bivariate25.html#deriving-hatbeta",
    "title": "The Bivariate Model",
    "section": "Deriving \\(\\hat{\\beta}\\)",
    "text": "Deriving \\(\\hat{\\beta}\\)\nStart with: \\[y = X\\beta + \\epsilon\\]\nMinimize sum of squared errors: \\[\\min_{\\beta} \\epsilon'\\epsilon = \\min_{\\beta} (y - X\\beta)'(y - X\\beta)\\]\nExpand: \\[(y - X\\beta)'(y - X\\beta) = y'y - y'X\\beta - \\beta'X'y + \\beta'X'X\\beta\\]\nSimplify using symmetry (\\(y'X\\beta = \\beta'X'y\\) as they’re scalars): \\[= y'y - 2\\beta'X'y + \\beta'X'X\\beta\\]\nTake derivative with respect to \\(\\beta\\) and set to zero: \\[\\frac{\\partial}{\\partial\\beta}(y'y - 2\\beta'X'y + \\beta'X'X\\beta) = 0\\] \\[-2X'y + 2X'X\\beta = 0\\]\nSolve for \\(\\beta\\): \\[X'X\\beta = X'y\\] \\[\\hat{\\beta} = (X'X)^{-1}X'y\\]"
  },
  {
    "objectID": "slides/bivariate25.html#variance-covariance-matrix",
    "href": "slides/bivariate25.html#variance-covariance-matrix",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance Matrix",
    "text": "Variance-Covariance Matrix\nStart with \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) and substitute \\(y = X\\beta + \\epsilon\\):\n\\[\\hat{\\beta} = (X'X)^{-1}X'(X\\beta + \\epsilon)\\] \\[= \\beta + (X'X)^{-1}X'\\epsilon\\]\nTherefore: \\[\\hat{\\beta} - \\beta = (X'X)^{-1}X'\\epsilon\\]\nThe variance-covariance matrix is: \\[Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)']\\] \\[= E[(X'X)^{-1}X'\\epsilon\\epsilon'X(X'X)^{-1}]\\]\nUnder homoskedasticity (\\(E[\\epsilon\\epsilon'] = \\sigma^2I\\)): \\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\]\nRecall that \\(sigma^2 = \\epsilon'\\epsilon / (N - k)\\), where \\(N\\) is the number of observations and \\(k\\) is the number of regressors including the constant."
  },
  {
    "objectID": "slides/bivariate25.html#variance-covariance-of-widehatbeta",
    "href": "slides/bivariate25.html#variance-covariance-of-widehatbeta",
    "title": "The Bivariate Model",
    "section": "Variance-Covariance of \\(\\widehat{\\beta}\\)",
    "text": "Variance-Covariance of \\(\\widehat{\\beta}\\)\n\\[ E[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)'] = \\] \\(~\\)\n\\[\\left[\n\\begin{array}{cccc}\nvar(\\beta_1) & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& var(\\beta_2) &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_2,\\beta_k) &\\cdots & var(\\beta_k)\\\\\n\\end{array} \\right] \\]"
  },
  {
    "objectID": "slides/bivariate25.html#standard-errors-of-beta_k",
    "href": "slides/bivariate25.html#standard-errors-of-beta_k",
    "title": "The Bivariate Model",
    "section": "Standard Errors of \\(\\beta_k\\)",
    "text": "Standard Errors of \\(\\beta_k\\)\n\\[\n\\left[\n\\begin{array}{cccc}\n\\sqrt{var(\\beta_1)} & cov(\\beta_1,\\beta_2) &\\cdots &cov(\\beta_1,\\beta_k)\\\\\ncov(\\beta_2,\\beta_1)& \\sqrt{var(\\beta_2)} &\\cdots &cov(\\beta_2,\\beta_k)\\\\\n\\vdots&\\vdots&\\ddots& \\vdots\\\\\ncov(\\beta_k,\\beta_1) &cov(\\beta_k,\\beta_2) &\\cdots & \\sqrt{var(\\beta_k)}\\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "slides/bivariate25.html#property-1",
    "href": "slides/bivariate25.html#property-1",
    "title": "The Bivariate Model",
    "section": "Property 1",
    "text": "Property 1\nIf \\(X' \\widehat{\\epsilon}  = 0\\) holds, then the following properties exist:\n\n\n\n\n\n\nProposition\n\n\n\nEach \\(x\\) variable (each column vector of \\(\\mathbf{X}\\)) is uncorrelated with \\(\\epsilon\\)."
  },
  {
    "objectID": "slides/bivariate25.html#property-2",
    "href": "slides/bivariate25.html#property-2",
    "title": "The Bivariate Model",
    "section": "Property 2",
    "text": "Property 2\nAssuming a constant in the matrix \\(\\mathbf{X}\\),\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\sum\\limits_{i-1}^{n} \\epsilon_i = 0\\)\nbecause each element of the matrix \\(X' \\widehat{\\epsilon}\\) would be nonzero due to the constant; only by multiplying by \\(\\epsilon_i=0\\) would each element equal zero, and then the sum must be zero."
  },
  {
    "objectID": "slides/bivariate25.html#property-3",
    "href": "slides/bivariate25.html#property-3",
    "title": "The Bivariate Model",
    "section": "Property 3",
    "text": "Property 3\n\n\n\n\n\n\nProposition\n\n\n\nThe mean of the residuals is zero.\nIf the sum of the residuals is zero, that sum divided by \\(N\\) must also be zero."
  },
  {
    "objectID": "slides/bivariate25.html#property-4",
    "href": "slides/bivariate25.html#property-4",
    "title": "The Bivariate Model",
    "section": "Property 4",
    "text": "Property 4\n\n\n\n\n\n\nProposition\n\n\n\nThe regression line (in the bivariate case) or the hyperplane (in the multivariate case) passes through the means of the observed variables, \\(\\mathbf{X}\\) and \\(y\\).\n\\[\n\\begin{align}\n\\epsilon = y - X\\widehat{\\beta} \\nonumber \\\\ \\\\\n\\text{multiplying by} ~  N^{-1} \\nonumber \\\\ \\\\\n\\bar{\\epsilon} = \\bar{y} - \\bar{X} \\widehat{\\beta} = 0  \\nonumber\n\\end{align}\n\\]\n\\(\\bar{y} - \\bar{X} \\widehat{\\beta}=0\\) implies \\(\\bar{y} = \\bar{X} \\widehat{\\beta}\\), and therefore implies the intersection of the means with the regression line. Moreover, at the point or plane \\(\\bar{y} - \\bar{X} \\widehat{\\beta}\\), the mean of the residuals equals zero, implying the regression line or hyperplane passes through it."
  },
  {
    "objectID": "slides/bivariate25.html#properties",
    "href": "slides/bivariate25.html#properties",
    "title": "The Bivariate Model",
    "section": "Properties",
    "text": "Properties\nNote that these properties are true because we are minimizing the sum of the squared residuals. They do not have particular meaning otherwise regarding the errors, whether we meet assumptions of the model, etc. The next step is to make a set of assumptions regarding the error term in order to facilitate statements about \\(\\widehat{\\beta}\\), and inferences about those estimates."
  },
  {
    "objectID": "slides/bivariate25.html#estimating-equation",
    "href": "slides/bivariate25.html#estimating-equation",
    "title": "The Bivariate Model",
    "section": "",
    "text": "Let \\(y\\) be a linear function of the \\(X\\)s and the unknowns, \\(\\beta\\), so the following produces a straight line:\n\\[y_{i}=\\beta_{0}+\\beta_{1}X_{1} + \\epsilon \\]\nand \\(\\epsilon\\) are the errors or disturbances."
  },
  {
    "objectID": "slides/olsmatrix25.html#step-1-define-the-objective-function",
    "href": "slides/olsmatrix25.html#step-1-define-the-objective-function",
    "title": "Deriving the OLS Estimator",
    "section": "Step 1: Define the Objective Function",
    "text": "Step 1: Define the Objective Function\nThe OLS estimator minimizes the sum of squared residuals:\n\\[\\min_{\\beta} (y - X\\beta)'(y - X\\beta)\\]"
  },
  {
    "objectID": "slides/olsmatrix25.html#step-2-first-order-conditions",
    "href": "slides/olsmatrix25.html#step-2-first-order-conditions",
    "title": "Deriving the OLS Estimator",
    "section": "Step 2: First Order Conditions",
    "text": "Step 2: First Order Conditions\nTake the derivative with respect to \\(\\beta\\) and set equal to zero:\n\\[\\frac{\\partial}{\\partial \\beta}(y - X\\beta)'(y - X\\beta) = 0\\]\nExpanding the expression:\n\\[\\frac{\\partial}{\\partial \\beta}(y'y - y'X\\beta - \\beta'X'y + \\beta'X'X\\beta) = 0\\]\nNote: \\(y'X\\beta\\) is a scalar, so \\(y'X\\beta = \\beta'X'y\\)"
  },
  {
    "objectID": "slides/olsmatrix25.html#step-3-solve-first-order-conditions",
    "href": "slides/olsmatrix25.html#step-3-solve-first-order-conditions",
    "title": "Deriving the OLS Estimator",
    "section": "Step 3: Solve First Order Conditions",
    "text": "Step 3: Solve First Order Conditions\n\\[-2X'y + 2X'X\\beta = 0\\]\nSolving for \\(\\beta\\):\n\\[X'X\\beta = X'y\\]\n\\[\\hat{\\beta} = (X'X)^{-1}X'y\\]\nThis is the OLS estimator for \\(\\beta\\)."
  },
  {
    "objectID": "slides/olsmatrix25.html#step-1-express-hatbeta-in-terms-of-true-beta",
    "href": "slides/olsmatrix25.html#step-1-express-hatbeta-in-terms-of-true-beta",
    "title": "Deriving the OLS Estimator",
    "section": "Step 1: Express \\(\\hat{\\beta}\\) in Terms of True \\(\\beta\\)",
    "text": "Step 1: Express \\(\\hat{\\beta}\\) in Terms of True \\(\\beta\\)\nSubstitute the true model into our estimator:\n\\[\\hat{\\beta} = (X'X)^{-1}X'(X\\beta + \\epsilon)\\] \\[\\hat{\\beta} = \\beta + (X'X)^{-1}X'\\epsilon\\]"
  },
  {
    "objectID": "slides/olsmatrix25.html#step-2-calculate-variance",
    "href": "slides/olsmatrix25.html#step-2-calculate-variance",
    "title": "Deriving the OLS Estimator",
    "section": "Step 2: Calculate Variance",
    "text": "Step 2: Calculate Variance\nThe variance-covariance matrix is:\n\\[Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)']\\]\nSubstituting:\n\\[Var(\\hat{\\beta}) = E[(X'X)^{-1}X'\\epsilon\\epsilon'X(X'X)^{-1}]\\]"
  },
  {
    "objectID": "slides/olsmatrix25.html#step-3-apply-classical-assumptions",
    "href": "slides/olsmatrix25.html#step-3-apply-classical-assumptions",
    "title": "Deriving the OLS Estimator",
    "section": "Step 3: Apply Classical Assumptions",
    "text": "Step 3: Apply Classical Assumptions\nUnder classical assumptions: - \\(E[\\epsilon] = 0\\) - \\(E[\\epsilon\\epsilon'] = \\sigma^2I\\)\nTherefore:\n\\[Var(\\hat{\\beta}) = (X'X)^{-1}X'E[\\epsilon\\epsilon']X(X'X)^{-1}\\] \\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}X'X(X'X)^{-1}\\] \\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\]\nThis is the variance-covariance matrix of the OLS estimator."
  },
  {
    "objectID": "slides/olsmatrix25.html#key-properties",
    "href": "slides/olsmatrix25.html#key-properties",
    "title": "Deriving the OLS Estimator",
    "section": "Key Properties",
    "text": "Key Properties\n\nThe OLS estimator \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) is:\n\nLinear in y\nUnbiased: \\(E[\\hat{\\beta}] = \\beta\\)\nBest (minimum variance) among linear unbiased estimators under classical assumptions\n\nThe variance-covariance matrix \\(Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\):\n\nDepends on the error variance \\(\\sigma^2\\)\nIs inversely related to sample size through \\(X'X\\)\nCan be estimated using \\(\\hat{\\sigma}^2 = \\frac{e'e}{n-k}\\) where \\(e\\) is the vector of residuals"
  },
  {
    "objectID": "slides/olsmatrix25.html#ols-derivation",
    "href": "slides/olsmatrix25.html#ols-derivation",
    "title": "OLS Derivation and Normal PDF Plot",
    "section": "",
    "text": "We start with the linear regression model in matrix form:\n\\(y = X\\beta + \\epsilon\\)\nwhere:\n\n\\(y\\) is an \\((n \\times 1)\\) vector of observed dependent variables.\n\\(X\\) is an \\((n \\times k)\\) matrix of observed independent variables (including a constant term if applicable).\n\\(\\beta\\) is a \\((k \\times 1)\\) vector of unknown coefficients to be estimated.\n\\(\\epsilon\\) is an \\((n \\times 1)\\) vector of unobserved error terms.\n\nThe Ordinary Least Squares (OLS) estimator aims to minimize the sum of squared residuals:\n\\(\\min_\\beta (y - X\\beta)^T(y - X\\beta)\\)\nTo find the minimum, we take the derivative with respect to \\(\\beta\\), set it to zero, and solve for \\(\\beta\\):\n\nDerivative:\n\n\\(\\frac{d}{d\\beta} [(y - X\\beta)^T(y - X\\beta)] = -2X^Ty + 2X^TX\\beta\\)\n\nSet to zero and solve:\n\n\\(-2X^Ty + 2X^TX\\beta = 0\\) \\(X^TX\\beta = X^Ty\\) \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\) (assuming \\(X^TX\\) is invertible)\nThis is the OLS estimator for \\(\\beta\\)."
  },
  {
    "objectID": "slides/olsmatrix25.html#variance-covariance-matrix-of-hatbeta",
    "href": "slides/olsmatrix25.html#variance-covariance-matrix-of-hatbeta",
    "title": "OLS Derivation and Normal PDF Plot",
    "section": "",
    "text": "The variance-covariance matrix of \\(\\hat{\\beta}\\) is given by:\n\\(Var(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T]\\)\nUnder the classical OLS assumptions (including homoscedasticity and no autocorrelation of errors), this simplifies to:\n\\(Var(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\\)\nwhere \\(\\sigma^2\\) is the variance of the error term, \\(\\epsilon\\). An unbiased estimator of \\(\\sigma^2\\) is given by:\n\\(s^2 = \\frac{(y - X\\hat{\\beta})^T(y - X\\hat{\\beta})}{(n - k)}\\)\nThus, an estimator of the variance-covariance matrix is:\n\\(\\widehat{Var}(\\hat{\\beta}) = s^2 (X^TX)^{-1}\\)"
  },
  {
    "objectID": "slides/predictionmethods25.html",
    "href": "slides/predictionmethods25.html",
    "title": "Prediction Methods",
    "section": "",
    "text": "We’ll use a dataset of Major League Baseball attendance to illustrate different methods of generating predictions from multivariate models.\n\n\ncode\nmlb &lt;- read.csv(\"/users/dave/documents/teaching/501/2023/slides/L3_multivariate/code/mlbattendance/MLBattend.csv\")  \n\n# rescale home attendance\nmlb$Home_attend &lt;- mlb$Home_attend/1000\n\ndatasummary(All(mlb) ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB attendance data\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        MLB attendance data\n              \n                 \n                mean\n                median\n                sd\n                min\n                max\n              \n        \n        \n        \n                \n                  Season      \n                  1985.06\n                  1985.00\n                  9.27  \n                  1969.00\n                  2000.00\n                \n                \n                  Home_attend \n                  1777.99\n                  1681.90\n                  755.87\n                  306.76 \n                  4483.35\n                \n                \n                  Runs_scored \n                  694.94 \n                  691.50 \n                  105.17\n                  329.00 \n                  1009.00\n                \n                \n                  Runs_allowed\n                  694.89 \n                  693.00 \n                  105.52\n                  331.00 \n                  1103.00\n                \n                \n                  Wins        \n                  78.85  \n                  79.00  \n                  12.67 \n                  37.00  \n                  114.00 \n                \n                \n                  Losses      \n                  78.88  \n                  79.00  \n                  12.65 \n                  40.00  \n                  110.00 \n                \n                \n                  Games_behind\n                  14.39  \n                  13.00  \n                  11.75 \n                  0.00   \n                  52.00  \n                \n        \n      \n    \n\n\n\n\n\nHere’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -691.488 \n                \n                \n                             \n                  (151.853)\n                \n                \n                  Runs_scored\n                  3.554    \n                \n                \n                             \n                  (0.216)  \n                \n                \n                  Num.Obs.   \n                  838      \n                \n                \n                  R2         \n                  0.244    \n                \n                \n                  R2 Adj.    \n                  0.244    \n                \n                \n                  AIC        \n                  13256.5  \n                \n                \n                  BIC        \n                  13270.7  \n                \n                \n                  Log.Lik.   \n                  -6625.265\n                \n                \n                  F          \n                  270.514  \n                \n                \n                  RMSE       \n                  656.62   \n                \n        \n      \n    \n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\n\n\n\nHere’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept) \n                  -70849.854\n                \n                \n                              \n                  (4473.400)\n                \n                \n                  Runs_scored \n                  3.123     \n                \n                \n                              \n                  (0.338)   \n                \n                \n                  Runs_allowed\n                  -1.887    \n                \n                \n                              \n                  (0.357)   \n                \n                \n                  Season      \n                  36.215    \n                \n                \n                              \n                  (2.284)   \n                \n                \n                  Games_behind\n                  -8.273    \n                \n                \n                              \n                  (2.755)   \n                \n                \n                  Num.Obs.    \n                  838       \n                \n                \n                  R2          \n                  0.471     \n                \n                \n                  R2 Adj.     \n                  0.469     \n                \n                \n                  AIC         \n                  12963.1   \n                \n                \n                  BIC         \n                  12991.5   \n                \n                \n                  Log.Lik.    \n                  -6475.561 \n                \n                \n                  F           \n                  185.758   \n                \n                \n                  RMSE        \n                  549.20    \n                \n        \n      \n    \n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "slides/predictionmethods25.html#bivariate-model-predictions",
    "href": "slides/predictionmethods25.html#bivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a simple model predicting season attendance at MLB games, regressing home attendance on runs scored, then generating “in-sample” predictions by computing:\n\\[\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\]\nwhere \\(x_i\\) is the number of runs scored for each row/observation in the estimation data.\n\n\ncode\nm1 &lt;- lm(data=mlb, Home_attend ~  Runs_scored)\nmodelsummary(m1)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -691.488 \n                \n                \n                             \n                  (151.853)\n                \n                \n                  Runs_scored\n                  3.554    \n                \n                \n                             \n                  (0.216)  \n                \n                \n                  Num.Obs.   \n                  838      \n                \n                \n                  R2         \n                  0.244    \n                \n                \n                  R2 Adj.    \n                  0.244    \n                \n                \n                  AIC        \n                  13256.5  \n                \n                \n                  BIC        \n                  13270.7  \n                \n                \n                  Log.Lik.   \n                  -6625.265\n                \n                \n                  F          \n                  270.514  \n                \n                \n                  RMSE       \n                  656.62   \n                \n        \n      \n    \n\n\n\ncode\np1 &lt;- data.frame(predict(m1), mlb$Runs_scored)\n\nggplot(p1, aes(x=mlb.Runs_scored, y=predict.m1.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")"
  },
  {
    "objectID": "slides/predictionmethods25.html#multivariate-model-predictions",
    "href": "slides/predictionmethods25.html#multivariate-model-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept) \n                  -70849.854\n                \n                \n                              \n                  (4473.400)\n                \n                \n                  Runs_scored \n                  3.123     \n                \n                \n                              \n                  (0.338)   \n                \n                \n                  Runs_allowed\n                  -1.887    \n                \n                \n                              \n                  (0.357)   \n                \n                \n                  Season      \n                  36.215    \n                \n                \n                              \n                  (2.284)   \n                \n                \n                  Games_behind\n                  -8.273    \n                \n                \n                              \n                  (2.755)   \n                \n                \n                  Num.Obs.    \n                  838       \n                \n                \n                  R2          \n                  0.471     \n                \n                \n                  R2 Adj.     \n                  0.469     \n                \n                \n                  AIC         \n                  12963.1   \n                \n                \n                  BIC         \n                  12991.5   \n                \n                \n                  Log.Lik.    \n                  -6475.561 \n                \n                \n                  F           \n                  185.758   \n                \n                \n                  RMSE        \n                  549.20    \n                \n        \n      \n    \n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "slides/predictionmethods25.html#at-means-predictions-adjusted-effects",
    "href": "slides/predictionmethods25.html#at-means-predictions-adjusted-effects",
    "title": "Prediction Methods",
    "section": "At-means predictions (adjusted effects)",
    "text": "At-means predictions (adjusted effects)\nAt-means predictions (also called “adjusted effects”) set all variables except the \\(x\\) of interest at a sensible value - usually the mean, median, or mode depending on the level of measurment. Holding those constant, but varying just the \\(x\\) of interest produces predictions of \\(y\\) that are neater and easier to discuss than the in-sample ones above.\n\nSummarize the estimation data\nLet’s find the means etc. of the variables in the model. It’s important only to consider the cases that are in the estimation data, that is, actually used in the model. Write code to identify cases in the model, then use the means, etc. of these for the predictions:\n\n\ncode\nmlb$used &lt;- TRUE\nmlb$used[na.action(m3)] &lt;- FALSE\nestdata &lt;- mlb %&gt;%  filter(used==\"TRUE\")\n\ndatasummary(Home_attend+Runs_scored +Runs_allowed + Season + Games_behind ~ mean+ median+ sd+min+max,  data=mlb, style=\"grid\", title = \"MLB estimation data\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        MLB estimation data\n              \n                 \n                mean\n                median\n                sd\n                min\n                max\n              \n        \n        \n        \n                \n                  Home_attend \n                  1777.99\n                  1681.90\n                  755.87\n                  306.76 \n                  4483.35\n                \n                \n                  Runs_scored \n                  694.94 \n                  691.50 \n                  105.17\n                  329.00 \n                  1009.00\n                \n                \n                  Runs_allowed\n                  694.89 \n                  693.00 \n                  105.52\n                  331.00 \n                  1103.00\n                \n                \n                  Season      \n                  1985.06\n                  1985.00\n                  9.27  \n                  1969.00\n                  2000.00\n                \n                \n                  Games_behind\n                  14.39  \n                  13.00  \n                  11.75 \n                  0.00   \n                  52.00  \n                \n        \n      \n    \n\n\n\nIn this case, it turns out we use all the cases in the data - it’s important to check this any time you’re making model predictions.\n\n\nGenerate at-mean predictions\nCreate a new data frame with as many observations as there are interesting values of the \\(x\\) variable of interest. Then, set all variables except the \\(x\\) of interest at their means, medians, or modes. Using the standard errors of the predictions, generate the boundaries of the confidence intervals by end point transformation.\n\\[ \\widehat{y} \\pm 1.96 \\times se(\\widehat{y}) \\]\n\n\ncode\noosdata &lt;- data.frame(Intercept=1, Runs_allowed=694 , Season=1985 , Games_behind=14 , Runs_scored= c(seq(330,1000,10)))\n\nmlb.predict &lt;- data.frame(oosdata, predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=oosdata))\n\n# confidence bounds by end point transformation\nmlb.predict &lt;- mlb.predict %&gt;% mutate(ub=fit.fit+1.96*se.fit) %&gt;% mutate(lb=fit.fit-1.96*se.fit)\n\natmean &lt;- ggplot(mlb.predict, aes(x=Runs_scored, y=fit.fit)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") + \n  theme_minimal() +\n  ggtitle(\"At-mean effects\")\n\natmean\n\n\n\n\n\n\n\n\n\n\n\nAside on standard errors of \\(\\widehat{y}\\)\nThe standard errors of \\(\\widehat{y}\\) are calculated using the \\(x\\) values, and the variance-covariance matrix of the coefficients. These are usually calculated as follows:\n\\[ X ~VX' \\]\nmultiplying the \\(X\\) matrix (remembering this could be the out of sample \\(X\\) matrix), the variance-covariance matrix of \\(\\widehat{\\beta}\\), and the transpose of the \\(X\\) matrix. The square root of the main diagonal gives the standard errors of \\(\\widehat{y}\\).\n\n\n\n\n\n\nUncertainty about \\(\\widehat{y}\\)\n\n\n\nThe intuition is that we’re using our uncertainty about the coefficients to generate measures of uncertainty about the predictions."
  },
  {
    "objectID": "slides/predictionmethods25.html#average-effects-end-point-boundaries",
    "href": "slides/predictionmethods25.html#average-effects-end-point-boundaries",
    "title": "Prediction Methods",
    "section": "Average effects, end point boundaries",
    "text": "Average effects, end point boundaries\nAverage effects are where we’re computing \\(N\\) predictions for every interesting value of \\(x\\), then taking the average of those predictions for each value of \\(x\\). This is a counterfactual approach - we’re using the actual data, changing only the variable of interest, as if all observations took on the same value of that variable.\nFor instance, in our MLB attendance model, we change the value of “Runs Scored” to the minimum value, 329, for every observation, keeping all the other variables as they are in the real estimation data. Compute the predictions for all the observations at \\(x = 329\\), then take the average of those predictions - now iterate to 330, 331, etc.\n\n\n\n\n\n\nCounterfactuals\n\n\n\nWe are treating the estimation data as if every team scored 329 runs - this is the counterfactual - then computing the average attendance for that counterfactual, and doing this for all counterfactuals (values of \\(x\\)).\n\n\n\n\ncode\n# preserve the original values of Runs Scored\nmlb &lt;- mlb%&gt;%\nmutate(original_runs_scored = Runs_scored)\n\nxb = 0\nse = 0\nrs = 0\n\nfor(i in seq(330,1000,1)){  #iterating by 1 to max number of obs we're taking medians for\n  mlb$Runs_scored &lt;- i\n  mlb.predict &lt;- data.frame(predict(m3, interval=\"confidence\", se.fit=TRUE, level=.05, newdata=mlb))\n  xb[i-329] &lt;- median(mlb.predict$fit.fit) #index using i but minus constant to start at row 1\n  se[i-329] &lt;- median(mlb.predict$se.fit)\n  rs[i-329] &lt;- i\n}\n\navg.pred &lt;- data.frame(xb, se, rs)\n\n# upper and lower bounds by end point transformation\n\navg.pred &lt;- avg.pred %&gt;% mutate(ub=xb+1.96*se) %&gt;% mutate(lb=xb-1.96*se)\n\n# reset the estimation data to the actual values of Runs Scored\nmlb &lt;- mlb%&gt;%\n  mutate(Runs_scored=original_runs_scored )\n\n#plot\naverage &lt;- ggplot(avg.pred, aes(x=rs, y=xb)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Average Effects\") +\n  theme_minimal()\n\naverage\n\n\n\n\n\n\n\n\n\n\nComparing at-mean and average effects\n\n\ncode\natmean + average"
  },
  {
    "objectID": "slides/predictionmethods25.html#simulated-effects",
    "href": "slides/predictionmethods25.html#simulated-effects",
    "title": "Prediction Methods",
    "section": "Simulated effects",
    "text": "Simulated effects\nThe last method we’ll consider is simulated effects. This is a way to generate predictions that are based on the distribution of the coefficients. The \\(\\widehat{\\beta}\\)s are estimates of the mean of a normal distribution, and the var-cov matrix of the coefficients is an estimate of the variance of that distribution. We can assume the distribution of \\(\\beta\\) is Normal because of the Central Limit Theorem.\nHere’s the process. Treat our \\(\\widehat{\\beta}\\)s as the mean of a normal distribution, and the var-cov matrix of the coefficients as the variance. Generate a large number of draws from that distribution, say 10,000. This will give us the simulated distribution of \\(\\widehat{\\beta}\\). We’ll have 10,000 estimates of \\(\\widehat{\\beta}\\).\nUsing these, we can now produce 10,000 predictions for each interesting value of \\(x\\). We can then summarize the distribution of those predictions, using the median as our point estimate, and the percentiles (2.5, 97.5) as our confidence boundaries.\n\n\ncode\nB &lt;- data.frame(rmvnorm(n=10000, mean=coef(m3), vcov(m3))) #simulated distribution of the estimates using the var-cov matrix of B as the variance, and the estimates of B as the mean.\n\ncolnames(B) &lt;-c('b0', 'b1', 'b2', 'b3', 'b4')\n\nsim.preds &lt;- data.frame ( lb = numeric(0),med= numeric(0),\n                            ub= numeric(0), r = numeric(0))\n\nfor(i in seq(330,1000,1)){\n  xbR  &lt;- quantile(B$b0 + B$b1*i + B$b2*694 + B$b3*1985 +B$b4*14, probs=c(.025,.5,.975))\n  xbR&lt;- data.frame(t(xbR))\n  sim.preds[(i-329):(i-329),1:4] &lt;- data.frame(xbR, i)\n}\n\n#plot\nsim &lt;- ggplot(sim.preds, aes(x=r, y=med)) + \n  geom_line() + \n  geom_ribbon(aes(ymin=lb, ymax=ub), alpha=.2) + \n  xlab(\"Runs Scored\") + \n  ylab(\"Expected Attendance (in thousands)\") +\n  ggtitle(\"Simulated Effects\") +\n  theme_minimal()\n\nsim\n\n\n\n\n\n\n\n\n\n\nComparing methods\n\n\ncode\n(atmean + average + sim)"
  },
  {
    "objectID": "slides/predictionmethods25.html#multivariate-model-in-sample-predictions",
    "href": "slides/predictionmethods25.html#multivariate-model-in-sample-predictions",
    "title": "Prediction Methods",
    "section": "",
    "text": "Here’s a model of attendance at MLB games, regressing home attendance on runs scored, runs allowed, season (year), and games behind, then generating “in-sample” predictions.\n\n\ncode\nm3 &lt;- lm(data=mlb, Home_attend ~  Runs_scored + Runs_allowed+ Season +Games_behind)\nmodelsummary(m3)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept) \n                  -70849.854\n                \n                \n                              \n                  (4473.400)\n                \n                \n                  Runs_scored \n                  3.123     \n                \n                \n                              \n                  (0.338)   \n                \n                \n                  Runs_allowed\n                  -1.887    \n                \n                \n                              \n                  (0.357)   \n                \n                \n                  Season      \n                  36.215    \n                \n                \n                              \n                  (2.284)   \n                \n                \n                  Games_behind\n                  -8.273    \n                \n                \n                              \n                  (2.755)   \n                \n                \n                  Num.Obs.    \n                  838       \n                \n                \n                  R2          \n                  0.471     \n                \n                \n                  R2 Adj.     \n                  0.469     \n                \n                \n                  AIC         \n                  12963.1   \n                \n                \n                  BIC         \n                  12991.5   \n                \n                \n                  Log.Lik.    \n                  -6475.561 \n                \n                \n                  F           \n                  185.758   \n                \n                \n                  RMSE        \n                  549.20    \n                \n        \n      \n    \n\n\n\ncode\np3 &lt;- data.frame(predict(m3), mlb$Runs_scored)\nggplot(p3, aes(x=mlb.Runs_scored, y=predict.m3.)) + \n  geom_line() +\n  xlab(\"Runs Scored\") +\n  ylab(\"Predicted Attendance (in thousands)\")   \n\n\n\n\n\n\n\n\n\nAs you can see, the predictions are pretty ugly because all four \\(x\\) variables are varying by observation, so we’re getting predictions based on each individual observation’s characteristics - interesting if we care about the 1977 Red Sox, but not something we can draw generalities from. What’s missing here?\n\n\n\n\n\n\nControl\n\n\n\nHold all variables constant at means, medians, or modes, except the one of interest, so we can focus on how changes in the \\(x\\) of interest affect \\(\\widehat{y}\\)."
  },
  {
    "objectID": "code/ex1answers2025.html",
    "href": "code/ex1answers2025.html",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\ncdf-pdf"
  },
  {
    "objectID": "code/ex1answers2025.html#q1",
    "href": "code/ex1answers2025.html#q1",
    "title": "exercise #1 answers",
    "section": "",
    "text": "code\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- plogis(xb)\nnormalcdf &lt;- pnorm(xb)\nlogitpdf &lt;- dlogis(xb)\nnormalpdf &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, logitcdf, normalcdf, logitpdf, normalpdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=normalcdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 2, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\",  x=\"x\") +\n  ggtitle(\"Normal and Logistic CDFs\")\n\npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=normalpdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .3, label = \"Normal\") +\n  annotate(\"text\", x = -3, y = .1, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal and Logistic PDFs\")\n\n\ncdf-pdf"
  },
  {
    "objectID": "code/ex1answers2025.html#q2",
    "href": "code/ex1answers2025.html#q2",
    "title": "exercise #1 answers",
    "section": "Q2",
    "text": "Q2\n\n\ncode\nxb &lt;- runif(1000, min=-4, max=4)\npdf1 &lt;- dnorm(xb, mean=0, sd=1)\npdf2 &lt;- dnorm(xb, mean=0, sd=sqrt(.25))\npdf3 &lt;- dnorm(xb, mean=-1, sd=sqrt(1.25))\npdf4 &lt;- dnorm(xb)\n\ndf &lt;- data.frame(xb, pdf1, pdf2, pdf3, pdf4)\n\nggplot(data=df, aes(x=xb, y=pdf1)) +\n  geom_line() +\n  geom_line(aes(y=pdf2), linetype=\"dotted\") +\n  geom_line(aes(y=pdf3), linetype=\"longdash\" ) +\n  annotate(\"text\", x = 2.5, y = .1, label = \"Normal (0,1)\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Normal (0, .25)\") +\n  annotate(\"text\", x = -3.3, y = .2, label = \"Normal (-1, 1.25)\") +\n  labs(y=\"Pr(Y=1)\", x=\"x\") +\n  ggtitle(\"Normal PDFs\")"
  },
  {
    "objectID": "slides/inquiry25.html",
    "href": "slides/inquiry25.html",
    "title": "A note on inquiry",
    "section": "",
    "text": "Inquiry and Model Choice\nPerhaps the most useful and important, though difficult skill to learn in science is how to choose a method of inquiry (say, a model). For one thing, there is a huge array of methods to choose from, and they’ll each have their proponents and their detractors. Within that noisy environment, there is always, always a complexity bias - the fancier, more difficult, more esoteric the model, the better it must be. For another, inquiry is also faddish. In the early 1990s, everthing was “colinear.” In the early 2000s, everything suffered selection bias and needed a Heckman-style model. More recently, everything is “endogenous,” a term throw around very loosely, with few suggestions as to remedies. Whatever the current fad is, expect Reviewer #2 to criticize your paper on those grounds, no matter how irrelevant.\n\nHow does a scholar learn to navigate this methodological battlefield, to choose appropriate methods, to get persuasive and reasonable answers from their models, and persuade others they’re reasonable enough they warrant publication? I think a good starting point is to truly adopt and internalize George Box’s observation that “all models are wrong, some are useful.” The contest is not to be “right” but to make choices in a correct way that lead to a useful model.\nThinking about models as useful or not useful, say on a “usefulness” continuum, invites us to think about what makes them useful, and to reflect on what our analytic goals are. For instance, are we mainly interested in prediction? Are we primarily interested in discovery or description? Are we focused on testing hypotheses or establishing causal patterns? These are related but different sorts of inquiry - they usually imply different types of methods. A method that’s useful for one type of question may not be useful for another.\nConsider a scholar interested in two principal questions. The first is “how widely influential is a variable, say age, in shaping support for political violence?” The second is “how important is this variable to explaining support for political violence compared to other variables?” These questions are not really about prediction or about causation - they’re more oriented to discovering and describing the landscape of correlations among a fairly wide set of variables across a relatively large number of studies using a relatively large number of different data sources. Pattern-seeking methods (say, ensemble models) might be really useful for these purposes.\nAnother scholar might be interested in what determines levels of support for political violence, when and why. This implies a different type of inquiry focused on causal paths and prediction. The methods this scholar would likely choose would be quite different from those chosen by the first scholar, perhaps using regression-based methods of causal inference.\nNotice the question here is not about relative “rightness” - which scholar is “more right”. Instead, the question is absolute - is scholar #1’s inquiry or model useful? Is scholar #2’s inquiry or model useful?\nWhat makes an inquiry “useful?” There are very involved debates on questions related to this in the philosophy of science, especially in debates about ontology, about the nature of assumptions, and about what comprises “theory.” I think about this in a very simple way. If an inquiry teaches us something and provokes further inquiry, it’s useful. The more an inquiry shows us and the more research or inquiry it provokes, the more useful it is.\nNot coincidentally, the more provocative or useful a model or inquiry is, the “stupider” I feel, because I’m reminded how much I don’t know, how much more there is to know, that there are questions I didn’t know existed and new lines of inquiry to pursue far beyond what I had imagined. I love this feeling.\nSo what do we do? Seek out challenges - new methods, new literatures and arguments. Discuss and argue with each other. Tackle the hard problems that engage you, provoke you, challenge you, make you curious, make you feel stupid. Choose methods that are useful to your inquiry; they may be simple or complex, it doesn’t matter. What matters is that those methods or models are useful. Read a lot, especially things out of your particular (narrow) area. Your literature will become an echo chamber - avoid that by reading as broadly as you can. Be picky. Read things the challenge and engage you and interest you. Read outside the discipline - read historical and journalistic accounts of the phenomena you’re interested in.\nIt’s easy to be overwhelmed by methods of inquiry, but important to recognize those methods as tools in a very broad toolbox. Only some of those are useful for the questions you’re asking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/inquiry25.html#some-cautionary-guidelines",
    "href": "slides/inquiry25.html#some-cautionary-guidelines",
    "title": "On Inquiry and Models",
    "section": "Some cautionary guidelines",
    "text": "Some cautionary guidelines\n``All models are wrong, but some are useful.’’ (George Box)\n\nBe suspicious of your results no matter what they tell you.\nAssume the data are noisy.\nAssume the estimator is wrong.\nAssume the model is misspecified.\nAssume your model assumptions are heroic.\nAssume your theory assumptions are heroic."
  },
  {
    "objectID": "slides/inquiry25.html#and-approaches",
    "href": "slides/inquiry25.html#and-approaches",
    "title": "On Inquiry and Models",
    "section": "And approaches",
    "text": "And approaches\n\nsearch for the limits of your results; evaluate their robustness.\ntame the data by cleaning or transforming; use alternative measures.\nexamine different specifications and estimation models.\nrelax the statistical assumptions.\nexamine the empirical implications and actual effects of the theory assumptions.\n\n\nDeveloping Papers\nFrom an interview with Harvard economist Susan Athey in Bowmaker (2012):\nQ: How do you achieve the right balance between communicating your research at an early stage versus the “close-to-finished” stage?\nA: I see young people making two kinds of mistakes: not presenting until something is done, and presenting something that’s half-baked. Generally, you have to realize that when you go out and give a seminar, you need to be prepared that people in the audience will have their main impression of you formed by that seminar. If there’s a group that you regularly present to, then it’s a little bit safer, but the world is full of Bayesians — people will update a lot based on one signal. Something that you present doesn’t have to be done, but you better be intelligent about it; you better be very clear on what you have done and what you haven’t done. And the part that you have done had better be good.\nSenior people and good people will be very happy to give you feedback on things that aren’t done. They’ll respect the research process. But what you don’t want to do is have something where the whole idea doesn’t make sense, where you haven’t thought it through, and where people will feel like you’re wasting their time, or they’ll just decide that you’re stupid.\nIt’s important for young people to practice. They practice for the job market, but probably underestimate the importance of doing it later. You see a lot of people mismanage their time. I find it very difficult to practice [laughs]. It’s a difficult thing to discipline yourself to do, but, until you become really comfortable and fluent, it’s crucial. And you also need to think from the perspective of the audience, particularly in terms of what questions people might have."
  }
]